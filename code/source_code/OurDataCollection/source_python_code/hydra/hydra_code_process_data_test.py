from pathlib import Path
import tensorflow as tf
import re
import os
import csv
import numpy as np
import pandas as pd
import sys
sys.path.append("/content/drive/MyDrive/HK2_2023-2024/NT230/Tools/pe_parser/pe_parser")
from pe_parser.hexadecimal_parser import HexParser


opcodes_vocabulary_mapping = {"UNK": 0, "NONE": 1, "PAD": 2, "andps": 3, "psubusb": 4, "por": 5, "jnz": 6, "fxch4": 7, "pabsw": 8, "bsr": 9, "setl": 10, "paddw": 11, "fxsave": 12, "vmwrite": 13, "bound": 14, "jo": 15, "movhps": 16, "into": 17, "vmovd": 18, "pxor": 19, "vinsertf128": 20, "vmovdqa": 21, "paddsw": 22, "fist": 23, "orps": 24, "vmovapd": 25, "paddusw": 26, "roundps": 27, "fsub": 28, "bsf": 29, "fcmovu": 30, "vorpd": 31, "packuswb": 32, "unpckhpd": 33, "shufpd": 34, "pminuw": 35, "psubq": 36, "arpl": 37, "pandn": 38, "extractps": 39, "vpunpckhbw": 40, "vcvtss2sd": 41, "vpmullw": 42, "bswap": 43, "rcr": 44, "pmovzxbd": 45, "movsd": 46, "lddqu": 47, "movhpd": 48, "packusdw": 49, "jns": 50, "pshuflw": 51, "jnb": 52, "psllw": 53, "cvttps2dq": 54, "cmova": 55, "psrlq": 56, "setnbe": 57, "fcmovbe": 58, "leave": 59, "punpckldq": 60, "ffreep": 61, "phaddd": 62, "cmovb": 63, "insertps": 64, "scas": 65, "cmpps": 66, "sub": 67, "punpckhqdq": 68, "xchg": 69, "fcmovnbe": 70, "rol": 71, "pmulhrsw": 72, "fstsw": 73, "movnti": 74, "jnp": 75, "setz": 76, "int": 77, "minsd": 78, "paddusb": 79, "mulps": 80, "fcompp": 81, "vextractf128": 82, "lar": 83, "psrld": 84, "seto": 85, "vperm2f128": 86, "js": 87, "fadd": 88, "not": 89, "jmp": 90, "lss": 91, "paddq": 92, "fcmovnb": 93, "pfsubr": 94, "psrlw": 95, "punpckhdq": 96, "sqrtss": 97, "jg": 98, "pavgw": 99, "rcpss": 100, "cvtsd2si": 101, "pmaddwd": 102, "ucomisd": 103, "btr": 104, "cmovbe": 105, "vmovsd": 106, "jge": 107, "cmpneqps": 108, "vpaddw": 109, "sbb": 110, "cmovp": 111, "fcomi": 112, "idiv": 113, "pavgusb": 114, "jl": 115, "psubusw": 116, "fsubr": 117, "btc": 118, "pfadd": 119, "pshufb": 120, "addps": 121, "pand": 122, "imul": 123, "lidt": 124, "pshufhw": 125, "movaps": 126, "fild": 127, "lfs": 128, "addsd": 129, "lgs": 130, "addpd": 131, "vmptrst": 132, "vpcmpeqw": 133, "pshufd": 134, "fidivr": 135, "divss": 136, "prefetcht2": 137, "psrldq": 138, "shl": 139, "psraw": 140, "cvtdq2pd": 141, "pcmpgtw": 142, "vmovddup": 143, "subpd": 144, "movsx": 145, "vpcmpgtw": 146, "xor": 147, "fmul": 148, "cmp": 149, "movss": 150, "retnw": 151, "vpextrw": 152, "femms": 153, "rcl": 154, "wait": 155, "pslld": 156, "orpd": 157, "vunpckhps": 158, "pinsrd": 159, "xbegin": 160, "inc": 161, "ficom": 162, "sgdt": 163, "test": 164, "setbe": 165, "mpsadbw": 166, "prefetchnta": 167, "cmpleps": 168, "pcmpeqd": 169, "cmovl": 170, "ffree": 171, "fdivp": 172, "cmpltps": 173, "cvttsd2si": 174, "pmaxub": 175, "vsqrtpd": 176, "fistp": 177, "fnstenv": 178, "punpcklbw": 179, "vpmaddwd": 180, "fxch": 181, "cvtpd2ps": 182, "ucomiss": 183, "setno": 184, "sar": 185, "maxps": 186, "pmuludq": 187, "cvtps2pd": 188, "vpaddusw": 189, "fsubrp": 190, "movlps": 191, "fld1": 192, "jb": 193, "and": 194, "jbe": 195, "pcmpeqw": 196, "fnsave": 197, "cmovo": 198, "out": 199, "frstor": 200, "ldmxcsr": 201, "div": 202, "cvtpi2ps": 203, "mulpd": 204, "add": 205, "pextrw": 206, "movq": 207, "pextrb": 208, "punpcklwd": 209, "shufps": 210, "fdivr": 211, "cvtsd2ss": 212, "adc": 213, "movq2dq": 214, "cmpltpd": 215, "paddb": 216, "cmps": 217, "addss": 218, "vpunpckldq": 219, "pmovzxwd": 220, "vaddps": 221, "cmovno": 222, "pfmul": 223, "pmaxsw": 224, "subss": 225, "cvtsi2ss": 226, "subsd": 227, "cdq": 228, "stmxcsr": 229, "vmptrld": 230, "setnl": 231, "shrd": 232, "push": 233, "or": 234, "fucompp": 235, "cvtdq2ps": 236, "vpshufhw": 237, "fnstsw": 238, "pmulhw": 239, "vmulps": 240, "xlat": 241, "cvtsi2sd": 242, "cmovnp": 243, "subps": 244, "pslldq": 245, "ror": 246, "fcmovne": 247, "pfrcpit1": 248, "mulss": 249, "psadbw": 250, "movmskps": 251, "fcomp": 252, "fnstcw": 253, "jp": 254, "divsd": 255, "movhlps": 256, "fmulp": 257, "fdiv": 258, "pfrcpit2": 259, "jno": 260, "setnz": 261, "fld": 262, "in": 263, "paddd": 264, "cmpeqsd": 265, "str": 266, "lds": 267, "setnb": 268, "aam": 269, "packsswb": 270, "sets": 271, "pminsw": 272, "vdivsd": 273, "xorpd": 274, "faddp": 275, "lldt": 276, "andnpd": 277, "pmovmskb": 278, "fucomp": 279, "retn": 280, "mov": 281, "fsubp": 282, "sqrtsd": 283, "clflush": 284, "pshufw": 285, "vrcpss": 286, "enter": 287, "vsubps": 288, "psubsb": 289, "fstp": 290, "movntdq": 291, "punpckhwd": 292, "cvtps2pi": 293, "fiadd": 294, "pfcmpge": 295, "setnp": 296, "pfrcp": 297, "psubw": 298, "movdq2q": 299, "lock": 300, "cmpsw": 301, "phaddw": 302, "sldt": 303, "pfsub": 304, "comisd": 305, "punpcklqdq": 306, "vcvttpd2dq": 307, "loopne": 308, "pop": 309, "lods": 310, "xadd": 311, "pblendvb": 312, "pcmpgtd": 313, "xorps": 314, "fcmovb": 315, "cmovge": 316, "movups": 317, "comiss": 318, "vdivps": 319, "pfnacc": 320, "fidiv": 321, "fucomi": 322, "fldenv": 323, "vmovaps": 324, "movntps": 325, "prefetchw": 326, "pmaddubsw": 327, "psubd": 328, "cmovns": 329, "pmulhuw": 330, "cmovs": 331, "setb": 332, "movlhps": 333, "movdqu": 334, "movzx": 335, "movmskpd": 336, "setle": 337, "psubsw": 338, "vshufps": 339, "prefetcht0": 340, "cmpnlesd": 341, "loop": 342, "cmpltsd": 343, "pswapd": 344, "vandnpd": 345, "unpcklpd": 346, "cld": 347, "cmovle": 348, "fldcw": 349, "lea": 350, "cmpxchg": 351, "fldz": 352, "vminsd": 353, "fcom": 354, "jz": 355, "cmpnlepd": 356, "shr": 357, "pminub": 358, "paddsb": 359, "fimul": 360, "vpsrad": 361, "rsqrtps": 362, "fxch7": 363, "minps": 364, "psubb": 365, "nop": 366, "call": 367, "minss": 368, "cvtss2sd": 369, "cvttps2pi": 370, "psrad": 371, "movntq": 372, "divps": 373, "vmovdqu": 374, "shld": 375, "cmplesd": 376, "stos": 377, "cvtss2si": 378, "vblendps": 379, "bts": 380, "mul": 381, "fisub": 382, "fcmove": 383, "retf": 384, "andpd": 385, "pinsrw": 386, "cmovg": 387, "pmullw": 388, "retfw": 389, "rcpps": 390, "vmread": 391, "sqrtps": 392, "maxss": 393, "jcxz": 394, "lsl": 395, "vpsllw": 396, "fisttp": 397, "packssdw": 398, "unpckhps": 399, "aad": 400, "lgdt": 401, "bt": 402, "vpermilps": 403, "pfmax": 404, "pfcmpgt": 405, "fucom": 406, "unpcklps": 407, "fbstp": 408, "psllq": 409, "movapd": 410, "palignr": 411, "fst": 412, "align": 413, "pextrd": 414, "vxorps": 415, "cvtps2dq": 416, "dec": 417, "ja": 418, "sal": 419, "movlpd": 420, "fisubr": 421, "phminposuw": 422, "andnps": 423, "pi2fd": 424, "movdqa": 425, "cmpxchg8b": 426, "vunpcklps": 427, "setnle": 428, "pf2id": 429, "fbld": 430, "jecxz": 431, "ficomp": 432, "punpckhbw": 433, "movd": 434, "fdivrp": 435, "loope": 436, "setp": 437, "mulsd": 438, "rsqrtss": 439, "pavgb": 440, "vmovups": 441, "fcmovnu": 442, "cvttss2si": 443, "les": 444, "xabort": 445, "neg": 446, "cmpneqpd": 447, "vpshuflw": 448, "pcmpeqb": 449, "setns": 450, "jle": 451, "pcmpgtb": 452, "dw": 453, "db": 454, "dd": 455, "stop_line": 456, "loc_": 457, "sub_": 458, "endp": 459}
bytes_vocabulary_mapping = {"0": 0, "1": 1, "2": 2, "3": 3, "4": 4, "5": 5, "6": 6, "7": 7, "8": 8, "9": 9, "A": 10, "B": 11, "C": 12, "D": 13, "E": 14, "F": 15, "10": 16, "11": 17, "12": 18, "13": 19, "14": 20, "15": 21, "16": 22, "17": 23, "18": 24, "19": 25, "1A": 26, "1B": 27, "1C": 28, "1D": 29, "1E": 30, "1F": 31, "20": 32, "21": 33, "22": 34, "23": 35, "24": 36, "25": 37, "26": 38, "27": 39, "28": 40, "29": 41, "2A": 42, "2B": 43, "2C": 44, "2D": 45, "2E": 46, "2F": 47, "30": 48, "31": 49, "32": 50, "33": 51, "34": 52, "35": 53, "36": 54, "37": 55, "38": 56, "39": 57, "3A": 58, "3B": 59, "3C": 60, "3D": 61, "3E": 62, "3F": 63, "40": 64, "41": 65, "42": 66, "43": 67, "44": 68, "45": 69, "46": 70, "47": 71, "48": 72, "49": 73, "4A": 74, "4B": 75, "4C": 76, "4D": 77, "4E": 78, "4F": 79, "50": 80, "51": 81, "52": 82, "53": 83, "54": 84, "55": 85, "56": 86, "57": 87, "58": 88, "59": 89, "5A": 90, "5B": 91, "5C": 92, "5D": 93, "5E": 94, "5F": 95, "60": 96, "61": 97, "62": 98, "63": 99, "64": 100, "65": 101, "66": 102, "67": 103, "68": 104, "69": 105, "6A": 106, "6B": 107, "6C": 108, "6D": 109, "6E": 110, "6F": 111, "70": 112, "71": 113, "72": 114, "73": 115, "74": 116, "75": 117, "76": 118, "77": 119, "78": 120, "79": 121, "7A": 122, "7B": 123, "7C": 124, "7D": 125, "7E": 126, "7F": 127, "80": 128, "81": 129, "82": 130, "83": 131, "84": 132, "85": 133, "86": 134, "87": 135, "88": 136, "89": 137, "8A": 138, "8B": 139, "8C": 140, "8D": 141, "8E": 142, "8F": 143, "90": 144, "91": 145, "92": 146, "93": 147, "94": 148, "95": 149, "96": 150, "97": 151, "98": 152, "99": 153, "9A": 154, "9B": 155, "9C": 156, "9D": 157, "9E": 158, "9F": 159, "A0": 160, "A1": 161, "A2": 162, "A3": 163, "A4": 164, "A5": 165, "A6": 166, "A7": 167, "A8": 168, "A9": 169, "AA": 170, "AB": 171, "AC": 172, "AD": 173, "AE": 174, "AF": 175, "B0": 176, "B1": 177, "B2": 178, "B3": 179, "B4": 180, "B5": 181, "B6": 182, "B7": 183, "B8": 184, "B9": 185, "BA": 186, "BB": 187, "BC": 188, "BD": 189, "BE": 190, "BF": 191, "C0": 192, "C1": 193, "C2": 194, "C3": 195, "C4": 196, "C5": 197, "C6": 198, "C7": 199, "C8": 200, "C9": 201, "CA": 202, "CB": 203, "CC": 204, "CD": 205, "CE": 206, "CF": 207, "D0": 208, "D1": 209, "D2": 210, "D3": 211, "D4": 212, "D5": 213, "D6": 214, "D7": 215, "D8": 216, "D9": 217, "DA": 218, "DB": 219, "DC": 220, "DD": 221, "DE": 222, "DF": 223, "E0": 224, "E1": 225, "E2": 226, "E3": 227, "E4": 228, "E5": 229, "E6": 230, "E7": 231, "E8": 232, "E9": 233, "EA": 234, "EB": 235, "EC": 236, "ED": 237, "EE": 238, "EF": 239, "F0": 240, "F1": 241, "F2": 242, "F3": 243, "F4": 244, "F5": 245, "F6": 246, "F7": 247, "F8": 248, "F9": 249, "FA": 250, "FB": 251, "FC": 252, "FD": 253, "FE": 254, "FF": 255, "??": 256, "PAD": 257}



def extract_opcode_features(asm_filepath,  opcodes=None):
    """
    Extract the frequences of a subset of 93 operation codes  based either on their commonness, or on their frequent
    use in malicious applications,

    Parameters
    ----------
        opcodes: list
            List of opcodes

    Return
    ---------
        opcode_features: collections.OrderedDict()
            Dictionary of features
    """
    if opcodes is None:
        opcodes = []
        with open("/content/drive/MyDrive/HK2_2023-2024/NT230/Tools/pe_parser/pe_parser/vocabulary/small_subset_opcodes.txt") as opcodes_file:
            lines = opcodes_file.readlines()
            for line in lines:
                opcode = line.strip()
                opcodes.append(opcode)
        """
        opcodes = ['add','al','bt','call','cdq','cld','cli','cmc','cmp','const','cwd','daa','db'
                ,'dd','dec','dw','endp','ends','faddp','fchs','fdiv','fdivp','fdivr','fild'
                ,'fistp','fld','fstcw','fstcwimul','fstp','fword','fxch','imul','in','inc'
                ,'ins','int','jb','je','jg','jge','jl','jmp','jnb','jno','jnz','jo','jz'
                ,'lea','loope','mov','movzx','mul','near','neg','not','or','out','outs'
                ,'pop','popf','proc','push','pushf','rcl','rcr','rdtsc','rep','ret','retn'
                ,'rol','ror','sal','sar','sbb','scas','setb','setle','setnle','setnz'
                ,'setz','shl','shld','shr','sidt','stc','std','sti','stos','sub','test'
                ,'wait','xchg','xor']
        """
    opcode_features = []
    with open(asm_filepath, "r", encoding="ISO-8859-1") as asm_file:
        asm_code = asm_file.readlines()
        for row in asm_code:
            parts = row.split()

            for opcode in opcodes:
                if opcode in parts:
                    opcode_features.append(opcode)
                    break
    opcode_features
    return opcode_features


def _bytes_feature(value):
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))

def _int64_feature(value):
    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))


def serialize_hydra_example(opcodes, bytes, apis_values, label):
    feature = {
        'opcodes': _bytes_feature(opcodes.encode('UTF-8')),
        'bytes': _bytes_feature(bytes.encode('UTF-8')),
        'APIs': _bytes_feature(apis_values.tobytes()),
        'label': _int64_feature(label)
    }
    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))
    return example_proto.SerializeToString()

def dataset_to_tfrecords(asm_filepath,
                         bytes_filepath,
                         data_extract_api_filepath,
                         tfrecords_filepath,
                         labels_filepath,
                         opcodes_vocabulary_mapping,
                         bytes_vocabulary_mapping,
                         max_mnemonics=50000,
                         max_bytes=2000000):
  tfwriter = tf.io.TFRecordWriter(tfrecords_filepath)
  opcodes_vocabulary_mapping = opcodes_vocabulary_mapping
  bytes_vocabulary_mapping = bytes_vocabulary_mapping

  i = 0
  with open(labels_filepath, "r") as labels_file:
    reader = csv.DictReader(labels_file, fieldnames=["Id", "Class"])
    reader.__next__()
    for row in reader:
      print("{};{}".format(i, row['Id']))

      # extract opcode

      fil_opcode_id = os.path.join(asm_filepath, row['Id'] + ".asm")
      opcode_sequences_list = extract_opcode_features(fil_opcode_id , opcodes=list(opcodes_vocabulary_mapping.keys()))
      if len(opcode_sequences_list) < max_mnemonics:
          while len(opcode_sequences_list) < max_mnemonics:
              opcode_sequences_list.append("PAD")
      else:
          opcode_sequences_list = opcode_sequences_list[:max_mnemonics]

      for j in range(len(opcode_sequences_list)):
        if opcode_sequences_list[j] not in opcodes_vocabulary_mapping.keys():
          opcode_sequences_list[j] = "UNK"

      raw_mnemonics = " ".join(opcode_sequences_list)
      

      #extract bytes
      hex_parser = HexParser(bytes_filepath + '/' + row['Id'] + ".bytes")
      bytes_sequence  = hex_parser.extract_hex_values()
      for k in range(len(bytes_sequence)):
          if bytes_sequence[k] not in bytes_vocabulary_mapping.keys():
              bytes_sequence[k] = "UNK"

      if len(bytes_sequence) < max_bytes:
          while len(bytes_sequence) < max_bytes:
              bytes_sequence.append("PAD")
      else:
          bytes_sequence = bytes_sequence[:max_bytes]
      raw_bytes_sequence = " ".join(bytes_sequence)

      #extract APIs

      df_api = pd.read_csv(data_extract_api_filepath)
      temp = df_api[df_api["Id"]==row['Id']]
      api_vector = temp.drop(columns=['Id']).values.astype(np.float32)

      example = serialize_hydra_example(raw_mnemonics,
                                        raw_bytes_sequence,
                                        api_vector,
                                        int(row['Class']) - 1)
      tfwriter.write(example)
      i += 1

#train dataset
dataset_to_tfrecords(asm_filepath="/content/drive/MyDrive/HK2_2023-2024/NT230/demo_malware/sample/malware/asm",
                         bytes_filepath="/content/drive/MyDrive/HK2_2023-2024/NT230/demo_malware/sample/malware/bytes",
                         data_extract_api_filepath="/content/drive/MyDrive/HK2_2023-2024/NT230/demo_malware/process_data/api/api_call.csv",
                         tfrecords_filepath="/content/drive/MyDrive/HK2_2023-2024/NT230/demo_malware/process_data/hydra/hydra_tf_writer_test",
                         labels_filepath="/content/drive/MyDrive/HK2_2023-2024/NT230/demo_malware/sample/label_test_api.csv",
                         opcodes_vocabulary_mapping=opcodes_vocabulary_mapping,
                         bytes_vocabulary_mapping=bytes_vocabulary_mapping,
                         max_mnemonics=50000,
                         max_bytes=2000000)



