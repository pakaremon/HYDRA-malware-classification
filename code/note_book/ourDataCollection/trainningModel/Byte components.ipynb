{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21712,"status":"ok","timestamp":1716333877610,"user":{"displayName":"Nguyễn Thành Công","userId":"00177559625414537077"},"user_tz":-420},"id":"eSkgtLojeT18","outputId":"4b226f4a-044a-4233-dc7e-48a7edfeb232"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"jdVgRZxGft7X"},"source":[]},{"cell_type":"markdown","metadata":{"id":"wjNl6JsDCALu"},"source":["# Train model Bytescode"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZXGUV_tXxJGs"},"outputs":[],"source":["from sklearn.metrics import confusion_matrix\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16809,"status":"ok","timestamp":1716333896587,"user":{"displayName":"Nguyễn Thành Công","userId":"00177559625414537077"},"user_tz":-420},"id":"kFSwya3_JYE9","outputId":"49930364-453d-4acf-ae6a-9e8a6c8f9dca"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tensorflow_text==2.15.0\n","  Downloading tensorflow_text-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_text==2.15.0) (0.16.1)\n","Requirement already satisfied: tensorflow<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_text==2.15.0) (2.15.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (24.3.25)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (3.9.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (18.1.1)\n","Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (0.2.0)\n","Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (1.25.2)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (24.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (4.11.0)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (0.37.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (1.63.0)\n","Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (2.15.2)\n","Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (2.15.0)\n","Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (2.15.0)\n","Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub>=0.13.0->tensorflow_text==2.15.0) (2.15.1)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (0.43.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (2.27.0)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (1.2.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (3.6)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (2.31.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (3.0.3)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (5.3.3)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (0.4.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (2024.2.2)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (2.1.5)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (0.6.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (3.2.2)\n","Installing collected packages: tensorflow_text\n","Successfully installed tensorflow_text-2.15.0\n"]}],"source":["%pip install tensorflow_text==2.15.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KDPpcPlSBWKB"},"outputs":[],"source":["import tensorflow as tf\n","import tensorflow_text as text\n","\n","\n","def _parse_tfrecord_function(example, lookup_table):\n","    example_fmt = {\n","            'bytes': tf.io.FixedLenFeature([], tf.string),\n","            'label': tf.io.FixedLenFeature([], tf.int64)\n","        }\n","    parsed = tf.io.parse_single_example(example, example_fmt)\n","    tokenizer = text.WhitespaceTokenizer()\n","    tokens = tokenizer.tokenize(parsed['bytes'])\n","    IDs = lookup_table.lookup(tokens)\n","\n","    return IDs, parsed['label']\n","\n","\n","def make_dataset(filepath, lookup_table, SHUFFLE_BUFFER_SIZE=1024, BATCH_SIZE=32, EPOCHS=5):\n","    dataset = tf.data.TFRecordDataset(filepath)\n","    dataset = dataset.shuffle(SHUFFLE_BUFFER_SIZE)\n","    dataset = dataset.repeat(EPOCHS)\n","    dataset = dataset.map(lambda x: _parse_tfrecord_function(x, lookup_table))\n","    dataset = dataset.batch(batch_size=BATCH_SIZE)\n","    return dataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1anHqpgGCKdD"},"outputs":[],"source":["parameters={\n","  \"output\": 9,\n","  \"E\": 8,\n","  \"max_bytes_values\":2000000,\n","  \"V\":259,\n","  \"kernel_sizes\":[\n","    32,\n","    32,\n","    16,\n","    16\n","  ],\n","  \"strides\":[\n","    4,\n","    4,\n","    8,\n","    8\n","  ],\n","  \"num_filters\": [\n","    48,\n","    96,\n","    128,\n","    192\n","  ],\n","  \"max_pool_size\":4,\n","  \"hidden\":[\n","    192,\n","    160,\n","    128\n","  ],\n","  \"dropout_rate\":0.5,\n","  \"learning_rate\":0.0005,\n","  \"batch_size\":1,\n","  \"buffer_size\":4,\n","  \"epochs\":30,\n","  \"gpu\":\"0\"\n","\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"duV7HqwhDvwo"},"outputs":[],"source":["def create_lookup_table(vocabulary_mapping, num_oov_buckets):\n","    keys = [k for k in vocabulary_mapping.keys()]\n","    values = [tf.constant(vocabulary_mapping[k], dtype=tf.int64) for k in keys]\n","\n","    table = tf.lookup.StaticVocabularyTable(\n","        tf.lookup.KeyValueTensorInitializer(\n","            keys=keys,\n","            values=values\n","        ),\n","        num_oov_buckets\n","    )\n","    return table"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E_LWcsc-CNGq"},"outputs":[],"source":["vocabulary_mapping={\"0\": 0, \"1\": 1, \"2\": 2, \"3\": 3, \"4\": 4, \"5\": 5, \"6\": 6, \"7\": 7, \"8\": 8, \"9\": 9, \"A\": 10, \"B\": 11, \"C\": 12, \"D\": 13, \"E\": 14, \"F\": 15, \"10\": 16, \"11\": 17, \"12\": 18, \"13\": 19, \"14\": 20, \"15\": 21, \"16\": 22, \"17\": 23, \"18\": 24, \"19\": 25, \"1A\": 26, \"1B\": 27, \"1C\": 28, \"1D\": 29, \"1E\": 30, \"1F\": 31, \"20\": 32, \"21\": 33, \"22\": 34, \"23\": 35, \"24\": 36, \"25\": 37, \"26\": 38, \"27\": 39, \"28\": 40, \"29\": 41, \"2A\": 42, \"2B\": 43, \"2C\": 44, \"2D\": 45, \"2E\": 46, \"2F\": 47, \"30\": 48, \"31\": 49, \"32\": 50, \"33\": 51, \"34\": 52, \"35\": 53, \"36\": 54, \"37\": 55, \"38\": 56, \"39\": 57, \"3A\": 58, \"3B\": 59, \"3C\": 60, \"3D\": 61, \"3E\": 62, \"3F\": 63, \"40\": 64, \"41\": 65, \"42\": 66, \"43\": 67, \"44\": 68, \"45\": 69, \"46\": 70, \"47\": 71, \"48\": 72, \"49\": 73, \"4A\": 74, \"4B\": 75, \"4C\": 76, \"4D\": 77, \"4E\": 78, \"4F\": 79, \"50\": 80, \"51\": 81, \"52\": 82, \"53\": 83, \"54\": 84, \"55\": 85, \"56\": 86, \"57\": 87, \"58\": 88, \"59\": 89, \"5A\": 90, \"5B\": 91, \"5C\": 92, \"5D\": 93, \"5E\": 94, \"5F\": 95, \"60\": 96, \"61\": 97, \"62\": 98, \"63\": 99, \"64\": 100, \"65\": 101, \"66\": 102, \"67\": 103, \"68\": 104, \"69\": 105, \"6A\": 106, \"6B\": 107, \"6C\": 108, \"6D\": 109, \"6E\": 110, \"6F\": 111, \"70\": 112, \"71\": 113, \"72\": 114, \"73\": 115, \"74\": 116, \"75\": 117, \"76\": 118, \"77\": 119, \"78\": 120, \"79\": 121, \"7A\": 122, \"7B\": 123, \"7C\": 124, \"7D\": 125, \"7E\": 126, \"7F\": 127, \"80\": 128, \"81\": 129, \"82\": 130, \"83\": 131, \"84\": 132, \"85\": 133, \"86\": 134, \"87\": 135, \"88\": 136, \"89\": 137, \"8A\": 138, \"8B\": 139, \"8C\": 140, \"8D\": 141, \"8E\": 142, \"8F\": 143, \"90\": 144, \"91\": 145, \"92\": 146, \"93\": 147, \"94\": 148, \"95\": 149, \"96\": 150, \"97\": 151, \"98\": 152, \"99\": 153, \"9A\": 154, \"9B\": 155, \"9C\": 156, \"9D\": 157, \"9E\": 158, \"9F\": 159, \"A0\": 160, \"A1\": 161, \"A2\": 162, \"A3\": 163, \"A4\": 164, \"A5\": 165, \"A6\": 166, \"A7\": 167, \"A8\": 168, \"A9\": 169, \"AA\": 170, \"AB\": 171, \"AC\": 172, \"AD\": 173, \"AE\": 174, \"AF\": 175, \"B0\": 176, \"B1\": 177, \"B2\": 178, \"B3\": 179, \"B4\": 180, \"B5\": 181, \"B6\": 182, \"B7\": 183, \"B8\": 184, \"B9\": 185, \"BA\": 186, \"BB\": 187, \"BC\": 188, \"BD\": 189, \"BE\": 190, \"BF\": 191, \"C0\": 192, \"C1\": 193, \"C2\": 194, \"C3\": 195, \"C4\": 196, \"C5\": 197, \"C6\": 198, \"C7\": 199, \"C8\": 200, \"C9\": 201, \"CA\": 202, \"CB\": 203, \"CC\": 204, \"CD\": 205, \"CE\": 206, \"CF\": 207, \"D0\": 208, \"D1\": 209, \"D2\": 210, \"D3\": 211, \"D4\": 212, \"D5\": 213, \"D6\": 214, \"D7\": 215, \"D8\": 216, \"D9\": 217, \"DA\": 218, \"DB\": 219, \"DC\": 220, \"DD\": 221, \"DE\": 222, \"DF\": 223, \"E0\": 224, \"E1\": 225, \"E2\": 226, \"E3\": 227, \"E4\": 228, \"E5\": 229, \"E6\": 230, \"E7\": 231, \"E8\": 232, \"E9\": 233, \"EA\": 234, \"EB\": 235, \"EC\": 236, \"ED\": 237, \"EE\": 238, \"EF\": 239, \"F0\": 240, \"F1\": 241, \"F2\": 242, \"F3\": 243, \"F4\": 244, \"F5\": 245, \"F6\": 246, \"F7\": 247, \"F8\": 248, \"F9\": 249, \"FA\": 250, \"FB\": 251, \"FC\": 252, \"FD\": 253, \"FE\": 254, \"FF\": 255, \"??\": 256, \"PAD\": 257}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GCtSm6P7DseB"},"outputs":[],"source":["lookup_table = create_lookup_table(vocabulary_mapping=vocabulary_mapping, num_oov_buckets=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IS0pnP71CR5o"},"outputs":[],"source":["# import tensorflow as tf\n","\n","\n","class DeepConv(tf.keras.Model):\n","    def __init__(self, parameters):\n","        super(DeepConv, self).__init__()\n","        self.parameters = parameters\n","\n","    def build(self, input_shapes):\n","        self.emb = tf.keras.layers.Embedding(self.parameters['V'], self.parameters['E'],\n","                                             input_shape=(None, self.parameters['max_bytes_values']))\n","\n","        self.conv_1 = tf.keras.layers.Conv2D(filters=self.parameters['num_filters'][0],\n","                                                  kernel_size=[self.parameters['kernel_sizes'][0],\n","                                                               self.parameters['E']],\n","                                                  strides=(self.parameters['strides'][0],1),\n","                                                  data_format='channels_last',\n","                                                  use_bias=True,\n","                                                  activation=\"relu\")\n","\n","        self.conv_2 = tf.keras.layers.Conv2D(filters=self.parameters['num_filters'][1],\n","                                                  kernel_size=[self.parameters['kernel_sizes'][1],\n","                                                               1],\n","                                                  strides=(self.parameters['strides'][1],1),\n","                                                  data_format='channels_last',\n","                                                  use_bias=True,\n","                                                  activation=\"relu\")\n","\n","        self.max_pool_1 = tf.keras.layers.MaxPooling2D(pool_size=(self.parameters['max_pool_size'], 1))\n","\n","        self.conv_3 = tf.keras.layers.Conv2D(filters=self.parameters['num_filters'][2],\n","                                                 kernel_size=[self.parameters['kernel_sizes'][2],\n","                                                              1],\n","                                                 strides=(self.parameters['strides'][2], 1),\n","                                                 data_format='channels_last',\n","                                                 use_bias=True,\n","                                                 activation=\"relu\")\n","\n","        self.conv_4 = tf.keras.layers.Conv2D(filters=self.parameters['num_filters'][3],\n","                                                 kernel_size=[self.parameters['kernel_sizes'][3],\n","                                                              1],\n","                                                 strides=(self.parameters['strides'][3], 1),\n","                                                 data_format='channels_last',\n","                                                 use_bias=True,\n","                                                 activation=\"relu\")\n","\n","        self.global_avg_pool = tf.keras.layers.GlobalAvgPool2D()\n","\n","        self.drop_1 = tf.keras.layers.Dropout(self.parameters[\"dropout_rate\"])\n","        self.dense_1 =  tf.keras.layers.Dense(self.parameters['hidden'][0],\n","                                                           activation=\"selu\")\n","\n","        self.drop_2 = tf.keras.layers.Dropout(self.parameters[\"dropout_rate\"])\n","        self.dense_2 = tf.keras.layers.Dense(self.parameters['hidden'][1],\n","                                                       activation=\"selu\")\n","\n","        self.drop_3 = tf.keras.layers.Dropout(self.parameters[\"dropout_rate\"])\n","\n","        self.dense_3 = tf.keras.layers.Dense(self.parameters['hidden'][2],\n","                                                       activation=\"selu\")\n","\n","        self.drop_4 = tf.keras.layers.Dropout(self.parameters[\"dropout_rate\"])\n","        self.out = tf.keras.layers.Dense(self.parameters['output'],\n","                                           activation=\"softmax\")\n","\n","\n","    def call(self, input_tensor, training=False):\n","        emb = self.emb(input_tensor)\n","        emb_expanded = tf.keras.backend.expand_dims(emb, axis=-1)\n","\n","        conv_1 = self.conv_1(emb_expanded)\n","        conv_2 = self.conv_2(conv_1)\n","\n","        max_pool_1 = self.max_pool_1(conv_2)\n","\n","        conv_3 = self.conv_3(max_pool_1)\n","        conv_4 = self.conv_4(conv_3)\n","\n","        features = self.global_avg_pool(conv_4)\n","\n","        drop_1 = self.drop_1(features, training=training)\n","        dense_1 = self.dense_1(drop_1)\n","\n","        drop_2 = self.drop_2(dense_1, training=training)\n","        dense_2 = self.dense_2(drop_2)\n","\n","        drop_3 = self.drop_3(dense_2, training=training)\n","        dense_3 = self.dense_3(drop_3)\n","\n","        drop_4 = self.drop_4(dense_3, training=training)\n","        output = self.out(drop_4)\n","\n","        return output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3DrzyxxhCfli"},"outputs":[],"source":["file_reader=\"/content/drive/MyDrive/HK2_2023-2024/NT230/demo_malware/process_data/bytes/bytes_tf_record_train\""]},{"cell_type":"markdown","metadata":{"id":"IwCjmo9bbLiC"},"source":["https://www.tensorflow.org/tutorials/keras/save_and_load"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14132,"status":"ok","timestamp":1716333915560,"user":{"displayName":"Nguyễn Thành Công","userId":"00177559625414537077"},"user_tz":-420},"id":"wmXKBbWNbFGu","outputId":"3c8ed5ce-ea7c-485e-9948-40622abdae25"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (6.0.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (3.9.0)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from h5py) (1.25.2)\n"]}],"source":["!pip install pyyaml h5py  # Required to save models in HDF5 format"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WUnN7oi6CeD2","outputId":"07539333-9778-4d96-b12e-bfaf92abf7ff"},"outputs":[{"output_type":"stream","name":"stdout","text":["LOADING WEIGHTS!!!!\n","/content/drive/MyDrive/HK2_2023-2024/NT230/demo_malware/result_training/models/DeepConv/model_ep_1.ckpt\n","Current epoch: 0\n","Iteration step: 0; Loss: 1.532, Accuracy: 0.000%\n","Iteration step: 1; Loss: 1.323, Accuracy: 50.000%\n","Iteration step: 2; Loss: 1.325, Accuracy: 33.333%\n","Iteration step: 3; Loss: 1.254, Accuracy: 50.000%\n","Iteration step: 4; Loss: 1.200, Accuracy: 60.000%\n","Iteration step: 5; Loss: 1.433, Accuracy: 50.000%\n","Iteration step: 6; Loss: 1.565, Accuracy: 42.857%\n","Iteration step: 7; Loss: 1.477, Accuracy: 50.000%\n","Iteration step: 8; Loss: 1.597, Accuracy: 44.444%\n","Iteration step: 9; Loss: 1.681, Accuracy: 40.000%\n","Iteration step: 10; Loss: 1.606, Accuracy: 45.455%\n","Iteration step: 11; Loss: 1.544, Accuracy: 50.000%\n","Iteration step: 12; Loss: 1.489, Accuracy: 53.846%\n","Iteration step: 13; Loss: 1.441, Accuracy: 57.143%\n","Iteration step: 14; Loss: 1.484, Accuracy: 53.333%\n","Iteration step: 15; Loss: 1.558, Accuracy: 50.000%\n","Iteration step: 16; Loss: 1.561, Accuracy: 47.059%\n","Iteration step: 17; Loss: 1.565, Accuracy: 44.444%\n","Iteration step: 18; Loss: 1.524, Accuracy: 47.368%\n","Iteration step: 19; Loss: 1.486, Accuracy: 50.000%\n","Iteration step: 20; Loss: 1.452, Accuracy: 52.381%\n","Iteration step: 21; Loss: 1.420, Accuracy: 54.545%\n","Iteration step: 22; Loss: 1.473, Accuracy: 52.174%\n","Iteration step: 23; Loss: 1.491, Accuracy: 50.000%\n","Iteration step: 24; Loss: 1.536, Accuracy: 48.000%\n","Iteration step: 25; Loss: 1.504, Accuracy: 50.000%\n","Iteration step: 26; Loss: 1.542, Accuracy: 48.148%\n","Iteration step: 27; Loss: 1.513, Accuracy: 50.000%\n","Iteration step: 28; Loss: 1.544, Accuracy: 48.276%\n","Iteration step: 29; Loss: 1.641, Accuracy: 46.667%\n","Iteration step: 30; Loss: 1.646, Accuracy: 45.161%\n","Iteration step: 31; Loss: 1.672, Accuracy: 43.750%\n","Iteration step: 32; Loss: 1.646, Accuracy: 45.455%\n","Iteration step: 33; Loss: 1.622, Accuracy: 47.059%\n","Iteration step: 34; Loss: 1.627, Accuracy: 45.714%\n","Iteration step: 35; Loss: 1.639, Accuracy: 44.444%\n","Iteration step: 36; Loss: 1.646, Accuracy: 43.243%\n","Iteration step: 37; Loss: 1.626, Accuracy: 44.737%\n","Iteration step: 38; Loss: 1.607, Accuracy: 46.154%\n","Iteration step: 39; Loss: 1.611, Accuracy: 45.000%\n","Iteration step: 40; Loss: 1.594, Accuracy: 46.341%\n","Iteration step: 41; Loss: 1.603, Accuracy: 45.238%\n","Iteration step: 42; Loss: 1.586, Accuracy: 46.512%\n","Iteration step: 43; Loss: 1.594, Accuracy: 45.455%\n","Iteration step: 44; Loss: 1.578, Accuracy: 46.667%\n","Iteration step: 45; Loss: 1.563, Accuracy: 47.826%\n","Iteration step: 46; Loss: 1.583, Accuracy: 46.809%\n","Iteration step: 47; Loss: 1.568, Accuracy: 47.917%\n","Iteration step: 48; Loss: 1.553, Accuracy: 48.980%\n","Iteration step: 49; Loss: 1.539, Accuracy: 50.000%\n","Iteration step: 50; Loss: 1.545, Accuracy: 49.020%\n","Iteration step: 51; Loss: 1.530, Accuracy: 50.000%\n","Iteration step: 52; Loss: 1.536, Accuracy: 49.057%\n","Iteration step: 53; Loss: 1.522, Accuracy: 50.000%\n","Iteration step: 54; Loss: 1.507, Accuracy: 50.909%\n","Iteration step: 55; Loss: 1.493, Accuracy: 51.786%\n","Iteration step: 56; Loss: 1.503, Accuracy: 50.877%\n","Iteration step: 57; Loss: 1.489, Accuracy: 51.724%\n","Iteration step: 58; Loss: 1.474, Accuracy: 52.542%\n","Iteration step: 59; Loss: 1.482, Accuracy: 51.667%\n","Iteration step: 60; Loss: 1.466, Accuracy: 52.459%\n","Iteration step: 61; Loss: 1.451, Accuracy: 53.226%\n","Iteration step: 62; Loss: 1.460, Accuracy: 52.381%\n","Iteration step: 63; Loss: 1.445, Accuracy: 53.125%\n","Iteration step: 64; Loss: 1.469, Accuracy: 52.308%\n","Iteration step: 65; Loss: 1.454, Accuracy: 53.030%\n","Iteration step: 66; Loss: 1.439, Accuracy: 53.731%\n","Iteration step: 67; Loss: 1.424, Accuracy: 54.412%\n","Iteration step: 68; Loss: 1.409, Accuracy: 55.072%\n","Iteration step: 69; Loss: 1.394, Accuracy: 55.714%\n","Iteration step: 70; Loss: 1.379, Accuracy: 56.338%\n","Iteration step: 71; Loss: 1.365, Accuracy: 56.944%\n","Iteration step: 72; Loss: 1.389, Accuracy: 56.164%\n","Iteration step: 73; Loss: 1.413, Accuracy: 55.405%\n","Iteration step: 74; Loss: 1.438, Accuracy: 54.667%\n","Iteration step: 75; Loss: 1.423, Accuracy: 55.263%\n","Iteration step: 76; Loss: 1.409, Accuracy: 55.844%\n","Iteration step: 77; Loss: 1.431, Accuracy: 55.128%\n","Iteration step: 78; Loss: 1.441, Accuracy: 54.430%\n","Iteration step: 79; Loss: 1.427, Accuracy: 55.000%\n","Iteration step: 80; Loss: 1.415, Accuracy: 55.556%\n","Iteration step: 81; Loss: 1.423, Accuracy: 54.878%\n","Iteration step: 82; Loss: 1.411, Accuracy: 55.422%\n","Iteration step: 83; Loss: 1.419, Accuracy: 54.762%\n","Iteration step: 84; Loss: 1.426, Accuracy: 54.118%\n","Iteration step: 85; Loss: 1.432, Accuracy: 53.488%\n","Iteration step: 86; Loss: 1.421, Accuracy: 54.023%\n","Iteration step: 87; Loss: 1.433, Accuracy: 53.409%\n","Iteration step: 88; Loss: 1.437, Accuracy: 52.809%\n","Iteration step: 89; Loss: 1.428, Accuracy: 53.333%\n","Iteration step: 90; Loss: 1.440, Accuracy: 52.747%\n","Iteration step: 91; Loss: 1.450, Accuracy: 52.174%\n","Iteration step: 92; Loss: 1.461, Accuracy: 51.613%\n","Iteration step: 93; Loss: 1.454, Accuracy: 52.128%\n","Iteration step: 94; Loss: 1.454, Accuracy: 51.579%\n","Iteration step: 95; Loss: 1.463, Accuracy: 51.042%\n","Iteration step: 96; Loss: 1.457, Accuracy: 51.546%\n","Iteration step: 97; Loss: 1.451, Accuracy: 52.041%\n","Iteration step: 98; Loss: 1.445, Accuracy: 52.525%\n","Iteration step: 99; Loss: 1.440, Accuracy: 53.000%\n","Iteration step: 100; Loss: 1.448, Accuracy: 52.475%\n","Iteration step: 101; Loss: 1.443, Accuracy: 52.941%\n","Iteration step: 102; Loss: 1.468, Accuracy: 52.427%\n","Iteration step: 103; Loss: 1.462, Accuracy: 52.885%\n","Iteration step: 104; Loss: 1.462, Accuracy: 52.381%\n","Iteration step: 105; Loss: 1.457, Accuracy: 52.830%\n","Iteration step: 106; Loss: 1.452, Accuracy: 53.271%\n","Iteration step: 107; Loss: 1.447, Accuracy: 53.704%\n","Iteration step: 108; Loss: 1.441, Accuracy: 54.128%\n","Iteration step: 109; Loss: 1.436, Accuracy: 54.545%\n","Iteration step: 110; Loss: 1.430, Accuracy: 54.955%\n","Iteration step: 111; Loss: 1.437, Accuracy: 54.464%\n","Iteration step: 112; Loss: 1.438, Accuracy: 53.982%\n","Iteration step: 113; Loss: 1.432, Accuracy: 54.386%\n","Iteration step: 114; Loss: 1.434, Accuracy: 53.913%\n","Iteration step: 115; Loss: 1.443, Accuracy: 53.448%\n","Iteration step: 116; Loss: 1.436, Accuracy: 53.846%\n","Iteration step: 117; Loss: 1.444, Accuracy: 53.390%\n","Iteration step: 118; Loss: 1.438, Accuracy: 53.782%\n","Iteration step: 119; Loss: 1.432, Accuracy: 54.167%\n","Iteration step: 120; Loss: 1.441, Accuracy: 53.719%\n","Iteration step: 121; Loss: 1.435, Accuracy: 54.098%\n","Iteration step: 122; Loss: 1.443, Accuracy: 53.659%\n","Iteration step: 123; Loss: 1.437, Accuracy: 54.032%\n","Iteration step: 124; Loss: 1.444, Accuracy: 53.600%\n","Iteration step: 125; Loss: 1.438, Accuracy: 53.968%\n","Iteration step: 126; Loss: 1.432, Accuracy: 54.331%\n","Iteration step: 127; Loss: 1.440, Accuracy: 53.906%\n","Iteration step: 128; Loss: 1.433, Accuracy: 54.264%\n","Iteration step: 129; Loss: 1.436, Accuracy: 53.846%\n","Iteration step: 130; Loss: 1.443, Accuracy: 53.435%\n","Iteration step: 131; Loss: 1.437, Accuracy: 53.788%\n","Iteration step: 132; Loss: 1.431, Accuracy: 54.135%\n","Iteration step: 133; Loss: 1.425, Accuracy: 54.478%\n","Iteration step: 134; Loss: 1.419, Accuracy: 54.815%\n","Iteration step: 135; Loss: 1.426, Accuracy: 54.412%\n","Iteration step: 136; Loss: 1.420, Accuracy: 54.745%\n","Iteration step: 137; Loss: 1.427, Accuracy: 54.348%\n","Iteration step: 138; Loss: 1.421, Accuracy: 54.676%\n","Iteration step: 139; Loss: 1.430, Accuracy: 54.286%\n","Iteration step: 140; Loss: 1.423, Accuracy: 54.610%\n","Iteration step: 141; Loss: 1.432, Accuracy: 54.225%\n","Iteration step: 142; Loss: 1.440, Accuracy: 53.846%\n","Iteration step: 143; Loss: 1.444, Accuracy: 53.472%\n","Iteration step: 144; Loss: 1.438, Accuracy: 53.793%\n","Iteration step: 145; Loss: 1.432, Accuracy: 54.110%\n","Iteration step: 146; Loss: 1.426, Accuracy: 54.422%\n","Iteration step: 147; Loss: 1.432, Accuracy: 54.054%\n","Iteration step: 148; Loss: 1.426, Accuracy: 54.362%\n","Iteration step: 149; Loss: 1.421, Accuracy: 54.667%\n","Iteration step: 150; Loss: 1.426, Accuracy: 54.305%\n","Iteration step: 151; Loss: 1.431, Accuracy: 53.947%\n","Iteration step: 152; Loss: 1.425, Accuracy: 54.248%\n","Iteration step: 153; Loss: 1.420, Accuracy: 54.545%\n","Iteration step: 154; Loss: 1.414, Accuracy: 54.839%\n","Iteration step: 155; Loss: 1.420, Accuracy: 54.487%\n","Iteration step: 156; Loss: 1.414, Accuracy: 54.777%\n","Iteration step: 157; Loss: 1.409, Accuracy: 55.063%\n","Iteration step: 158; Loss: 1.403, Accuracy: 55.346%\n","Iteration step: 159; Loss: 1.398, Accuracy: 55.625%\n","Iteration step: 160; Loss: 1.405, Accuracy: 55.280%\n","Iteration step: 161; Loss: 1.400, Accuracy: 55.556%\n","Iteration step: 162; Loss: 1.407, Accuracy: 55.215%\n","Iteration step: 163; Loss: 1.401, Accuracy: 55.488%\n","Iteration step: 164; Loss: 1.406, Accuracy: 55.152%\n","Iteration step: 165; Loss: 1.413, Accuracy: 54.819%\n","Iteration step: 166; Loss: 1.432, Accuracy: 54.491%\n","Iteration step: 167; Loss: 1.427, Accuracy: 54.762%\n","Iteration step: 168; Loss: 1.431, Accuracy: 54.438%\n","Iteration step: 169; Loss: 1.426, Accuracy: 54.706%\n","Iteration step: 170; Loss: 1.430, Accuracy: 54.386%\n","Iteration step: 171; Loss: 1.425, Accuracy: 54.651%\n","Iteration step: 172; Loss: 1.430, Accuracy: 54.335%\n","Iteration step: 173; Loss: 1.426, Accuracy: 54.598%\n","Iteration step: 174; Loss: 1.421, Accuracy: 54.857%\n","Iteration step: 175; Loss: 1.416, Accuracy: 55.114%\n","Iteration step: 176; Loss: 1.412, Accuracy: 55.367%\n","Iteration step: 177; Loss: 1.407, Accuracy: 55.618%\n","Iteration step: 178; Loss: 1.403, Accuracy: 55.866%\n","Iteration step: 179; Loss: 1.406, Accuracy: 55.556%\n","Iteration step: 180; Loss: 1.402, Accuracy: 55.801%\n","Iteration step: 181; Loss: 1.407, Accuracy: 55.495%\n","Iteration step: 182; Loss: 1.402, Accuracy: 55.738%\n","Iteration step: 183; Loss: 1.408, Accuracy: 55.435%\n","Iteration step: 184; Loss: 1.403, Accuracy: 55.676%\n","Iteration step: 185; Loss: 1.407, Accuracy: 55.376%\n","Iteration step: 186; Loss: 1.412, Accuracy: 55.080%\n","Iteration step: 187; Loss: 1.408, Accuracy: 55.319%\n","Iteration step: 188; Loss: 1.403, Accuracy: 55.556%\n","Iteration step: 189; Loss: 1.399, Accuracy: 55.789%\n","Iteration step: 190; Loss: 1.404, Accuracy: 55.497%\n","Iteration step: 191; Loss: 1.409, Accuracy: 55.208%\n","Iteration step: 192; Loss: 1.415, Accuracy: 54.922%\n","Iteration step: 193; Loss: 1.411, Accuracy: 55.155%\n","Iteration step: 194; Loss: 1.406, Accuracy: 55.385%\n","Iteration step: 195; Loss: 1.409, Accuracy: 55.102%\n","Iteration step: 196; Loss: 1.405, Accuracy: 55.330%\n","Iteration step: 197; Loss: 1.410, Accuracy: 55.051%\n","Iteration step: 198; Loss: 1.413, Accuracy: 54.774%\n","Iteration step: 199; Loss: 1.418, Accuracy: 54.500%\n","Iteration step: 200; Loss: 1.422, Accuracy: 54.229%\n","Iteration step: 201; Loss: 1.418, Accuracy: 54.455%\n","Iteration step: 202; Loss: 1.422, Accuracy: 54.187%\n","Iteration step: 203; Loss: 1.426, Accuracy: 53.922%\n","Iteration step: 204; Loss: 1.429, Accuracy: 53.659%\n","Iteration step: 205; Loss: 1.432, Accuracy: 53.398%\n","Iteration step: 206; Loss: 1.435, Accuracy: 53.140%\n","Iteration step: 207; Loss: 1.439, Accuracy: 52.885%\n","Iteration step: 208; Loss: 1.436, Accuracy: 53.110%\n","Iteration step: 209; Loss: 1.438, Accuracy: 52.857%\n","Iteration step: 210; Loss: 1.440, Accuracy: 52.607%\n","Iteration step: 211; Loss: 1.441, Accuracy: 52.358%\n","Iteration step: 212; Loss: 1.444, Accuracy: 52.113%\n","Iteration step: 213; Loss: 1.441, Accuracy: 52.336%\n","Iteration step: 214; Loss: 1.444, Accuracy: 52.093%\n","Iteration step: 215; Loss: 1.446, Accuracy: 51.852%\n","Iteration step: 216; Loss: 1.449, Accuracy: 51.613%\n","Iteration step: 217; Loss: 1.447, Accuracy: 51.835%\n","Iteration step: 218; Loss: 1.446, Accuracy: 52.055%\n","Iteration step: 219; Loss: 1.447, Accuracy: 51.818%\n","Iteration step: 220; Loss: 1.450, Accuracy: 51.584%\n","Iteration step: 221; Loss: 1.448, Accuracy: 51.802%\n","Iteration step: 222; Loss: 1.450, Accuracy: 51.570%\n","Iteration step: 223; Loss: 1.448, Accuracy: 51.786%\n","Iteration step: 224; Loss: 1.449, Accuracy: 51.556%\n","Iteration step: 225; Loss: 1.450, Accuracy: 51.327%\n","Iteration step: 226; Loss: 1.452, Accuracy: 51.101%\n","Iteration step: 227; Loss: 1.450, Accuracy: 51.316%\n","Iteration step: 228; Loss: 1.449, Accuracy: 51.528%\n","Iteration step: 229; Loss: 1.452, Accuracy: 51.304%\n","Epoch: 0; Validation loss 1.4696245193481445; acc: 0.5130434632301331\n","Current epoch: 1\n","Iteration step: 0; Loss: 1.081, Accuracy: 100.000%\n","Iteration step: 1; Loss: 1.337, Accuracy: 50.000%\n","Iteration step: 2; Loss: 1.246, Accuracy: 66.667%\n","Iteration step: 3; Loss: 1.197, Accuracy: 75.000%\n","Iteration step: 4; Loss: 1.272, Accuracy: 60.000%\n","Iteration step: 5; Loss: 1.230, Accuracy: 66.667%\n","Iteration step: 6; Loss: 1.356, Accuracy: 57.143%\n","Iteration step: 7; Loss: 1.450, Accuracy: 50.000%\n","Iteration step: 8; Loss: 1.397, Accuracy: 55.556%\n","Iteration step: 9; Loss: 1.353, Accuracy: 60.000%\n","Iteration step: 10; Loss: 1.415, Accuracy: 54.545%\n","Iteration step: 11; Loss: 1.468, Accuracy: 50.000%\n","Iteration step: 12; Loss: 1.426, Accuracy: 53.846%\n","Iteration step: 13; Loss: 1.481, Accuracy: 50.000%\n","Iteration step: 14; Loss: 1.442, Accuracy: 53.333%\n","Iteration step: 15; Loss: 1.407, Accuracy: 56.250%\n","Iteration step: 16; Loss: 1.374, Accuracy: 58.824%\n","Iteration step: 17; Loss: 1.344, Accuracy: 61.111%\n","Iteration step: 18; Loss: 1.379, Accuracy: 57.895%\n","Iteration step: 19; Loss: 1.348, Accuracy: 60.000%\n","Iteration step: 20; Loss: 1.369, Accuracy: 57.143%\n","Iteration step: 21; Loss: 1.339, Accuracy: 59.091%\n","Iteration step: 22; Loss: 1.388, Accuracy: 56.522%\n","Iteration step: 23; Loss: 1.407, Accuracy: 54.167%\n","Iteration step: 24; Loss: 1.424, Accuracy: 52.000%\n","Iteration step: 25; Loss: 1.447, Accuracy: 50.000%\n","Iteration step: 26; Loss: 1.481, Accuracy: 48.148%\n","Iteration step: 27; Loss: 1.511, Accuracy: 46.429%\n","Iteration step: 28; Loss: 1.542, Accuracy: 44.828%\n","Iteration step: 29; Loss: 1.563, Accuracy: 43.333%\n","Iteration step: 30; Loss: 1.540, Accuracy: 45.161%\n","Iteration step: 31; Loss: 1.519, Accuracy: 46.875%\n","Iteration step: 32; Loss: 1.500, Accuracy: 48.485%\n","Iteration step: 33; Loss: 1.483, Accuracy: 50.000%\n","Iteration step: 34; Loss: 1.561, Accuracy: 48.571%\n","Iteration step: 35; Loss: 1.543, Accuracy: 50.000%\n","Iteration step: 36; Loss: 1.554, Accuracy: 48.649%\n","Iteration step: 37; Loss: 1.537, Accuracy: 50.000%\n","Iteration step: 38; Loss: 1.547, Accuracy: 48.718%\n","Iteration step: 39; Loss: 1.551, Accuracy: 47.500%\n","Iteration step: 40; Loss: 1.559, Accuracy: 46.341%\n","Iteration step: 41; Loss: 1.544, Accuracy: 47.619%\n","Iteration step: 42; Loss: 1.561, Accuracy: 46.512%\n","Iteration step: 43; Loss: 1.565, Accuracy: 45.455%\n","Iteration step: 44; Loss: 1.574, Accuracy: 44.444%\n","Iteration step: 45; Loss: 1.561, Accuracy: 45.652%\n","Iteration step: 46; Loss: 1.548, Accuracy: 46.809%\n","Iteration step: 47; Loss: 1.535, Accuracy: 47.917%\n","Iteration step: 48; Loss: 1.540, Accuracy: 46.939%\n","Iteration step: 49; Loss: 1.528, Accuracy: 48.000%\n","Iteration step: 50; Loss: 1.516, Accuracy: 49.020%\n","Iteration step: 51; Loss: 1.505, Accuracy: 50.000%\n","Iteration step: 52; Loss: 1.493, Accuracy: 50.943%\n","Iteration step: 53; Loss: 1.482, Accuracy: 51.852%\n","Iteration step: 54; Loss: 1.487, Accuracy: 50.909%\n","Iteration step: 55; Loss: 1.475, Accuracy: 51.786%\n","Iteration step: 56; Loss: 1.463, Accuracy: 52.632%\n","Iteration step: 57; Loss: 1.451, Accuracy: 53.448%\n","Iteration step: 58; Loss: 1.439, Accuracy: 54.237%\n","Iteration step: 59; Loss: 1.445, Accuracy: 53.333%\n","Iteration step: 60; Loss: 1.455, Accuracy: 52.459%\n","Iteration step: 61; Loss: 1.443, Accuracy: 53.226%\n","Iteration step: 62; Loss: 1.430, Accuracy: 53.968%\n","Iteration step: 63; Loss: 1.417, Accuracy: 54.688%\n","Iteration step: 64; Loss: 1.436, Accuracy: 53.846%\n","Iteration step: 65; Loss: 1.423, Accuracy: 54.545%\n","Iteration step: 66; Loss: 1.410, Accuracy: 55.224%\n","Iteration step: 67; Loss: 1.397, Accuracy: 55.882%\n","Iteration step: 68; Loss: 1.414, Accuracy: 55.072%\n","Iteration step: 69; Loss: 1.401, Accuracy: 55.714%\n","Iteration step: 70; Loss: 1.418, Accuracy: 54.930%\n","Iteration step: 71; Loss: 1.405, Accuracy: 55.556%\n","Iteration step: 72; Loss: 1.392, Accuracy: 56.164%\n","Iteration step: 73; Loss: 1.401, Accuracy: 55.405%\n","Iteration step: 74; Loss: 1.388, Accuracy: 56.000%\n","Iteration step: 75; Loss: 1.375, Accuracy: 56.579%\n","Iteration step: 76; Loss: 1.363, Accuracy: 57.143%\n","Iteration step: 77; Loss: 1.350, Accuracy: 57.692%\n","Iteration step: 78; Loss: 1.361, Accuracy: 56.962%\n","Iteration step: 79; Loss: 1.371, Accuracy: 56.250%\n","Iteration step: 80; Loss: 1.392, Accuracy: 55.556%\n","Iteration step: 81; Loss: 1.411, Accuracy: 54.878%\n","Iteration step: 82; Loss: 1.419, Accuracy: 54.217%\n","Iteration step: 83; Loss: 1.426, Accuracy: 53.571%\n","Iteration step: 84; Loss: 1.440, Accuracy: 52.941%\n","Iteration step: 85; Loss: 1.429, Accuracy: 53.488%\n","Iteration step: 86; Loss: 1.433, Accuracy: 52.874%\n","Iteration step: 87; Loss: 1.446, Accuracy: 52.273%\n","Iteration step: 88; Loss: 1.458, Accuracy: 51.685%\n","Iteration step: 89; Loss: 1.449, Accuracy: 52.222%\n","Iteration step: 90; Loss: 1.459, Accuracy: 51.648%\n","Iteration step: 91; Loss: 1.451, Accuracy: 52.174%\n","Iteration step: 92; Loss: 1.453, Accuracy: 51.613%\n","Iteration step: 93; Loss: 1.461, Accuracy: 51.064%\n","Iteration step: 94; Loss: 1.453, Accuracy: 51.579%\n","Iteration step: 95; Loss: 1.447, Accuracy: 52.083%\n","Iteration step: 96; Loss: 1.456, Accuracy: 51.546%\n","Iteration step: 97; Loss: 1.482, Accuracy: 51.020%\n","Iteration step: 98; Loss: 1.476, Accuracy: 51.515%\n","Iteration step: 99; Loss: 1.470, Accuracy: 52.000%\n","Iteration step: 100; Loss: 1.478, Accuracy: 51.485%\n","Iteration step: 101; Loss: 1.479, Accuracy: 50.980%\n","Iteration step: 102; Loss: 1.474, Accuracy: 51.456%\n","Iteration step: 103; Loss: 1.468, Accuracy: 51.923%\n","Iteration step: 104; Loss: 1.463, Accuracy: 52.381%\n","Iteration step: 105; Loss: 1.458, Accuracy: 52.830%\n","Iteration step: 106; Loss: 1.453, Accuracy: 53.271%\n","Iteration step: 107; Loss: 1.447, Accuracy: 53.704%\n","Iteration step: 108; Loss: 1.442, Accuracy: 54.128%\n","Iteration step: 109; Loss: 1.436, Accuracy: 54.545%\n","Iteration step: 110; Loss: 1.444, Accuracy: 54.054%\n","Iteration step: 111; Loss: 1.439, Accuracy: 54.464%\n","Iteration step: 112; Loss: 1.440, Accuracy: 53.982%\n","Iteration step: 113; Loss: 1.442, Accuracy: 53.509%\n","Iteration step: 114; Loss: 1.436, Accuracy: 53.913%\n","Iteration step: 115; Loss: 1.444, Accuracy: 53.448%\n","Iteration step: 116; Loss: 1.446, Accuracy: 52.991%\n","Iteration step: 117; Loss: 1.440, Accuracy: 53.390%\n","Iteration step: 118; Loss: 1.434, Accuracy: 53.782%\n","Iteration step: 119; Loss: 1.442, Accuracy: 53.333%\n","Iteration step: 120; Loss: 1.436, Accuracy: 53.719%\n","Iteration step: 121; Loss: 1.444, Accuracy: 53.279%\n","Iteration step: 122; Loss: 1.438, Accuracy: 53.659%\n","Iteration step: 123; Loss: 1.445, Accuracy: 53.226%\n","Iteration step: 124; Loss: 1.439, Accuracy: 53.600%\n","Iteration step: 125; Loss: 1.433, Accuracy: 53.968%\n","Iteration step: 126; Loss: 1.427, Accuracy: 54.331%\n","Iteration step: 127; Loss: 1.434, Accuracy: 53.906%\n","Iteration step: 128; Loss: 1.428, Accuracy: 54.264%\n","Iteration step: 129; Loss: 1.430, Accuracy: 53.846%\n","Iteration step: 130; Loss: 1.437, Accuracy: 53.435%\n","Iteration step: 131; Loss: 1.431, Accuracy: 53.788%\n","Iteration step: 132; Loss: 1.425, Accuracy: 54.135%\n","Iteration step: 133; Loss: 1.419, Accuracy: 54.478%\n","Iteration step: 134; Loss: 1.413, Accuracy: 54.815%\n","Iteration step: 135; Loss: 1.420, Accuracy: 54.412%\n","Iteration step: 136; Loss: 1.414, Accuracy: 54.745%\n","Iteration step: 137; Loss: 1.421, Accuracy: 54.348%\n","Iteration step: 138; Loss: 1.414, Accuracy: 54.676%\n","Iteration step: 139; Loss: 1.424, Accuracy: 54.286%\n","Iteration step: 140; Loss: 1.433, Accuracy: 53.901%\n","Iteration step: 141; Loss: 1.427, Accuracy: 54.225%\n","Iteration step: 142; Loss: 1.421, Accuracy: 54.545%\n","Iteration step: 143; Loss: 1.424, Accuracy: 54.167%\n","Iteration step: 144; Loss: 1.432, Accuracy: 53.793%\n","Iteration step: 145; Loss: 1.426, Accuracy: 54.110%\n","Iteration step: 146; Loss: 1.432, Accuracy: 53.741%\n","Iteration step: 147; Loss: 1.427, Accuracy: 54.054%\n","Iteration step: 148; Loss: 1.421, Accuracy: 54.362%\n","Iteration step: 149; Loss: 1.415, Accuracy: 54.667%\n","Iteration step: 150; Loss: 1.421, Accuracy: 54.305%\n","Iteration step: 151; Loss: 1.415, Accuracy: 54.605%\n","Iteration step: 152; Loss: 1.419, Accuracy: 54.248%\n","Iteration step: 153; Loss: 1.414, Accuracy: 54.545%\n","Iteration step: 154; Loss: 1.408, Accuracy: 54.839%\n","Iteration step: 155; Loss: 1.403, Accuracy: 55.128%\n","Iteration step: 156; Loss: 1.408, Accuracy: 54.777%\n","Iteration step: 157; Loss: 1.403, Accuracy: 55.063%\n","Iteration step: 158; Loss: 1.397, Accuracy: 55.346%\n","Iteration step: 159; Loss: 1.404, Accuracy: 55.000%\n","Iteration step: 160; Loss: 1.408, Accuracy: 54.658%\n","Iteration step: 161; Loss: 1.415, Accuracy: 54.321%\n","Iteration step: 162; Loss: 1.422, Accuracy: 53.988%\n","Iteration step: 163; Loss: 1.417, Accuracy: 54.268%\n","Iteration step: 164; Loss: 1.435, Accuracy: 53.939%\n","Iteration step: 165; Loss: 1.430, Accuracy: 54.217%\n","Iteration step: 166; Loss: 1.425, Accuracy: 54.491%\n","Iteration step: 167; Loss: 1.420, Accuracy: 54.762%\n","Iteration step: 168; Loss: 1.424, Accuracy: 54.438%\n","Iteration step: 169; Loss: 1.419, Accuracy: 54.706%\n","Iteration step: 170; Loss: 1.414, Accuracy: 54.971%\n","Iteration step: 171; Loss: 1.420, Accuracy: 54.651%\n","Iteration step: 172; Loss: 1.423, Accuracy: 54.335%\n","Iteration step: 173; Loss: 1.418, Accuracy: 54.598%\n","Iteration step: 174; Loss: 1.414, Accuracy: 54.857%\n","Iteration step: 175; Loss: 1.409, Accuracy: 55.114%\n","Iteration step: 176; Loss: 1.414, Accuracy: 54.802%\n","Iteration step: 177; Loss: 1.410, Accuracy: 55.056%\n","Iteration step: 178; Loss: 1.413, Accuracy: 54.749%\n","Iteration step: 179; Loss: 1.409, Accuracy: 55.000%\n","Iteration step: 180; Loss: 1.404, Accuracy: 55.249%\n","Iteration step: 181; Loss: 1.400, Accuracy: 55.495%\n","Iteration step: 182; Loss: 1.395, Accuracy: 55.738%\n","Iteration step: 183; Loss: 1.401, Accuracy: 55.435%\n","Iteration step: 184; Loss: 1.397, Accuracy: 55.676%\n","Iteration step: 185; Loss: 1.392, Accuracy: 55.914%\n","Iteration step: 186; Loss: 1.396, Accuracy: 55.615%\n","Iteration step: 187; Loss: 1.402, Accuracy: 55.319%\n","Iteration step: 188; Loss: 1.408, Accuracy: 55.026%\n","Iteration step: 189; Loss: 1.412, Accuracy: 54.737%\n","Iteration step: 190; Loss: 1.407, Accuracy: 54.974%\n","Iteration step: 191; Loss: 1.402, Accuracy: 55.208%\n","Iteration step: 192; Loss: 1.407, Accuracy: 54.922%\n","Iteration step: 193; Loss: 1.403, Accuracy: 55.155%\n","Iteration step: 194; Loss: 1.409, Accuracy: 54.872%\n","Iteration step: 195; Loss: 1.411, Accuracy: 54.592%\n","Iteration step: 196; Loss: 1.416, Accuracy: 54.315%\n","Iteration step: 197; Loss: 1.412, Accuracy: 54.545%\n","Iteration step: 198; Loss: 1.408, Accuracy: 54.774%\n","Iteration step: 199; Loss: 1.412, Accuracy: 54.500%\n","Iteration step: 200; Loss: 1.408, Accuracy: 54.726%\n","Iteration step: 201; Loss: 1.413, Accuracy: 54.455%\n","Iteration step: 202; Loss: 1.418, Accuracy: 54.187%\n","Iteration step: 203; Loss: 1.423, Accuracy: 53.922%\n","Iteration step: 204; Loss: 1.425, Accuracy: 53.659%\n","Iteration step: 205; Loss: 1.427, Accuracy: 53.398%\n","Iteration step: 206; Loss: 1.431, Accuracy: 53.140%\n","Iteration step: 207; Loss: 1.434, Accuracy: 52.885%\n","Iteration step: 208; Loss: 1.436, Accuracy: 52.632%\n","Iteration step: 209; Loss: 1.438, Accuracy: 52.381%\n","Iteration step: 210; Loss: 1.435, Accuracy: 52.607%\n","Iteration step: 211; Loss: 1.438, Accuracy: 52.358%\n","Iteration step: 212; Loss: 1.441, Accuracy: 52.113%\n","Iteration step: 213; Loss: 1.438, Accuracy: 52.336%\n","Iteration step: 214; Loss: 1.441, Accuracy: 52.093%\n","Iteration step: 215; Loss: 1.438, Accuracy: 52.315%\n","Iteration step: 216; Loss: 1.441, Accuracy: 52.074%\n","Iteration step: 217; Loss: 1.439, Accuracy: 52.294%\n","Iteration step: 218; Loss: 1.442, Accuracy: 52.055%\n","Iteration step: 219; Loss: 1.444, Accuracy: 51.818%\n","Iteration step: 220; Loss: 1.442, Accuracy: 52.036%\n","Iteration step: 221; Loss: 1.446, Accuracy: 51.802%\n","Iteration step: 222; Loss: 1.447, Accuracy: 51.570%\n","Iteration step: 223; Loss: 1.448, Accuracy: 51.339%\n","Iteration step: 224; Loss: 1.446, Accuracy: 51.556%\n","Iteration step: 225; Loss: 1.447, Accuracy: 51.327%\n","Iteration step: 226; Loss: 1.450, Accuracy: 51.101%\n","Iteration step: 227; Loss: 1.448, Accuracy: 51.316%\n","Iteration step: 228; Loss: 1.446, Accuracy: 51.528%\n","Iteration step: 229; Loss: 1.449, Accuracy: 51.304%\n","Epoch: 1; Validation loss 1.4584269523620605; acc: 0.5130434632301331\n","Current epoch: 2\n","Iteration step: 0; Loss: 1.620, Accuracy: 0.000%\n","Iteration step: 1; Loss: 1.330, Accuracy: 50.000%\n","Iteration step: 2; Loss: 1.231, Accuracy: 66.667%\n","Iteration step: 3; Loss: 1.323, Accuracy: 50.000%\n","Iteration step: 4; Loss: 1.261, Accuracy: 60.000%\n","Iteration step: 5; Loss: 1.218, Accuracy: 66.667%\n","Iteration step: 6; Loss: 1.322, Accuracy: 57.143%\n","Iteration step: 7; Loss: 1.279, Accuracy: 62.500%\n","Iteration step: 8; Loss: 1.376, Accuracy: 55.556%\n","Iteration step: 9; Loss: 1.454, Accuracy: 50.000%\n","Iteration step: 10; Loss: 1.407, Accuracy: 54.545%\n","Iteration step: 11; Loss: 1.368, Accuracy: 58.333%\n","Iteration step: 12; Loss: 1.333, Accuracy: 61.538%\n","Iteration step: 13; Loss: 1.302, Accuracy: 64.286%\n","Iteration step: 14; Loss: 1.358, Accuracy: 60.000%\n","Iteration step: 15; Loss: 1.327, Accuracy: 62.500%\n","Iteration step: 16; Loss: 1.376, Accuracy: 58.824%\n","Iteration step: 17; Loss: 1.392, Accuracy: 55.556%\n","Iteration step: 18; Loss: 1.431, Accuracy: 52.632%\n","Iteration step: 19; Loss: 1.443, Accuracy: 50.000%\n","Iteration step: 20; Loss: 1.453, Accuracy: 47.619%\n","Iteration step: 21; Loss: 1.482, Accuracy: 45.455%\n","Iteration step: 22; Loss: 1.454, Accuracy: 47.826%\n","Iteration step: 23; Loss: 1.428, Accuracy: 50.000%\n","Iteration step: 24; Loss: 1.538, Accuracy: 48.000%\n","Iteration step: 25; Loss: 1.563, Accuracy: 46.154%\n","Iteration step: 26; Loss: 1.586, Accuracy: 44.444%\n","Iteration step: 27; Loss: 1.607, Accuracy: 42.857%\n","Iteration step: 28; Loss: 1.628, Accuracy: 41.379%\n","Iteration step: 29; Loss: 1.644, Accuracy: 40.000%\n","Iteration step: 30; Loss: 1.657, Accuracy: 38.710%\n","Iteration step: 31; Loss: 1.634, Accuracy: 40.625%\n","Iteration step: 32; Loss: 1.635, Accuracy: 39.394%\n","Iteration step: 33; Loss: 1.615, Accuracy: 41.176%\n","Iteration step: 34; Loss: 1.597, Accuracy: 42.857%\n","Iteration step: 35; Loss: 1.580, Accuracy: 44.444%\n","Iteration step: 36; Loss: 1.564, Accuracy: 45.946%\n","Iteration step: 37; Loss: 1.548, Accuracy: 47.368%\n","Iteration step: 38; Loss: 1.559, Accuracy: 46.154%\n","Iteration step: 39; Loss: 1.544, Accuracy: 47.500%\n","Iteration step: 40; Loss: 1.547, Accuracy: 46.341%\n","Iteration step: 41; Loss: 1.556, Accuracy: 45.238%\n","Iteration step: 42; Loss: 1.541, Accuracy: 46.512%\n","Iteration step: 43; Loss: 1.558, Accuracy: 45.455%\n","Iteration step: 44; Loss: 1.543, Accuracy: 46.667%\n","Iteration step: 45; Loss: 1.530, Accuracy: 47.826%\n","Iteration step: 46; Loss: 1.516, Accuracy: 48.936%\n","Iteration step: 47; Loss: 1.521, Accuracy: 47.917%\n","Iteration step: 48; Loss: 1.508, Accuracy: 48.980%\n","Iteration step: 49; Loss: 1.494, Accuracy: 50.000%\n","Iteration step: 50; Loss: 1.506, Accuracy: 49.020%\n","Iteration step: 51; Loss: 1.493, Accuracy: 50.000%\n","Iteration step: 52; Loss: 1.480, Accuracy: 50.943%\n","Iteration step: 53; Loss: 1.467, Accuracy: 51.852%\n","Iteration step: 54; Loss: 1.473, Accuracy: 50.909%\n","Iteration step: 55; Loss: 1.482, Accuracy: 50.000%\n","Iteration step: 56; Loss: 1.469, Accuracy: 50.877%\n","Iteration step: 57; Loss: 1.456, Accuracy: 51.724%\n","Iteration step: 58; Loss: 1.443, Accuracy: 52.542%\n","Iteration step: 59; Loss: 1.450, Accuracy: 51.667%\n","Iteration step: 60; Loss: 1.437, Accuracy: 52.459%\n","Iteration step: 61; Loss: 1.424, Accuracy: 53.226%\n","Iteration step: 62; Loss: 1.411, Accuracy: 53.968%\n","Iteration step: 63; Loss: 1.398, Accuracy: 54.688%\n","Iteration step: 64; Loss: 1.385, Accuracy: 55.385%\n","Iteration step: 65; Loss: 1.393, Accuracy: 54.545%\n","Iteration step: 66; Loss: 1.380, Accuracy: 55.224%\n","Iteration step: 67; Loss: 1.367, Accuracy: 55.882%\n","Iteration step: 68; Loss: 1.353, Accuracy: 56.522%\n","Iteration step: 69; Loss: 1.340, Accuracy: 57.143%\n","Iteration step: 70; Loss: 1.360, Accuracy: 56.338%\n","Iteration step: 71; Loss: 1.346, Accuracy: 56.944%\n","Iteration step: 72; Loss: 1.372, Accuracy: 56.164%\n","Iteration step: 73; Loss: 1.359, Accuracy: 56.757%\n","Iteration step: 74; Loss: 1.370, Accuracy: 56.000%\n","Iteration step: 75; Loss: 1.393, Accuracy: 55.263%\n","Iteration step: 76; Loss: 1.411, Accuracy: 54.545%\n","Iteration step: 77; Loss: 1.398, Accuracy: 55.128%\n","Iteration step: 78; Loss: 1.386, Accuracy: 55.696%\n","Iteration step: 79; Loss: 1.404, Accuracy: 55.000%\n","Iteration step: 80; Loss: 1.392, Accuracy: 55.556%\n","Iteration step: 81; Loss: 1.400, Accuracy: 54.878%\n","Iteration step: 82; Loss: 1.389, Accuracy: 55.422%\n","Iteration step: 83; Loss: 1.396, Accuracy: 54.762%\n","Iteration step: 84; Loss: 1.402, Accuracy: 54.118%\n","Iteration step: 85; Loss: 1.415, Accuracy: 53.488%\n","Iteration step: 86; Loss: 1.405, Accuracy: 54.023%\n","Iteration step: 87; Loss: 1.418, Accuracy: 53.409%\n","Iteration step: 88; Loss: 1.422, Accuracy: 52.809%\n","Iteration step: 89; Loss: 1.413, Accuracy: 53.333%\n","Iteration step: 90; Loss: 1.404, Accuracy: 53.846%\n","Iteration step: 91; Loss: 1.408, Accuracy: 53.261%\n","Iteration step: 92; Loss: 1.417, Accuracy: 52.688%\n","Iteration step: 93; Loss: 1.420, Accuracy: 52.128%\n","Iteration step: 94; Loss: 1.412, Accuracy: 52.632%\n","Iteration step: 95; Loss: 1.421, Accuracy: 52.083%\n","Iteration step: 96; Loss: 1.431, Accuracy: 51.546%\n","Iteration step: 97; Loss: 1.458, Accuracy: 51.020%\n","Iteration step: 98; Loss: 1.451, Accuracy: 51.515%\n","Iteration step: 99; Loss: 1.445, Accuracy: 52.000%\n","Iteration step: 100; Loss: 1.454, Accuracy: 51.485%\n","Iteration step: 101; Loss: 1.448, Accuracy: 51.961%\n","Iteration step: 102; Loss: 1.449, Accuracy: 51.456%\n","Iteration step: 103; Loss: 1.443, Accuracy: 51.923%\n","Iteration step: 104; Loss: 1.451, Accuracy: 51.429%\n","Iteration step: 105; Loss: 1.446, Accuracy: 51.887%\n","Iteration step: 106; Loss: 1.440, Accuracy: 52.336%\n","Iteration step: 107; Loss: 1.435, Accuracy: 52.778%\n","Iteration step: 108; Loss: 1.430, Accuracy: 53.211%\n","Iteration step: 109; Loss: 1.424, Accuracy: 53.636%\n","Iteration step: 110; Loss: 1.432, Accuracy: 53.153%\n","Iteration step: 111; Loss: 1.433, Accuracy: 52.679%\n","Iteration step: 112; Loss: 1.435, Accuracy: 52.212%\n","Iteration step: 113; Loss: 1.429, Accuracy: 52.632%\n","Iteration step: 114; Loss: 1.424, Accuracy: 53.043%\n","Iteration step: 115; Loss: 1.418, Accuracy: 53.448%\n","Iteration step: 116; Loss: 1.427, Accuracy: 52.991%\n","Iteration step: 117; Loss: 1.421, Accuracy: 53.390%\n","Iteration step: 118; Loss: 1.415, Accuracy: 53.782%\n","Iteration step: 119; Loss: 1.424, Accuracy: 53.333%\n","Iteration step: 120; Loss: 1.418, Accuracy: 53.719%\n","Iteration step: 121; Loss: 1.426, Accuracy: 53.279%\n","Iteration step: 122; Loss: 1.420, Accuracy: 53.659%\n","Iteration step: 123; Loss: 1.414, Accuracy: 54.032%\n","Iteration step: 124; Loss: 1.408, Accuracy: 54.400%\n","Iteration step: 125; Loss: 1.402, Accuracy: 54.762%\n","Iteration step: 126; Loss: 1.396, Accuracy: 55.118%\n","Iteration step: 127; Loss: 1.390, Accuracy: 55.469%\n","Iteration step: 128; Loss: 1.383, Accuracy: 55.814%\n","Iteration step: 129; Loss: 1.392, Accuracy: 55.385%\n","Iteration step: 130; Loss: 1.401, Accuracy: 54.962%\n","Iteration step: 131; Loss: 1.405, Accuracy: 54.545%\n","Iteration step: 132; Loss: 1.413, Accuracy: 54.135%\n","Iteration step: 133; Loss: 1.407, Accuracy: 54.478%\n","Iteration step: 134; Loss: 1.415, Accuracy: 54.074%\n","Iteration step: 135; Loss: 1.408, Accuracy: 54.412%\n","Iteration step: 136; Loss: 1.416, Accuracy: 54.015%\n","Iteration step: 137; Loss: 1.409, Accuracy: 54.348%\n","Iteration step: 138; Loss: 1.418, Accuracy: 53.957%\n","Iteration step: 139; Loss: 1.412, Accuracy: 54.286%\n","Iteration step: 140; Loss: 1.420, Accuracy: 53.901%\n","Iteration step: 141; Loss: 1.414, Accuracy: 54.225%\n","Iteration step: 142; Loss: 1.408, Accuracy: 54.545%\n","Iteration step: 143; Loss: 1.416, Accuracy: 54.167%\n","Iteration step: 144; Loss: 1.410, Accuracy: 54.483%\n","Iteration step: 145; Loss: 1.405, Accuracy: 54.795%\n","Iteration step: 146; Loss: 1.411, Accuracy: 54.422%\n","Iteration step: 147; Loss: 1.405, Accuracy: 54.730%\n","Iteration step: 148; Loss: 1.411, Accuracy: 54.362%\n","Iteration step: 149; Loss: 1.406, Accuracy: 54.667%\n","Iteration step: 150; Loss: 1.410, Accuracy: 54.305%\n","Iteration step: 151; Loss: 1.404, Accuracy: 54.605%\n","Iteration step: 152; Loss: 1.409, Accuracy: 54.248%\n","Iteration step: 153; Loss: 1.414, Accuracy: 53.896%\n","Iteration step: 154; Loss: 1.409, Accuracy: 54.194%\n","Iteration step: 155; Loss: 1.403, Accuracy: 54.487%\n","Iteration step: 156; Loss: 1.399, Accuracy: 54.777%\n","Iteration step: 157; Loss: 1.394, Accuracy: 55.063%\n","Iteration step: 158; Loss: 1.398, Accuracy: 54.717%\n","Iteration step: 159; Loss: 1.393, Accuracy: 55.000%\n","Iteration step: 160; Loss: 1.388, Accuracy: 55.280%\n","Iteration step: 161; Loss: 1.394, Accuracy: 54.938%\n","Iteration step: 162; Loss: 1.389, Accuracy: 55.215%\n","Iteration step: 163; Loss: 1.395, Accuracy: 54.878%\n","Iteration step: 164; Loss: 1.390, Accuracy: 55.152%\n","Iteration step: 165; Loss: 1.397, Accuracy: 54.819%\n","Iteration step: 166; Loss: 1.401, Accuracy: 54.491%\n","Iteration step: 167; Loss: 1.396, Accuracy: 54.762%\n","Iteration step: 168; Loss: 1.414, Accuracy: 54.438%\n","Iteration step: 169; Loss: 1.409, Accuracy: 54.706%\n","Iteration step: 170; Loss: 1.412, Accuracy: 54.386%\n","Iteration step: 171; Loss: 1.418, Accuracy: 54.070%\n","Iteration step: 172; Loss: 1.413, Accuracy: 54.335%\n","Iteration step: 173; Loss: 1.408, Accuracy: 54.598%\n","Iteration step: 174; Loss: 1.404, Accuracy: 54.857%\n","Iteration step: 175; Loss: 1.399, Accuracy: 55.114%\n","Iteration step: 176; Loss: 1.395, Accuracy: 55.367%\n","Iteration step: 177; Loss: 1.391, Accuracy: 55.618%\n","Iteration step: 178; Loss: 1.394, Accuracy: 55.307%\n","Iteration step: 179; Loss: 1.399, Accuracy: 55.000%\n","Iteration step: 180; Loss: 1.394, Accuracy: 55.249%\n","Iteration step: 181; Loss: 1.390, Accuracy: 55.495%\n","Iteration step: 182; Loss: 1.396, Accuracy: 55.191%\n","Iteration step: 183; Loss: 1.391, Accuracy: 55.435%\n","Iteration step: 184; Loss: 1.397, Accuracy: 55.135%\n","Iteration step: 185; Loss: 1.400, Accuracy: 54.839%\n","Iteration step: 186; Loss: 1.404, Accuracy: 54.545%\n","Iteration step: 187; Loss: 1.399, Accuracy: 54.787%\n","Iteration step: 188; Loss: 1.405, Accuracy: 54.497%\n","Iteration step: 189; Loss: 1.400, Accuracy: 54.737%\n","Iteration step: 190; Loss: 1.406, Accuracy: 54.450%\n","Iteration step: 191; Loss: 1.411, Accuracy: 54.167%\n","Iteration step: 192; Loss: 1.407, Accuracy: 54.404%\n","Iteration step: 193; Loss: 1.411, Accuracy: 54.124%\n","Iteration step: 194; Loss: 1.407, Accuracy: 54.359%\n","Iteration step: 195; Loss: 1.403, Accuracy: 54.592%\n","Iteration step: 196; Loss: 1.399, Accuracy: 54.822%\n","Iteration step: 197; Loss: 1.402, Accuracy: 54.545%\n","Iteration step: 198; Loss: 1.398, Accuracy: 54.774%\n","Iteration step: 199; Loss: 1.394, Accuracy: 55.000%\n","Iteration step: 200; Loss: 1.398, Accuracy: 54.726%\n","Iteration step: 201; Loss: 1.401, Accuracy: 54.455%\n","Iteration step: 202; Loss: 1.405, Accuracy: 54.187%\n","Iteration step: 203; Loss: 1.410, Accuracy: 53.922%\n","Iteration step: 204; Loss: 1.412, Accuracy: 53.659%\n","Iteration step: 205; Loss: 1.416, Accuracy: 53.398%\n","Iteration step: 206; Loss: 1.420, Accuracy: 53.140%\n","Iteration step: 207; Loss: 1.424, Accuracy: 52.885%\n","Iteration step: 208; Loss: 1.420, Accuracy: 53.110%\n","Iteration step: 209; Loss: 1.427, Accuracy: 52.857%\n","Iteration step: 210; Loss: 1.431, Accuracy: 52.607%\n","Iteration step: 211; Loss: 1.434, Accuracy: 52.358%\n","Iteration step: 212; Loss: 1.432, Accuracy: 52.582%\n","Iteration step: 213; Loss: 1.433, Accuracy: 52.336%\n","Iteration step: 214; Loss: 1.436, Accuracy: 52.093%\n","Iteration step: 215; Loss: 1.438, Accuracy: 51.852%\n","Iteration step: 216; Loss: 1.436, Accuracy: 52.074%\n","Iteration step: 217; Loss: 1.433, Accuracy: 52.294%\n","Iteration step: 218; Loss: 1.437, Accuracy: 52.055%\n","Iteration step: 219; Loss: 1.435, Accuracy: 52.273%\n","Iteration step: 220; Loss: 1.436, Accuracy: 52.036%\n","Iteration step: 221; Loss: 1.438, Accuracy: 51.802%\n","Iteration step: 222; Loss: 1.441, Accuracy: 51.570%\n","Iteration step: 223; Loss: 1.439, Accuracy: 51.786%\n","Iteration step: 224; Loss: 1.437, Accuracy: 52.000%\n","Iteration step: 225; Loss: 1.438, Accuracy: 51.770%\n","Iteration step: 226; Loss: 1.439, Accuracy: 51.542%\n","Iteration step: 227; Loss: 1.442, Accuracy: 51.316%\n","Iteration step: 228; Loss: 1.440, Accuracy: 51.528%\n","Iteration step: 229; Loss: 1.443, Accuracy: 51.304%\n","Epoch: 2; Validation loss 1.4449712038040161; acc: 0.5130434632301331\n","Current epoch: 3\n","Iteration step: 0; Loss: 0.987, Accuracy: 100.000%\n","Iteration step: 1; Loss: 1.313, Accuracy: 50.000%\n","Iteration step: 2; Loss: 1.202, Accuracy: 66.667%\n","Iteration step: 3; Loss: 1.144, Accuracy: 75.000%\n","Iteration step: 4; Loss: 1.341, Accuracy: 60.000%\n","Iteration step: 5; Loss: 1.472, Accuracy: 50.000%\n","Iteration step: 6; Loss: 1.545, Accuracy: 42.857%\n","Iteration step: 7; Loss: 1.470, Accuracy: 50.000%\n","Iteration step: 8; Loss: 1.411, Accuracy: 55.556%\n","Iteration step: 9; Loss: 1.362, Accuracy: 60.000%\n","Iteration step: 10; Loss: 1.385, Accuracy: 54.545%\n","Iteration step: 11; Loss: 1.345, Accuracy: 58.333%\n","Iteration step: 12; Loss: 1.309, Accuracy: 61.538%\n","Iteration step: 13; Loss: 1.276, Accuracy: 64.286%\n","Iteration step: 14; Loss: 1.246, Accuracy: 66.667%\n","Iteration step: 15; Loss: 1.302, Accuracy: 62.500%\n","Iteration step: 16; Loss: 1.322, Accuracy: 58.824%\n","Iteration step: 17; Loss: 1.290, Accuracy: 61.111%\n","Iteration step: 18; Loss: 1.352, Accuracy: 57.895%\n","Iteration step: 19; Loss: 1.319, Accuracy: 60.000%\n","Iteration step: 20; Loss: 1.336, Accuracy: 57.143%\n","Iteration step: 21; Loss: 1.375, Accuracy: 54.545%\n","Iteration step: 22; Loss: 1.344, Accuracy: 56.522%\n","Iteration step: 23; Loss: 1.479, Accuracy: 54.167%\n","Iteration step: 24; Loss: 1.520, Accuracy: 52.000%\n","Iteration step: 25; Loss: 1.525, Accuracy: 50.000%\n","Iteration step: 26; Loss: 1.558, Accuracy: 48.148%\n","Iteration step: 27; Loss: 1.585, Accuracy: 46.429%\n","Iteration step: 28; Loss: 1.607, Accuracy: 44.828%\n","Iteration step: 29; Loss: 1.626, Accuracy: 43.333%\n","Iteration step: 30; Loss: 1.638, Accuracy: 41.935%\n","Iteration step: 31; Loss: 1.615, Accuracy: 43.750%\n","Iteration step: 32; Loss: 1.595, Accuracy: 45.455%\n","Iteration step: 33; Loss: 1.613, Accuracy: 44.118%\n","Iteration step: 34; Loss: 1.595, Accuracy: 45.714%\n","Iteration step: 35; Loss: 1.577, Accuracy: 47.222%\n","Iteration step: 36; Loss: 1.580, Accuracy: 45.946%\n","Iteration step: 37; Loss: 1.564, Accuracy: 47.368%\n","Iteration step: 38; Loss: 1.549, Accuracy: 48.718%\n","Iteration step: 39; Loss: 1.535, Accuracy: 50.000%\n","Iteration step: 40; Loss: 1.550, Accuracy: 48.780%\n","Iteration step: 41; Loss: 1.554, Accuracy: 47.619%\n","Iteration step: 42; Loss: 1.566, Accuracy: 46.512%\n","Iteration step: 43; Loss: 1.552, Accuracy: 47.727%\n","Iteration step: 44; Loss: 1.538, Accuracy: 48.889%\n","Iteration step: 45; Loss: 1.525, Accuracy: 50.000%\n","Iteration step: 46; Loss: 1.512, Accuracy: 51.064%\n","Iteration step: 47; Loss: 1.499, Accuracy: 52.083%\n","Iteration step: 48; Loss: 1.486, Accuracy: 53.061%\n","Iteration step: 49; Loss: 1.492, Accuracy: 52.000%\n","Iteration step: 50; Loss: 1.501, Accuracy: 50.980%\n","Iteration step: 51; Loss: 1.488, Accuracy: 51.923%\n","Iteration step: 52; Loss: 1.474, Accuracy: 52.830%\n","Iteration step: 53; Loss: 1.480, Accuracy: 51.852%\n","Iteration step: 54; Loss: 1.466, Accuracy: 52.727%\n","Iteration step: 55; Loss: 1.481, Accuracy: 51.786%\n","Iteration step: 56; Loss: 1.491, Accuracy: 50.877%\n","Iteration step: 57; Loss: 1.477, Accuracy: 51.724%\n","Iteration step: 58; Loss: 1.483, Accuracy: 50.847%\n","Iteration step: 59; Loss: 1.469, Accuracy: 51.667%\n","Iteration step: 60; Loss: 1.474, Accuracy: 50.820%\n","Iteration step: 61; Loss: 1.461, Accuracy: 51.613%\n","Iteration step: 62; Loss: 1.447, Accuracy: 52.381%\n","Iteration step: 63; Loss: 1.468, Accuracy: 51.562%\n","Iteration step: 64; Loss: 1.455, Accuracy: 52.308%\n","Iteration step: 65; Loss: 1.441, Accuracy: 53.030%\n","Iteration step: 66; Loss: 1.428, Accuracy: 53.731%\n","Iteration step: 67; Loss: 1.414, Accuracy: 54.412%\n","Iteration step: 68; Loss: 1.400, Accuracy: 55.072%\n","Iteration step: 69; Loss: 1.382, Accuracy: 55.714%\n","Iteration step: 70; Loss: 1.367, Accuracy: 56.338%\n","Iteration step: 71; Loss: 1.353, Accuracy: 56.944%\n","Iteration step: 72; Loss: 1.337, Accuracy: 57.534%\n","Iteration step: 73; Loss: 1.371, Accuracy: 56.757%\n","Iteration step: 74; Loss: 1.403, Accuracy: 56.000%\n","Iteration step: 75; Loss: 1.419, Accuracy: 55.263%\n","Iteration step: 76; Loss: 1.404, Accuracy: 55.844%\n","Iteration step: 77; Loss: 1.426, Accuracy: 55.128%\n","Iteration step: 78; Loss: 1.445, Accuracy: 54.430%\n","Iteration step: 79; Loss: 1.453, Accuracy: 53.750%\n","Iteration step: 80; Loss: 1.441, Accuracy: 54.321%\n","Iteration step: 81; Loss: 1.430, Accuracy: 54.878%\n","Iteration step: 82; Loss: 1.420, Accuracy: 55.422%\n","Iteration step: 83; Loss: 1.425, Accuracy: 54.762%\n","Iteration step: 84; Loss: 1.445, Accuracy: 54.118%\n","Iteration step: 85; Loss: 1.449, Accuracy: 53.488%\n","Iteration step: 86; Loss: 1.441, Accuracy: 54.023%\n","Iteration step: 87; Loss: 1.444, Accuracy: 53.409%\n","Iteration step: 88; Loss: 1.447, Accuracy: 52.809%\n","Iteration step: 89; Loss: 1.454, Accuracy: 52.222%\n","Iteration step: 90; Loss: 1.461, Accuracy: 51.648%\n","Iteration step: 91; Loss: 1.455, Accuracy: 52.174%\n","Iteration step: 92; Loss: 1.461, Accuracy: 51.613%\n","Iteration step: 93; Loss: 1.467, Accuracy: 51.064%\n","Iteration step: 94; Loss: 1.462, Accuracy: 51.579%\n","Iteration step: 95; Loss: 1.469, Accuracy: 51.042%\n","Iteration step: 96; Loss: 1.464, Accuracy: 51.546%\n","Iteration step: 97; Loss: 1.460, Accuracy: 52.041%\n","Iteration step: 98; Loss: 1.484, Accuracy: 51.515%\n","Iteration step: 99; Loss: 1.480, Accuracy: 52.000%\n","Iteration step: 100; Loss: 1.486, Accuracy: 51.485%\n","Iteration step: 101; Loss: 1.482, Accuracy: 51.961%\n","Iteration step: 102; Loss: 1.478, Accuracy: 52.427%\n","Iteration step: 103; Loss: 1.479, Accuracy: 51.923%\n","Iteration step: 104; Loss: 1.475, Accuracy: 52.381%\n","Iteration step: 105; Loss: 1.470, Accuracy: 52.830%\n","Iteration step: 106; Loss: 1.472, Accuracy: 52.336%\n","Iteration step: 107; Loss: 1.467, Accuracy: 52.778%\n","Iteration step: 108; Loss: 1.463, Accuracy: 53.211%\n","Iteration step: 109; Loss: 1.458, Accuracy: 53.636%\n","Iteration step: 110; Loss: 1.454, Accuracy: 54.054%\n","Iteration step: 111; Loss: 1.455, Accuracy: 53.571%\n","Iteration step: 112; Loss: 1.450, Accuracy: 53.982%\n","Iteration step: 113; Loss: 1.445, Accuracy: 54.386%\n","Iteration step: 114; Loss: 1.451, Accuracy: 53.913%\n","Iteration step: 115; Loss: 1.458, Accuracy: 53.448%\n","Iteration step: 116; Loss: 1.459, Accuracy: 52.991%\n","Iteration step: 117; Loss: 1.454, Accuracy: 53.390%\n","Iteration step: 118; Loss: 1.448, Accuracy: 53.782%\n","Iteration step: 119; Loss: 1.443, Accuracy: 54.167%\n","Iteration step: 120; Loss: 1.437, Accuracy: 54.545%\n","Iteration step: 121; Loss: 1.432, Accuracy: 54.918%\n","Iteration step: 122; Loss: 1.426, Accuracy: 55.285%\n","Iteration step: 123; Loss: 1.433, Accuracy: 54.839%\n","Iteration step: 124; Loss: 1.441, Accuracy: 54.400%\n","Iteration step: 125; Loss: 1.449, Accuracy: 53.968%\n","Iteration step: 126; Loss: 1.442, Accuracy: 54.331%\n","Iteration step: 127; Loss: 1.436, Accuracy: 54.688%\n","Iteration step: 128; Loss: 1.443, Accuracy: 54.264%\n","Iteration step: 129; Loss: 1.446, Accuracy: 53.846%\n","Iteration step: 130; Loss: 1.453, Accuracy: 53.435%\n","Iteration step: 131; Loss: 1.447, Accuracy: 53.788%\n","Iteration step: 132; Loss: 1.453, Accuracy: 53.383%\n","Iteration step: 133; Loss: 1.447, Accuracy: 53.731%\n","Iteration step: 134; Loss: 1.441, Accuracy: 54.074%\n","Iteration step: 135; Loss: 1.435, Accuracy: 54.412%\n","Iteration step: 136; Loss: 1.429, Accuracy: 54.745%\n","Iteration step: 137; Loss: 1.436, Accuracy: 54.348%\n","Iteration step: 138; Loss: 1.430, Accuracy: 54.676%\n","Iteration step: 139; Loss: 1.438, Accuracy: 54.286%\n","Iteration step: 140; Loss: 1.432, Accuracy: 54.610%\n","Iteration step: 141; Loss: 1.439, Accuracy: 54.225%\n","Iteration step: 142; Loss: 1.433, Accuracy: 54.545%\n","Iteration step: 143; Loss: 1.427, Accuracy: 54.861%\n","Iteration step: 144; Loss: 1.431, Accuracy: 54.483%\n","Iteration step: 145; Loss: 1.425, Accuracy: 54.795%\n","Iteration step: 146; Loss: 1.420, Accuracy: 55.102%\n","Iteration step: 147; Loss: 1.425, Accuracy: 54.730%\n","Iteration step: 148; Loss: 1.430, Accuracy: 54.362%\n","Iteration step: 149; Loss: 1.424, Accuracy: 54.667%\n","Iteration step: 150; Loss: 1.429, Accuracy: 54.305%\n","Iteration step: 151; Loss: 1.423, Accuracy: 54.605%\n","Iteration step: 152; Loss: 1.417, Accuracy: 54.902%\n","Iteration step: 153; Loss: 1.424, Accuracy: 54.545%\n","Iteration step: 154; Loss: 1.418, Accuracy: 54.839%\n","Iteration step: 155; Loss: 1.423, Accuracy: 54.487%\n","Iteration step: 156; Loss: 1.417, Accuracy: 54.777%\n","Iteration step: 157; Loss: 1.424, Accuracy: 54.430%\n","Iteration step: 158; Loss: 1.428, Accuracy: 54.088%\n","Iteration step: 159; Loss: 1.423, Accuracy: 54.375%\n","Iteration step: 160; Loss: 1.418, Accuracy: 54.658%\n","Iteration step: 161; Loss: 1.412, Accuracy: 54.938%\n","Iteration step: 162; Loss: 1.407, Accuracy: 55.215%\n","Iteration step: 163; Loss: 1.402, Accuracy: 55.488%\n","Iteration step: 164; Loss: 1.408, Accuracy: 55.152%\n","Iteration step: 165; Loss: 1.432, Accuracy: 54.819%\n","Iteration step: 166; Loss: 1.436, Accuracy: 54.491%\n","Iteration step: 167; Loss: 1.443, Accuracy: 54.167%\n","Iteration step: 168; Loss: 1.438, Accuracy: 54.438%\n","Iteration step: 169; Loss: 1.433, Accuracy: 54.706%\n","Iteration step: 170; Loss: 1.438, Accuracy: 54.386%\n","Iteration step: 171; Loss: 1.434, Accuracy: 54.651%\n","Iteration step: 172; Loss: 1.429, Accuracy: 54.913%\n","Iteration step: 173; Loss: 1.425, Accuracy: 55.172%\n","Iteration step: 174; Loss: 1.420, Accuracy: 55.429%\n","Iteration step: 175; Loss: 1.416, Accuracy: 55.682%\n","Iteration step: 176; Loss: 1.411, Accuracy: 55.932%\n","Iteration step: 177; Loss: 1.416, Accuracy: 55.618%\n","Iteration step: 178; Loss: 1.420, Accuracy: 55.307%\n","Iteration step: 179; Loss: 1.423, Accuracy: 55.000%\n","Iteration step: 180; Loss: 1.419, Accuracy: 55.249%\n","Iteration step: 181; Loss: 1.414, Accuracy: 55.495%\n","Iteration step: 182; Loss: 1.410, Accuracy: 55.738%\n","Iteration step: 183; Loss: 1.413, Accuracy: 55.435%\n","Iteration step: 184; Loss: 1.419, Accuracy: 55.135%\n","Iteration step: 185; Loss: 1.414, Accuracy: 55.376%\n","Iteration step: 186; Loss: 1.417, Accuracy: 55.080%\n","Iteration step: 187; Loss: 1.423, Accuracy: 54.787%\n","Iteration step: 188; Loss: 1.428, Accuracy: 54.497%\n","Iteration step: 189; Loss: 1.433, Accuracy: 54.211%\n","Iteration step: 190; Loss: 1.438, Accuracy: 53.927%\n","Iteration step: 191; Loss: 1.433, Accuracy: 54.167%\n","Iteration step: 192; Loss: 1.429, Accuracy: 54.404%\n","Iteration step: 193; Loss: 1.425, Accuracy: 54.639%\n","Iteration step: 194; Loss: 1.422, Accuracy: 54.872%\n","Iteration step: 195; Loss: 1.424, Accuracy: 54.592%\n","Iteration step: 196; Loss: 1.420, Accuracy: 54.822%\n","Iteration step: 197; Loss: 1.416, Accuracy: 55.051%\n","Iteration step: 198; Loss: 1.412, Accuracy: 55.276%\n","Iteration step: 199; Loss: 1.417, Accuracy: 55.000%\n","Iteration step: 200; Loss: 1.422, Accuracy: 54.726%\n","Iteration step: 201; Loss: 1.426, Accuracy: 54.455%\n","Iteration step: 202; Loss: 1.430, Accuracy: 54.187%\n","Iteration step: 203; Loss: 1.432, Accuracy: 53.922%\n","Iteration step: 204; Loss: 1.436, Accuracy: 53.659%\n","Iteration step: 205; Loss: 1.440, Accuracy: 53.398%\n","Iteration step: 206; Loss: 1.442, Accuracy: 53.140%\n","Iteration step: 207; Loss: 1.445, Accuracy: 52.885%\n","Iteration step: 208; Loss: 1.449, Accuracy: 52.632%\n","Iteration step: 209; Loss: 1.450, Accuracy: 52.381%\n","Iteration step: 210; Loss: 1.453, Accuracy: 52.133%\n","Iteration step: 211; Loss: 1.450, Accuracy: 52.358%\n","Iteration step: 212; Loss: 1.452, Accuracy: 52.113%\n","Iteration step: 213; Loss: 1.455, Accuracy: 51.869%\n","Iteration step: 214; Loss: 1.452, Accuracy: 52.093%\n","Iteration step: 215; Loss: 1.454, Accuracy: 51.852%\n","Iteration step: 216; Loss: 1.458, Accuracy: 51.613%\n","Iteration step: 217; Loss: 1.455, Accuracy: 51.835%\n","Iteration step: 218; Loss: 1.457, Accuracy: 51.598%\n","Iteration step: 219; Loss: 1.461, Accuracy: 51.364%\n","Iteration step: 220; Loss: 1.462, Accuracy: 51.131%\n","Iteration step: 221; Loss: 1.459, Accuracy: 51.351%\n","Iteration step: 222; Loss: 1.457, Accuracy: 51.570%\n","Iteration step: 223; Loss: 1.455, Accuracy: 51.786%\n","Iteration step: 224; Loss: 1.456, Accuracy: 51.556%\n","Iteration step: 225; Loss: 1.454, Accuracy: 51.770%\n","Iteration step: 226; Loss: 1.456, Accuracy: 51.542%\n","Iteration step: 227; Loss: 1.454, Accuracy: 51.754%\n","Iteration step: 228; Loss: 1.455, Accuracy: 51.528%\n","Iteration step: 229; Loss: 1.458, Accuracy: 51.304%\n","Epoch: 3; Validation loss 1.441586971282959; acc: 0.5130434632301331\n","Current epoch: 4\n","Iteration step: 0; Loss: 0.979, Accuracy: 100.000%\n","Iteration step: 1; Loss: 0.977, Accuracy: 100.000%\n","Iteration step: 2; Loss: 0.974, Accuracy: 100.000%\n","Iteration step: 3; Loss: 1.143, Accuracy: 75.000%\n","Iteration step: 4; Loss: 1.243, Accuracy: 60.000%\n","Iteration step: 5; Loss: 1.393, Accuracy: 50.000%\n","Iteration step: 6; Loss: 1.329, Accuracy: 57.143%\n","Iteration step: 7; Loss: 1.406, Accuracy: 50.000%\n","Iteration step: 8; Loss: 1.354, Accuracy: 55.556%\n","Iteration step: 9; Loss: 1.312, Accuracy: 60.000%\n","Iteration step: 10; Loss: 1.276, Accuracy: 63.636%\n","Iteration step: 11; Loss: 1.349, Accuracy: 58.333%\n","Iteration step: 12; Loss: 1.407, Accuracy: 53.846%\n","Iteration step: 13; Loss: 1.460, Accuracy: 50.000%\n","Iteration step: 14; Loss: 1.422, Accuracy: 53.333%\n","Iteration step: 15; Loss: 1.389, Accuracy: 56.250%\n","Iteration step: 16; Loss: 1.359, Accuracy: 58.824%\n","Iteration step: 17; Loss: 1.376, Accuracy: 55.556%\n","Iteration step: 18; Loss: 1.349, Accuracy: 57.895%\n","Iteration step: 19; Loss: 1.387, Accuracy: 55.000%\n","Iteration step: 20; Loss: 1.421, Accuracy: 52.381%\n","Iteration step: 21; Loss: 1.394, Accuracy: 54.545%\n","Iteration step: 22; Loss: 1.407, Accuracy: 52.174%\n","Iteration step: 23; Loss: 1.383, Accuracy: 54.167%\n","Iteration step: 24; Loss: 1.415, Accuracy: 52.000%\n","Iteration step: 25; Loss: 1.445, Accuracy: 50.000%\n","Iteration step: 26; Loss: 1.547, Accuracy: 48.148%\n","Iteration step: 27; Loss: 1.552, Accuracy: 46.429%\n","Iteration step: 28; Loss: 1.572, Accuracy: 44.828%\n","Iteration step: 29; Loss: 1.591, Accuracy: 43.333%\n","Iteration step: 30; Loss: 1.610, Accuracy: 41.935%\n","Iteration step: 31; Loss: 1.624, Accuracy: 40.625%\n","Iteration step: 32; Loss: 1.602, Accuracy: 42.424%\n","Iteration step: 33; Loss: 1.605, Accuracy: 41.176%\n","Iteration step: 34; Loss: 1.585, Accuracy: 42.857%\n","Iteration step: 35; Loss: 1.567, Accuracy: 44.444%\n","Iteration step: 36; Loss: 1.578, Accuracy: 43.243%\n","Iteration step: 37; Loss: 1.561, Accuracy: 44.737%\n","Iteration step: 38; Loss: 1.545, Accuracy: 46.154%\n","Iteration step: 39; Loss: 1.529, Accuracy: 47.500%\n","Iteration step: 40; Loss: 1.515, Accuracy: 48.780%\n","Iteration step: 41; Loss: 1.519, Accuracy: 47.619%\n","Iteration step: 42; Loss: 1.529, Accuracy: 46.512%\n","Iteration step: 43; Loss: 1.545, Accuracy: 45.455%\n","Iteration step: 44; Loss: 1.531, Accuracy: 46.667%\n","Iteration step: 45; Loss: 1.517, Accuracy: 47.826%\n","Iteration step: 46; Loss: 1.503, Accuracy: 48.936%\n","Iteration step: 47; Loss: 1.490, Accuracy: 50.000%\n","Iteration step: 48; Loss: 1.502, Accuracy: 48.980%\n","Iteration step: 49; Loss: 1.506, Accuracy: 48.000%\n","Iteration step: 50; Loss: 1.494, Accuracy: 49.020%\n","Iteration step: 51; Loss: 1.481, Accuracy: 50.000%\n","Iteration step: 52; Loss: 1.469, Accuracy: 50.943%\n","Iteration step: 53; Loss: 1.457, Accuracy: 51.852%\n","Iteration step: 54; Loss: 1.445, Accuracy: 52.727%\n","Iteration step: 55; Loss: 1.433, Accuracy: 53.571%\n","Iteration step: 56; Loss: 1.421, Accuracy: 54.386%\n","Iteration step: 57; Loss: 1.409, Accuracy: 55.172%\n","Iteration step: 58; Loss: 1.397, Accuracy: 55.932%\n","Iteration step: 59; Loss: 1.405, Accuracy: 55.000%\n","Iteration step: 60; Loss: 1.412, Accuracy: 54.098%\n","Iteration step: 61; Loss: 1.419, Accuracy: 53.226%\n","Iteration step: 62; Loss: 1.436, Accuracy: 52.381%\n","Iteration step: 63; Loss: 1.424, Accuracy: 53.125%\n","Iteration step: 64; Loss: 1.436, Accuracy: 52.308%\n","Iteration step: 65; Loss: 1.424, Accuracy: 53.030%\n","Iteration step: 66; Loss: 1.412, Accuracy: 53.731%\n","Iteration step: 67; Loss: 1.400, Accuracy: 54.412%\n","Iteration step: 68; Loss: 1.389, Accuracy: 55.072%\n","Iteration step: 69; Loss: 1.377, Accuracy: 55.714%\n","Iteration step: 70; Loss: 1.366, Accuracy: 56.338%\n","Iteration step: 71; Loss: 1.381, Accuracy: 55.556%\n","Iteration step: 72; Loss: 1.369, Accuracy: 56.164%\n","Iteration step: 73; Loss: 1.376, Accuracy: 55.405%\n","Iteration step: 74; Loss: 1.365, Accuracy: 56.000%\n","Iteration step: 75; Loss: 1.382, Accuracy: 55.263%\n","Iteration step: 76; Loss: 1.399, Accuracy: 54.545%\n","Iteration step: 77; Loss: 1.413, Accuracy: 53.846%\n","Iteration step: 78; Loss: 1.402, Accuracy: 54.430%\n","Iteration step: 79; Loss: 1.408, Accuracy: 53.750%\n","Iteration step: 80; Loss: 1.397, Accuracy: 54.321%\n","Iteration step: 81; Loss: 1.387, Accuracy: 54.878%\n","Iteration step: 82; Loss: 1.393, Accuracy: 54.217%\n","Iteration step: 83; Loss: 1.399, Accuracy: 53.571%\n","Iteration step: 84; Loss: 1.389, Accuracy: 54.118%\n","Iteration step: 85; Loss: 1.379, Accuracy: 54.651%\n","Iteration step: 86; Loss: 1.384, Accuracy: 54.023%\n","Iteration step: 87; Loss: 1.375, Accuracy: 54.545%\n","Iteration step: 88; Loss: 1.388, Accuracy: 53.933%\n","Iteration step: 89; Loss: 1.400, Accuracy: 53.333%\n","Iteration step: 90; Loss: 1.391, Accuracy: 53.846%\n","Iteration step: 91; Loss: 1.395, Accuracy: 53.261%\n","Iteration step: 92; Loss: 1.406, Accuracy: 52.688%\n","Iteration step: 93; Loss: 1.410, Accuracy: 52.128%\n","Iteration step: 94; Loss: 1.401, Accuracy: 52.632%\n","Iteration step: 95; Loss: 1.412, Accuracy: 52.083%\n","Iteration step: 96; Loss: 1.404, Accuracy: 52.577%\n","Iteration step: 97; Loss: 1.414, Accuracy: 52.041%\n","Iteration step: 98; Loss: 1.446, Accuracy: 51.515%\n","Iteration step: 99; Loss: 1.438, Accuracy: 52.000%\n","Iteration step: 100; Loss: 1.431, Accuracy: 52.475%\n","Iteration step: 101; Loss: 1.424, Accuracy: 52.941%\n","Iteration step: 102; Loss: 1.417, Accuracy: 53.398%\n","Iteration step: 103; Loss: 1.426, Accuracy: 52.885%\n","Iteration step: 104; Loss: 1.419, Accuracy: 53.333%\n","Iteration step: 105; Loss: 1.421, Accuracy: 52.830%\n","Iteration step: 106; Loss: 1.414, Accuracy: 53.271%\n","Iteration step: 107; Loss: 1.423, Accuracy: 52.778%\n","Iteration step: 108; Loss: 1.416, Accuracy: 53.211%\n","Iteration step: 109; Loss: 1.410, Accuracy: 53.636%\n","Iteration step: 110; Loss: 1.418, Accuracy: 53.153%\n","Iteration step: 111; Loss: 1.411, Accuracy: 53.571%\n","Iteration step: 112; Loss: 1.414, Accuracy: 53.097%\n","Iteration step: 113; Loss: 1.407, Accuracy: 53.509%\n","Iteration step: 114; Loss: 1.416, Accuracy: 53.043%\n","Iteration step: 115; Loss: 1.409, Accuracy: 53.448%\n","Iteration step: 116; Loss: 1.403, Accuracy: 53.846%\n","Iteration step: 117; Loss: 1.397, Accuracy: 54.237%\n","Iteration step: 118; Loss: 1.391, Accuracy: 54.622%\n","Iteration step: 119; Loss: 1.385, Accuracy: 55.000%\n","Iteration step: 120; Loss: 1.379, Accuracy: 55.372%\n","Iteration step: 121; Loss: 1.387, Accuracy: 54.918%\n","Iteration step: 122; Loss: 1.381, Accuracy: 55.285%\n","Iteration step: 123; Loss: 1.375, Accuracy: 55.645%\n","Iteration step: 124; Loss: 1.383, Accuracy: 55.200%\n","Iteration step: 125; Loss: 1.392, Accuracy: 54.762%\n","Iteration step: 126; Loss: 1.385, Accuracy: 55.118%\n","Iteration step: 127; Loss: 1.393, Accuracy: 54.688%\n","Iteration step: 128; Loss: 1.397, Accuracy: 54.264%\n","Iteration step: 129; Loss: 1.391, Accuracy: 54.615%\n","Iteration step: 130; Loss: 1.398, Accuracy: 54.198%\n","Iteration step: 131; Loss: 1.392, Accuracy: 54.545%\n","Iteration step: 132; Loss: 1.400, Accuracy: 54.135%\n","Iteration step: 133; Loss: 1.407, Accuracy: 53.731%\n","Iteration step: 134; Loss: 1.401, Accuracy: 54.074%\n","Iteration step: 135; Loss: 1.395, Accuracy: 54.412%\n","Iteration step: 136; Loss: 1.403, Accuracy: 54.015%\n","Iteration step: 137; Loss: 1.407, Accuracy: 53.623%\n","Iteration step: 138; Loss: 1.401, Accuracy: 53.957%\n","Iteration step: 139; Loss: 1.395, Accuracy: 54.286%\n","Iteration step: 140; Loss: 1.403, Accuracy: 53.901%\n","Iteration step: 141; Loss: 1.410, Accuracy: 53.521%\n","Iteration step: 142; Loss: 1.405, Accuracy: 53.846%\n","Iteration step: 143; Loss: 1.399, Accuracy: 54.167%\n","Iteration step: 144; Loss: 1.394, Accuracy: 54.483%\n","Iteration step: 145; Loss: 1.389, Accuracy: 54.795%\n","Iteration step: 146; Loss: 1.383, Accuracy: 55.102%\n","Iteration step: 147; Loss: 1.387, Accuracy: 54.730%\n","Iteration step: 148; Loss: 1.393, Accuracy: 54.362%\n","Iteration step: 149; Loss: 1.388, Accuracy: 54.667%\n","Iteration step: 150; Loss: 1.394, Accuracy: 54.305%\n","Iteration step: 151; Loss: 1.388, Accuracy: 54.605%\n","Iteration step: 152; Loss: 1.383, Accuracy: 54.902%\n","Iteration step: 153; Loss: 1.378, Accuracy: 55.195%\n","Iteration step: 154; Loss: 1.384, Accuracy: 54.839%\n","Iteration step: 155; Loss: 1.379, Accuracy: 55.128%\n","Iteration step: 156; Loss: 1.373, Accuracy: 55.414%\n","Iteration step: 157; Loss: 1.380, Accuracy: 55.063%\n","Iteration step: 158; Loss: 1.375, Accuracy: 55.346%\n","Iteration step: 159; Loss: 1.381, Accuracy: 55.000%\n","Iteration step: 160; Loss: 1.376, Accuracy: 55.280%\n","Iteration step: 161; Loss: 1.383, Accuracy: 54.938%\n","Iteration step: 162; Loss: 1.387, Accuracy: 54.601%\n","Iteration step: 163; Loss: 1.382, Accuracy: 54.878%\n","Iteration step: 164; Loss: 1.377, Accuracy: 55.152%\n","Iteration step: 165; Loss: 1.372, Accuracy: 55.422%\n","Iteration step: 166; Loss: 1.391, Accuracy: 55.090%\n","Iteration step: 167; Loss: 1.395, Accuracy: 54.762%\n","Iteration step: 168; Loss: 1.390, Accuracy: 55.030%\n","Iteration step: 169; Loss: 1.394, Accuracy: 54.706%\n","Iteration step: 170; Loss: 1.398, Accuracy: 54.386%\n","Iteration step: 171; Loss: 1.403, Accuracy: 54.070%\n","Iteration step: 172; Loss: 1.399, Accuracy: 54.335%\n","Iteration step: 173; Loss: 1.394, Accuracy: 54.598%\n","Iteration step: 174; Loss: 1.390, Accuracy: 54.857%\n","Iteration step: 175; Loss: 1.395, Accuracy: 54.545%\n","Iteration step: 176; Loss: 1.390, Accuracy: 54.802%\n","Iteration step: 177; Loss: 1.386, Accuracy: 55.056%\n","Iteration step: 178; Loss: 1.382, Accuracy: 55.307%\n","Iteration step: 179; Loss: 1.385, Accuracy: 55.000%\n","Iteration step: 180; Loss: 1.381, Accuracy: 55.249%\n","Iteration step: 181; Loss: 1.376, Accuracy: 55.495%\n","Iteration step: 182; Loss: 1.372, Accuracy: 55.738%\n","Iteration step: 183; Loss: 1.378, Accuracy: 55.435%\n","Iteration step: 184; Loss: 1.384, Accuracy: 55.135%\n","Iteration step: 185; Loss: 1.380, Accuracy: 55.376%\n","Iteration step: 186; Loss: 1.385, Accuracy: 55.080%\n","Iteration step: 187; Loss: 1.381, Accuracy: 55.319%\n","Iteration step: 188; Loss: 1.384, Accuracy: 55.026%\n","Iteration step: 189; Loss: 1.387, Accuracy: 54.737%\n","Iteration step: 190; Loss: 1.392, Accuracy: 54.450%\n","Iteration step: 191; Loss: 1.397, Accuracy: 54.167%\n","Iteration step: 192; Loss: 1.393, Accuracy: 54.404%\n","Iteration step: 193; Loss: 1.389, Accuracy: 54.639%\n","Iteration step: 194; Loss: 1.385, Accuracy: 54.872%\n","Iteration step: 195; Loss: 1.388, Accuracy: 54.592%\n","Iteration step: 196; Loss: 1.384, Accuracy: 54.822%\n","Iteration step: 197; Loss: 1.380, Accuracy: 55.051%\n","Iteration step: 198; Loss: 1.376, Accuracy: 55.276%\n","Iteration step: 199; Loss: 1.381, Accuracy: 55.000%\n","Iteration step: 200; Loss: 1.386, Accuracy: 54.726%\n","Iteration step: 201; Loss: 1.391, Accuracy: 54.455%\n","Iteration step: 202; Loss: 1.393, Accuracy: 54.187%\n","Iteration step: 203; Loss: 1.398, Accuracy: 53.922%\n","Iteration step: 204; Loss: 1.400, Accuracy: 53.659%\n","Iteration step: 205; Loss: 1.404, Accuracy: 53.398%\n","Iteration step: 206; Loss: 1.408, Accuracy: 53.140%\n","Iteration step: 207; Loss: 1.412, Accuracy: 52.885%\n","Iteration step: 208; Loss: 1.416, Accuracy: 52.632%\n","Iteration step: 209; Loss: 1.413, Accuracy: 52.857%\n","Iteration step: 210; Loss: 1.417, Accuracy: 52.607%\n","Iteration step: 211; Loss: 1.418, Accuracy: 52.358%\n","Iteration step: 212; Loss: 1.420, Accuracy: 52.113%\n","Iteration step: 213; Loss: 1.423, Accuracy: 51.869%\n","Iteration step: 214; Loss: 1.420, Accuracy: 52.093%\n","Iteration step: 215; Loss: 1.418, Accuracy: 52.315%\n","Iteration step: 216; Loss: 1.421, Accuracy: 52.074%\n","Iteration step: 217; Loss: 1.419, Accuracy: 52.294%\n","Iteration step: 218; Loss: 1.422, Accuracy: 52.055%\n","Iteration step: 219; Loss: 1.425, Accuracy: 51.818%\n","Iteration step: 220; Loss: 1.423, Accuracy: 52.036%\n","Iteration step: 221; Loss: 1.424, Accuracy: 51.802%\n","Iteration step: 222; Loss: 1.422, Accuracy: 52.018%\n","Iteration step: 223; Loss: 1.419, Accuracy: 52.232%\n","Iteration step: 224; Loss: 1.417, Accuracy: 52.444%\n","Iteration step: 225; Loss: 1.420, Accuracy: 52.212%\n","Iteration step: 226; Loss: 1.423, Accuracy: 51.982%\n","Iteration step: 227; Loss: 1.424, Accuracy: 51.754%\n","Iteration step: 228; Loss: 1.427, Accuracy: 51.528%\n","Iteration step: 229; Loss: 1.428, Accuracy: 51.304%\n","Epoch: 4; Validation loss 1.4252182245254517; acc: 0.5130434632301331\n","Current epoch: 5\n","Iteration step: 0; Loss: 1.701, Accuracy: 0.000%\n","Iteration step: 1; Loss: 1.695, Accuracy: 0.000%\n","Iteration step: 2; Loss: 1.435, Accuracy: 33.333%\n","Iteration step: 3; Loss: 1.306, Accuracy: 50.000%\n","Iteration step: 4; Loss: 1.229, Accuracy: 60.000%\n","Iteration step: 5; Loss: 1.177, Accuracy: 66.667%\n","Iteration step: 6; Loss: 1.139, Accuracy: 71.429%\n","Iteration step: 7; Loss: 1.267, Accuracy: 62.500%\n","Iteration step: 8; Loss: 1.365, Accuracy: 55.556%\n","Iteration step: 9; Loss: 1.318, Accuracy: 60.000%\n","Iteration step: 10; Loss: 1.279, Accuracy: 63.636%\n","Iteration step: 11; Loss: 1.343, Accuracy: 58.333%\n","Iteration step: 12; Loss: 1.307, Accuracy: 61.538%\n","Iteration step: 13; Loss: 1.366, Accuracy: 57.143%\n","Iteration step: 14; Loss: 1.333, Accuracy: 60.000%\n","Iteration step: 15; Loss: 1.307, Accuracy: 62.500%\n","Iteration step: 16; Loss: 1.328, Accuracy: 58.824%\n","Iteration step: 17; Loss: 1.373, Accuracy: 55.556%\n","Iteration step: 18; Loss: 1.389, Accuracy: 52.632%\n","Iteration step: 19; Loss: 1.361, Accuracy: 55.000%\n","Iteration step: 20; Loss: 1.400, Accuracy: 52.381%\n","Iteration step: 21; Loss: 1.432, Accuracy: 50.000%\n","Iteration step: 22; Loss: 1.406, Accuracy: 52.174%\n","Iteration step: 23; Loss: 1.417, Accuracy: 50.000%\n","Iteration step: 24; Loss: 1.448, Accuracy: 48.000%\n","Iteration step: 25; Loss: 1.424, Accuracy: 50.000%\n","Iteration step: 26; Loss: 1.453, Accuracy: 48.148%\n","Iteration step: 27; Loss: 1.431, Accuracy: 50.000%\n","Iteration step: 28; Loss: 1.456, Accuracy: 48.276%\n","Iteration step: 29; Loss: 1.543, Accuracy: 46.667%\n","Iteration step: 30; Loss: 1.563, Accuracy: 45.161%\n","Iteration step: 31; Loss: 1.582, Accuracy: 43.750%\n","Iteration step: 32; Loss: 1.560, Accuracy: 45.455%\n","Iteration step: 33; Loss: 1.540, Accuracy: 47.059%\n","Iteration step: 34; Loss: 1.544, Accuracy: 45.714%\n","Iteration step: 35; Loss: 1.526, Accuracy: 47.222%\n","Iteration step: 36; Loss: 1.508, Accuracy: 48.649%\n","Iteration step: 37; Loss: 1.522, Accuracy: 47.368%\n","Iteration step: 38; Loss: 1.506, Accuracy: 48.718%\n","Iteration step: 39; Loss: 1.519, Accuracy: 47.500%\n","Iteration step: 40; Loss: 1.531, Accuracy: 46.341%\n","Iteration step: 41; Loss: 1.536, Accuracy: 45.238%\n","Iteration step: 42; Loss: 1.520, Accuracy: 46.512%\n","Iteration step: 43; Loss: 1.532, Accuracy: 45.455%\n","Iteration step: 44; Loss: 1.517, Accuracy: 46.667%\n","Iteration step: 45; Loss: 1.504, Accuracy: 47.826%\n","Iteration step: 46; Loss: 1.490, Accuracy: 48.936%\n","Iteration step: 47; Loss: 1.495, Accuracy: 47.917%\n","Iteration step: 48; Loss: 1.482, Accuracy: 48.980%\n","Iteration step: 49; Loss: 1.497, Accuracy: 48.000%\n","Iteration step: 50; Loss: 1.485, Accuracy: 49.020%\n","Iteration step: 51; Loss: 1.472, Accuracy: 50.000%\n","Iteration step: 52; Loss: 1.460, Accuracy: 50.943%\n","Iteration step: 53; Loss: 1.448, Accuracy: 51.852%\n","Iteration step: 54; Loss: 1.453, Accuracy: 50.909%\n","Iteration step: 55; Loss: 1.441, Accuracy: 51.786%\n","Iteration step: 56; Loss: 1.429, Accuracy: 52.632%\n","Iteration step: 57; Loss: 1.417, Accuracy: 53.448%\n","Iteration step: 58; Loss: 1.423, Accuracy: 52.542%\n","Iteration step: 59; Loss: 1.429, Accuracy: 51.667%\n","Iteration step: 60; Loss: 1.417, Accuracy: 52.459%\n","Iteration step: 61; Loss: 1.404, Accuracy: 53.226%\n","Iteration step: 62; Loss: 1.422, Accuracy: 52.381%\n","Iteration step: 63; Loss: 1.435, Accuracy: 51.562%\n","Iteration step: 64; Loss: 1.422, Accuracy: 52.308%\n","Iteration step: 65; Loss: 1.410, Accuracy: 53.030%\n","Iteration step: 66; Loss: 1.398, Accuracy: 53.731%\n","Iteration step: 67; Loss: 1.385, Accuracy: 54.412%\n","Iteration step: 68; Loss: 1.373, Accuracy: 55.072%\n","Iteration step: 69; Loss: 1.361, Accuracy: 55.714%\n","Iteration step: 70; Loss: 1.379, Accuracy: 54.930%\n","Iteration step: 71; Loss: 1.367, Accuracy: 55.556%\n","Iteration step: 72; Loss: 1.384, Accuracy: 54.795%\n","Iteration step: 73; Loss: 1.372, Accuracy: 55.405%\n","Iteration step: 74; Loss: 1.359, Accuracy: 56.000%\n","Iteration step: 75; Loss: 1.347, Accuracy: 56.579%\n","Iteration step: 76; Loss: 1.367, Accuracy: 55.844%\n","Iteration step: 77; Loss: 1.385, Accuracy: 55.128%\n","Iteration step: 78; Loss: 1.373, Accuracy: 55.696%\n","Iteration step: 79; Loss: 1.382, Accuracy: 55.000%\n","Iteration step: 80; Loss: 1.391, Accuracy: 54.321%\n","Iteration step: 81; Loss: 1.400, Accuracy: 53.659%\n","Iteration step: 82; Loss: 1.388, Accuracy: 54.217%\n","Iteration step: 83; Loss: 1.378, Accuracy: 54.762%\n","Iteration step: 84; Loss: 1.366, Accuracy: 55.294%\n","Iteration step: 85; Loss: 1.380, Accuracy: 54.651%\n","Iteration step: 86; Loss: 1.386, Accuracy: 54.023%\n","Iteration step: 87; Loss: 1.392, Accuracy: 53.409%\n","Iteration step: 88; Loss: 1.404, Accuracy: 52.809%\n","Iteration step: 89; Loss: 1.409, Accuracy: 52.222%\n","Iteration step: 90; Loss: 1.400, Accuracy: 52.747%\n","Iteration step: 91; Loss: 1.389, Accuracy: 53.261%\n","Iteration step: 92; Loss: 1.399, Accuracy: 52.688%\n","Iteration step: 93; Loss: 1.402, Accuracy: 52.128%\n","Iteration step: 94; Loss: 1.413, Accuracy: 51.579%\n","Iteration step: 95; Loss: 1.445, Accuracy: 51.042%\n","Iteration step: 96; Loss: 1.455, Accuracy: 50.515%\n","Iteration step: 97; Loss: 1.443, Accuracy: 51.020%\n","Iteration step: 98; Loss: 1.436, Accuracy: 51.515%\n","Iteration step: 99; Loss: 1.429, Accuracy: 52.000%\n","Iteration step: 100; Loss: 1.436, Accuracy: 51.485%\n","Iteration step: 101; Loss: 1.430, Accuracy: 51.961%\n","Iteration step: 102; Loss: 1.432, Accuracy: 51.456%\n","Iteration step: 103; Loss: 1.425, Accuracy: 51.923%\n","Iteration step: 104; Loss: 1.419, Accuracy: 52.381%\n","Iteration step: 105; Loss: 1.413, Accuracy: 52.830%\n","Iteration step: 106; Loss: 1.405, Accuracy: 53.271%\n","Iteration step: 107; Loss: 1.413, Accuracy: 52.778%\n","Iteration step: 108; Loss: 1.407, Accuracy: 53.211%\n","Iteration step: 109; Loss: 1.400, Accuracy: 53.636%\n","Iteration step: 110; Loss: 1.391, Accuracy: 54.054%\n","Iteration step: 111; Loss: 1.379, Accuracy: 54.464%\n","Iteration step: 112; Loss: 1.406, Accuracy: 53.982%\n","Iteration step: 113; Loss: 1.453, Accuracy: 53.509%\n","Iteration step: 114; Loss: 1.463, Accuracy: 53.043%\n","Iteration step: 115; Loss: 1.483, Accuracy: 52.586%\n","Iteration step: 116; Loss: 1.476, Accuracy: 52.991%\n","Iteration step: 117; Loss: 1.469, Accuracy: 53.390%\n","Iteration step: 118; Loss: 1.464, Accuracy: 53.782%\n","Iteration step: 119; Loss: 1.470, Accuracy: 53.333%\n","Iteration step: 120; Loss: 1.465, Accuracy: 53.719%\n","Iteration step: 121; Loss: 1.461, Accuracy: 54.098%\n","Iteration step: 122; Loss: 1.456, Accuracy: 54.472%\n","Iteration step: 123; Loss: 1.452, Accuracy: 54.839%\n","Iteration step: 124; Loss: 1.457, Accuracy: 54.400%\n","Iteration step: 125; Loss: 1.452, Accuracy: 54.762%\n","Iteration step: 126; Loss: 1.448, Accuracy: 55.118%\n","Iteration step: 127; Loss: 1.454, Accuracy: 54.688%\n","Iteration step: 128; Loss: 1.449, Accuracy: 55.039%\n","Iteration step: 129; Loss: 1.450, Accuracy: 54.615%\n","Iteration step: 130; Loss: 1.456, Accuracy: 54.198%\n","Iteration step: 131; Loss: 1.461, Accuracy: 53.788%\n","Iteration step: 132; Loss: 1.467, Accuracy: 53.383%\n","Iteration step: 133; Loss: 1.462, Accuracy: 53.731%\n","Iteration step: 134; Loss: 1.467, Accuracy: 53.333%\n","Iteration step: 135; Loss: 1.461, Accuracy: 53.676%\n","Iteration step: 136; Loss: 1.456, Accuracy: 54.015%\n","Iteration step: 137; Loss: 1.452, Accuracy: 54.348%\n","Iteration step: 138; Loss: 1.458, Accuracy: 53.957%\n","Iteration step: 139; Loss: 1.464, Accuracy: 53.571%\n","Iteration step: 140; Loss: 1.459, Accuracy: 53.901%\n","Iteration step: 141; Loss: 1.453, Accuracy: 54.225%\n","Iteration step: 142; Loss: 1.448, Accuracy: 54.545%\n","Iteration step: 143; Loss: 1.451, Accuracy: 54.167%\n","Iteration step: 144; Loss: 1.457, Accuracy: 53.793%\n","Iteration step: 145; Loss: 1.452, Accuracy: 54.110%\n","Iteration step: 146; Loss: 1.446, Accuracy: 54.422%\n","Iteration step: 147; Loss: 1.441, Accuracy: 54.730%\n","Iteration step: 148; Loss: 1.436, Accuracy: 55.034%\n","Iteration step: 149; Loss: 1.430, Accuracy: 55.333%\n","Iteration step: 150; Loss: 1.436, Accuracy: 54.967%\n","Iteration step: 151; Loss: 1.442, Accuracy: 54.605%\n","Iteration step: 152; Loss: 1.436, Accuracy: 54.902%\n","Iteration step: 153; Loss: 1.430, Accuracy: 55.195%\n","Iteration step: 154; Loss: 1.425, Accuracy: 55.484%\n","Iteration step: 155; Loss: 1.419, Accuracy: 55.769%\n","Iteration step: 156; Loss: 1.424, Accuracy: 55.414%\n","Iteration step: 157; Loss: 1.431, Accuracy: 55.063%\n","Iteration step: 158; Loss: 1.436, Accuracy: 54.717%\n","Iteration step: 159; Loss: 1.442, Accuracy: 54.375%\n","Iteration step: 160; Loss: 1.433, Accuracy: 54.658%\n","Iteration step: 161; Loss: 1.424, Accuracy: 54.938%\n","Iteration step: 162; Loss: 1.416, Accuracy: 55.215%\n","Iteration step: 163; Loss: 1.423, Accuracy: 54.878%\n","Iteration step: 164; Loss: 1.417, Accuracy: 55.152%\n","Iteration step: 165; Loss: 1.435, Accuracy: 54.819%\n","Iteration step: 166; Loss: 1.427, Accuracy: 55.090%\n","Iteration step: 167; Loss: 1.422, Accuracy: 55.357%\n","Iteration step: 168; Loss: 1.428, Accuracy: 55.030%\n","Iteration step: 169; Loss: 1.423, Accuracy: 55.294%\n","Iteration step: 170; Loss: 1.416, Accuracy: 55.556%\n","Iteration step: 171; Loss: 1.421, Accuracy: 55.233%\n","Iteration step: 172; Loss: 1.425, Accuracy: 54.913%\n","Iteration step: 173; Loss: 1.421, Accuracy: 55.172%\n","Iteration step: 174; Loss: 1.416, Accuracy: 55.429%\n","Iteration step: 175; Loss: 1.421, Accuracy: 55.114%\n","Iteration step: 176; Loss: 1.415, Accuracy: 55.367%\n","Iteration step: 177; Loss: 1.411, Accuracy: 55.618%\n","Iteration step: 178; Loss: 1.407, Accuracy: 55.866%\n","Iteration step: 179; Loss: 1.411, Accuracy: 55.556%\n","Iteration step: 180; Loss: 1.403, Accuracy: 55.801%\n","Iteration step: 181; Loss: 1.409, Accuracy: 55.495%\n","Iteration step: 182; Loss: 1.412, Accuracy: 55.191%\n","Iteration step: 183; Loss: 1.417, Accuracy: 54.891%\n","Iteration step: 184; Loss: 1.410, Accuracy: 55.135%\n","Iteration step: 185; Loss: 1.402, Accuracy: 55.376%\n","Iteration step: 186; Loss: 1.406, Accuracy: 55.080%\n","Iteration step: 187; Loss: 1.409, Accuracy: 54.787%\n","Iteration step: 188; Loss: 1.405, Accuracy: 55.026%\n","Iteration step: 189; Loss: 1.409, Accuracy: 54.737%\n","Iteration step: 190; Loss: 1.406, Accuracy: 54.974%\n","Iteration step: 191; Loss: 1.409, Accuracy: 54.688%\n","Iteration step: 192; Loss: 1.402, Accuracy: 54.922%\n","Iteration step: 193; Loss: 1.399, Accuracy: 55.155%\n","Iteration step: 194; Loss: 1.402, Accuracy: 54.872%\n","Iteration step: 195; Loss: 1.412, Accuracy: 54.592%\n","Iteration step: 196; Loss: 1.410, Accuracy: 54.822%\n","Iteration step: 197; Loss: 1.411, Accuracy: 54.545%\n","Iteration step: 198; Loss: 1.408, Accuracy: 54.774%\n","Iteration step: 199; Loss: 1.412, Accuracy: 54.500%\n","Iteration step: 200; Loss: 1.415, Accuracy: 54.229%\n","Iteration step: 201; Loss: 1.416, Accuracy: 53.960%\n","Iteration step: 202; Loss: 1.419, Accuracy: 53.695%\n","Iteration step: 203; Loss: 1.420, Accuracy: 53.431%\n","Iteration step: 204; Loss: 1.423, Accuracy: 53.171%\n","Iteration step: 205; Loss: 1.420, Accuracy: 53.398%\n","Iteration step: 206; Loss: 1.423, Accuracy: 53.140%\n","Iteration step: 207; Loss: 1.427, Accuracy: 52.885%\n","Iteration step: 208; Loss: 1.430, Accuracy: 52.632%\n","Iteration step: 209; Loss: 1.428, Accuracy: 52.857%\n","Iteration step: 210; Loss: 1.428, Accuracy: 52.607%\n","Iteration step: 211; Loss: 1.431, Accuracy: 52.358%\n","Iteration step: 212; Loss: 1.429, Accuracy: 52.582%\n","Iteration step: 213; Loss: 1.432, Accuracy: 52.336%\n","Iteration step: 214; Loss: 1.433, Accuracy: 52.093%\n","Iteration step: 215; Loss: 1.431, Accuracy: 52.315%\n","Iteration step: 216; Loss: 1.434, Accuracy: 52.074%\n","Iteration step: 217; Loss: 1.432, Accuracy: 52.294%\n","Iteration step: 218; Loss: 1.435, Accuracy: 52.055%\n","Iteration step: 219; Loss: 1.438, Accuracy: 51.818%\n","Iteration step: 220; Loss: 1.436, Accuracy: 52.036%\n","Iteration step: 221; Loss: 1.434, Accuracy: 52.252%\n","Iteration step: 222; Loss: 1.437, Accuracy: 52.018%\n","Iteration step: 223; Loss: 1.435, Accuracy: 52.232%\n","Iteration step: 224; Loss: 1.436, Accuracy: 52.000%\n","Iteration step: 225; Loss: 1.434, Accuracy: 52.212%\n","Iteration step: 226; Loss: 1.435, Accuracy: 51.982%\n","Iteration step: 227; Loss: 1.437, Accuracy: 51.754%\n","Iteration step: 228; Loss: 1.438, Accuracy: 51.528%\n","Iteration step: 229; Loss: 1.441, Accuracy: 51.304%\n","Epoch: 5; Validation loss 1.451865553855896; acc: 0.5130434632301331\n","Current epoch: 6\n","Iteration step: 0; Loss: 1.521, Accuracy: 0.000%\n","Iteration step: 1; Loss: 1.282, Accuracy: 50.000%\n","Iteration step: 2; Loss: 1.202, Accuracy: 66.667%\n","Iteration step: 3; Loss: 1.162, Accuracy: 75.000%\n","Iteration step: 4; Loss: 1.136, Accuracy: 80.000%\n","Iteration step: 5; Loss: 1.195, Accuracy: 66.667%\n","Iteration step: 6; Loss: 1.311, Accuracy: 57.143%\n","Iteration step: 7; Loss: 1.415, Accuracy: 50.000%\n","Iteration step: 8; Loss: 1.370, Accuracy: 55.556%\n","Iteration step: 9; Loss: 1.333, Accuracy: 60.000%\n","Iteration step: 10; Loss: 1.397, Accuracy: 54.545%\n","Iteration step: 11; Loss: 1.459, Accuracy: 50.000%\n","Iteration step: 12; Loss: 1.422, Accuracy: 53.846%\n","Iteration step: 13; Loss: 1.472, Accuracy: 50.000%\n","Iteration step: 14; Loss: 1.519, Accuracy: 46.667%\n","Iteration step: 15; Loss: 1.485, Accuracy: 50.000%\n","Iteration step: 16; Loss: 1.455, Accuracy: 52.941%\n","Iteration step: 17; Loss: 1.428, Accuracy: 55.556%\n","Iteration step: 18; Loss: 1.403, Accuracy: 57.895%\n","Iteration step: 19; Loss: 1.411, Accuracy: 55.000%\n","Iteration step: 20; Loss: 1.388, Accuracy: 57.143%\n","Iteration step: 21; Loss: 1.397, Accuracy: 54.545%\n","Iteration step: 22; Loss: 1.427, Accuracy: 52.174%\n","Iteration step: 23; Loss: 1.537, Accuracy: 50.000%\n","Iteration step: 24; Loss: 1.559, Accuracy: 48.000%\n","Iteration step: 25; Loss: 1.579, Accuracy: 46.154%\n","Iteration step: 26; Loss: 1.597, Accuracy: 44.444%\n","Iteration step: 27; Loss: 1.574, Accuracy: 46.429%\n","Iteration step: 28; Loss: 1.589, Accuracy: 44.828%\n","Iteration step: 29; Loss: 1.608, Accuracy: 43.333%\n","Iteration step: 30; Loss: 1.587, Accuracy: 45.161%\n","Iteration step: 31; Loss: 1.587, Accuracy: 43.750%\n","Iteration step: 32; Loss: 1.568, Accuracy: 45.455%\n","Iteration step: 33; Loss: 1.569, Accuracy: 44.118%\n","Iteration step: 34; Loss: 1.552, Accuracy: 45.714%\n","Iteration step: 35; Loss: 1.564, Accuracy: 44.444%\n","Iteration step: 36; Loss: 1.547, Accuracy: 45.946%\n","Iteration step: 37; Loss: 1.532, Accuracy: 47.368%\n","Iteration step: 38; Loss: 1.544, Accuracy: 46.154%\n","Iteration step: 39; Loss: 1.530, Accuracy: 47.500%\n","Iteration step: 40; Loss: 1.546, Accuracy: 46.341%\n","Iteration step: 41; Loss: 1.531, Accuracy: 47.619%\n","Iteration step: 42; Loss: 1.541, Accuracy: 46.512%\n","Iteration step: 43; Loss: 1.552, Accuracy: 45.455%\n","Iteration step: 44; Loss: 1.538, Accuracy: 46.667%\n","Iteration step: 45; Loss: 1.525, Accuracy: 47.826%\n","Iteration step: 46; Loss: 1.512, Accuracy: 48.936%\n","Iteration step: 47; Loss: 1.500, Accuracy: 50.000%\n","Iteration step: 48; Loss: 1.504, Accuracy: 48.980%\n","Iteration step: 49; Loss: 1.491, Accuracy: 50.000%\n","Iteration step: 50; Loss: 1.495, Accuracy: 49.020%\n","Iteration step: 51; Loss: 1.483, Accuracy: 50.000%\n","Iteration step: 52; Loss: 1.471, Accuracy: 50.943%\n","Iteration step: 53; Loss: 1.476, Accuracy: 50.000%\n","Iteration step: 54; Loss: 1.464, Accuracy: 50.909%\n","Iteration step: 55; Loss: 1.453, Accuracy: 51.786%\n","Iteration step: 56; Loss: 1.441, Accuracy: 52.632%\n","Iteration step: 57; Loss: 1.452, Accuracy: 51.724%\n","Iteration step: 58; Loss: 1.457, Accuracy: 50.847%\n","Iteration step: 59; Loss: 1.445, Accuracy: 51.667%\n","Iteration step: 60; Loss: 1.434, Accuracy: 52.459%\n","Iteration step: 61; Loss: 1.423, Accuracy: 53.226%\n","Iteration step: 62; Loss: 1.428, Accuracy: 52.381%\n","Iteration step: 63; Loss: 1.416, Accuracy: 53.125%\n","Iteration step: 64; Loss: 1.405, Accuracy: 53.846%\n","Iteration step: 65; Loss: 1.394, Accuracy: 54.545%\n","Iteration step: 66; Loss: 1.411, Accuracy: 53.731%\n","Iteration step: 67; Loss: 1.399, Accuracy: 54.412%\n","Iteration step: 68; Loss: 1.413, Accuracy: 53.623%\n","Iteration step: 69; Loss: 1.402, Accuracy: 54.286%\n","Iteration step: 70; Loss: 1.391, Accuracy: 54.930%\n","Iteration step: 71; Loss: 1.380, Accuracy: 55.556%\n","Iteration step: 72; Loss: 1.369, Accuracy: 56.164%\n","Iteration step: 73; Loss: 1.383, Accuracy: 55.405%\n","Iteration step: 74; Loss: 1.399, Accuracy: 54.667%\n","Iteration step: 75; Loss: 1.388, Accuracy: 55.263%\n","Iteration step: 76; Loss: 1.377, Accuracy: 55.844%\n","Iteration step: 77; Loss: 1.365, Accuracy: 56.410%\n","Iteration step: 78; Loss: 1.381, Accuracy: 55.696%\n","Iteration step: 79; Loss: 1.389, Accuracy: 55.000%\n","Iteration step: 80; Loss: 1.378, Accuracy: 55.556%\n","Iteration step: 81; Loss: 1.367, Accuracy: 56.098%\n","Iteration step: 82; Loss: 1.375, Accuracy: 55.422%\n","Iteration step: 83; Loss: 1.383, Accuracy: 54.762%\n","Iteration step: 84; Loss: 1.391, Accuracy: 54.118%\n","Iteration step: 85; Loss: 1.404, Accuracy: 53.488%\n","Iteration step: 86; Loss: 1.410, Accuracy: 52.874%\n","Iteration step: 87; Loss: 1.400, Accuracy: 53.409%\n","Iteration step: 88; Loss: 1.390, Accuracy: 53.933%\n","Iteration step: 89; Loss: 1.395, Accuracy: 53.333%\n","Iteration step: 90; Loss: 1.386, Accuracy: 53.846%\n","Iteration step: 91; Loss: 1.397, Accuracy: 53.261%\n","Iteration step: 92; Loss: 1.401, Accuracy: 52.688%\n","Iteration step: 93; Loss: 1.411, Accuracy: 52.128%\n","Iteration step: 94; Loss: 1.422, Accuracy: 51.579%\n","Iteration step: 95; Loss: 1.414, Accuracy: 52.083%\n","Iteration step: 96; Loss: 1.406, Accuracy: 52.577%\n","Iteration step: 97; Loss: 1.439, Accuracy: 52.041%\n","Iteration step: 98; Loss: 1.450, Accuracy: 51.515%\n","Iteration step: 99; Loss: 1.460, Accuracy: 51.000%\n","Iteration step: 100; Loss: 1.452, Accuracy: 51.485%\n","Iteration step: 101; Loss: 1.454, Accuracy: 50.980%\n","Iteration step: 102; Loss: 1.447, Accuracy: 51.456%\n","Iteration step: 103; Loss: 1.440, Accuracy: 51.923%\n","Iteration step: 104; Loss: 1.448, Accuracy: 51.429%\n","Iteration step: 105; Loss: 1.442, Accuracy: 51.887%\n","Iteration step: 106; Loss: 1.436, Accuracy: 52.336%\n","Iteration step: 107; Loss: 1.429, Accuracy: 52.778%\n","Iteration step: 108; Loss: 1.424, Accuracy: 53.211%\n","Iteration step: 109; Loss: 1.418, Accuracy: 53.636%\n","Iteration step: 110; Loss: 1.412, Accuracy: 54.054%\n","Iteration step: 111; Loss: 1.413, Accuracy: 53.571%\n","Iteration step: 112; Loss: 1.415, Accuracy: 53.097%\n","Iteration step: 113; Loss: 1.409, Accuracy: 53.509%\n","Iteration step: 114; Loss: 1.403, Accuracy: 53.913%\n","Iteration step: 115; Loss: 1.397, Accuracy: 54.310%\n","Iteration step: 116; Loss: 1.406, Accuracy: 53.846%\n","Iteration step: 117; Loss: 1.400, Accuracy: 54.237%\n","Iteration step: 118; Loss: 1.394, Accuracy: 54.622%\n","Iteration step: 119; Loss: 1.403, Accuracy: 54.167%\n","Iteration step: 120; Loss: 1.397, Accuracy: 54.545%\n","Iteration step: 121; Loss: 1.405, Accuracy: 54.098%\n","Iteration step: 122; Loss: 1.399, Accuracy: 54.472%\n","Iteration step: 123; Loss: 1.393, Accuracy: 54.839%\n","Iteration step: 124; Loss: 1.386, Accuracy: 55.200%\n","Iteration step: 125; Loss: 1.395, Accuracy: 54.762%\n","Iteration step: 126; Loss: 1.389, Accuracy: 55.118%\n","Iteration step: 127; Loss: 1.397, Accuracy: 54.688%\n","Iteration step: 128; Loss: 1.405, Accuracy: 54.264%\n","Iteration step: 129; Loss: 1.408, Accuracy: 53.846%\n","Iteration step: 130; Loss: 1.415, Accuracy: 53.435%\n","Iteration step: 131; Loss: 1.409, Accuracy: 53.788%\n","Iteration step: 132; Loss: 1.403, Accuracy: 54.135%\n","Iteration step: 133; Loss: 1.410, Accuracy: 53.731%\n","Iteration step: 134; Loss: 1.404, Accuracy: 54.074%\n","Iteration step: 135; Loss: 1.411, Accuracy: 53.676%\n","Iteration step: 136; Loss: 1.419, Accuracy: 53.285%\n","Iteration step: 137; Loss: 1.413, Accuracy: 53.623%\n","Iteration step: 138; Loss: 1.421, Accuracy: 53.237%\n","Iteration step: 139; Loss: 1.428, Accuracy: 52.857%\n","Iteration step: 140; Loss: 1.422, Accuracy: 53.191%\n","Iteration step: 141; Loss: 1.417, Accuracy: 53.521%\n","Iteration step: 142; Loss: 1.412, Accuracy: 53.846%\n","Iteration step: 143; Loss: 1.415, Accuracy: 53.472%\n","Iteration step: 144; Loss: 1.410, Accuracy: 53.793%\n","Iteration step: 145; Loss: 1.405, Accuracy: 54.110%\n","Iteration step: 146; Loss: 1.411, Accuracy: 53.741%\n","Iteration step: 147; Loss: 1.406, Accuracy: 54.054%\n","Iteration step: 148; Loss: 1.400, Accuracy: 54.362%\n","Iteration step: 149; Loss: 1.395, Accuracy: 54.667%\n","Iteration step: 150; Loss: 1.391, Accuracy: 54.967%\n","Iteration step: 151; Loss: 1.386, Accuracy: 55.263%\n","Iteration step: 152; Loss: 1.390, Accuracy: 54.902%\n","Iteration step: 153; Loss: 1.385, Accuracy: 55.195%\n","Iteration step: 154; Loss: 1.390, Accuracy: 54.839%\n","Iteration step: 155; Loss: 1.385, Accuracy: 55.128%\n","Iteration step: 156; Loss: 1.380, Accuracy: 55.414%\n","Iteration step: 157; Loss: 1.371, Accuracy: 55.696%\n","Iteration step: 158; Loss: 1.363, Accuracy: 55.975%\n","Iteration step: 159; Loss: 1.367, Accuracy: 55.625%\n","Iteration step: 160; Loss: 1.373, Accuracy: 55.280%\n","Iteration step: 161; Loss: 1.366, Accuracy: 55.556%\n","Iteration step: 162; Loss: 1.361, Accuracy: 55.828%\n","Iteration step: 163; Loss: 1.353, Accuracy: 56.098%\n","Iteration step: 164; Loss: 1.360, Accuracy: 55.758%\n","Iteration step: 165; Loss: 1.379, Accuracy: 55.422%\n","Iteration step: 166; Loss: 1.383, Accuracy: 55.090%\n","Iteration step: 167; Loss: 1.386, Accuracy: 54.762%\n","Iteration step: 168; Loss: 1.381, Accuracy: 55.030%\n","Iteration step: 169; Loss: 1.377, Accuracy: 55.294%\n","Iteration step: 170; Loss: 1.369, Accuracy: 55.556%\n","Iteration step: 171; Loss: 1.364, Accuracy: 55.814%\n","Iteration step: 172; Loss: 1.360, Accuracy: 56.069%\n","Iteration step: 173; Loss: 1.367, Accuracy: 55.747%\n","Iteration step: 174; Loss: 1.374, Accuracy: 55.429%\n","Iteration step: 175; Loss: 1.382, Accuracy: 55.114%\n","Iteration step: 176; Loss: 1.378, Accuracy: 55.367%\n","Iteration step: 177; Loss: 1.373, Accuracy: 55.618%\n","Iteration step: 178; Loss: 1.379, Accuracy: 55.307%\n","Iteration step: 179; Loss: 1.375, Accuracy: 55.556%\n","Iteration step: 180; Loss: 1.367, Accuracy: 55.801%\n","Iteration step: 181; Loss: 1.372, Accuracy: 55.495%\n","Iteration step: 182; Loss: 1.365, Accuracy: 55.738%\n","Iteration step: 183; Loss: 1.370, Accuracy: 55.435%\n","Iteration step: 184; Loss: 1.363, Accuracy: 55.676%\n","Iteration step: 185; Loss: 1.357, Accuracy: 55.914%\n","Iteration step: 186; Loss: 1.361, Accuracy: 55.615%\n","Iteration step: 187; Loss: 1.364, Accuracy: 55.319%\n","Iteration step: 188; Loss: 1.366, Accuracy: 55.026%\n","Iteration step: 189; Loss: 1.363, Accuracy: 55.263%\n","Iteration step: 190; Loss: 1.368, Accuracy: 54.974%\n","Iteration step: 191; Loss: 1.372, Accuracy: 54.688%\n","Iteration step: 192; Loss: 1.376, Accuracy: 54.404%\n","Iteration step: 193; Loss: 1.378, Accuracy: 54.124%\n","Iteration step: 194; Loss: 1.375, Accuracy: 54.359%\n","Iteration step: 195; Loss: 1.372, Accuracy: 54.592%\n","Iteration step: 196; Loss: 1.369, Accuracy: 54.822%\n","Iteration step: 197; Loss: 1.373, Accuracy: 54.545%\n","Iteration step: 198; Loss: 1.370, Accuracy: 54.774%\n","Iteration step: 199; Loss: 1.364, Accuracy: 55.000%\n","Iteration step: 200; Loss: 1.366, Accuracy: 54.726%\n","Iteration step: 201; Loss: 1.370, Accuracy: 54.455%\n","Iteration step: 202; Loss: 1.373, Accuracy: 54.187%\n","Iteration step: 203; Loss: 1.375, Accuracy: 53.922%\n","Iteration step: 204; Loss: 1.379, Accuracy: 53.659%\n","Iteration step: 205; Loss: 1.383, Accuracy: 53.398%\n","Iteration step: 206; Loss: 1.386, Accuracy: 53.140%\n","Iteration step: 207; Loss: 1.418, Accuracy: 52.885%\n","Iteration step: 208; Loss: 1.413, Accuracy: 53.110%\n","Iteration step: 209; Loss: 1.416, Accuracy: 52.857%\n","Iteration step: 210; Loss: 1.418, Accuracy: 52.607%\n","Iteration step: 211; Loss: 1.419, Accuracy: 52.358%\n","Iteration step: 212; Loss: 1.417, Accuracy: 52.582%\n","Iteration step: 213; Loss: 1.415, Accuracy: 52.804%\n","Iteration step: 214; Loss: 1.416, Accuracy: 52.558%\n","Iteration step: 215; Loss: 1.414, Accuracy: 52.778%\n","Iteration step: 216; Loss: 1.417, Accuracy: 52.535%\n","Iteration step: 217; Loss: 1.420, Accuracy: 52.294%\n","Iteration step: 218; Loss: 1.423, Accuracy: 52.055%\n","Iteration step: 219; Loss: 1.424, Accuracy: 51.818%\n","Iteration step: 220; Loss: 1.422, Accuracy: 52.036%\n","Iteration step: 221; Loss: 1.423, Accuracy: 51.802%\n","Iteration step: 222; Loss: 1.426, Accuracy: 51.570%\n","Iteration step: 223; Loss: 1.424, Accuracy: 51.786%\n","Iteration step: 224; Loss: 1.425, Accuracy: 51.556%\n","Iteration step: 225; Loss: 1.426, Accuracy: 51.327%\n","Iteration step: 226; Loss: 1.424, Accuracy: 51.542%\n","Iteration step: 227; Loss: 1.427, Accuracy: 51.316%\n","Iteration step: 228; Loss: 1.425, Accuracy: 51.528%\n","Iteration step: 229; Loss: 1.428, Accuracy: 51.304%\n","Epoch: 6; Validation loss 1.5168758630752563; acc: 0.49130433797836304\n","Current epoch: 7\n","Iteration step: 0; Loss: 1.017, Accuracy: 100.000%\n","Iteration step: 1; Loss: 1.015, Accuracy: 100.000%\n","Iteration step: 2; Loss: 1.205, Accuracy: 66.667%\n","Iteration step: 3; Loss: 1.153, Accuracy: 75.000%\n","Iteration step: 4; Loss: 1.236, Accuracy: 60.000%\n","Iteration step: 5; Loss: 1.194, Accuracy: 66.667%\n","Iteration step: 6; Loss: 1.315, Accuracy: 57.143%\n","Iteration step: 7; Loss: 1.272, Accuracy: 62.500%\n","Iteration step: 8; Loss: 1.366, Accuracy: 55.556%\n","Iteration step: 9; Loss: 1.448, Accuracy: 50.000%\n","Iteration step: 10; Loss: 1.509, Accuracy: 45.455%\n","Iteration step: 11; Loss: 1.467, Accuracy: 50.000%\n","Iteration step: 12; Loss: 1.425, Accuracy: 53.846%\n","Iteration step: 13; Loss: 1.476, Accuracy: 50.000%\n","Iteration step: 14; Loss: 1.438, Accuracy: 53.333%\n","Iteration step: 15; Loss: 1.404, Accuracy: 56.250%\n","Iteration step: 16; Loss: 1.414, Accuracy: 52.941%\n","Iteration step: 17; Loss: 1.384, Accuracy: 55.556%\n","Iteration step: 18; Loss: 1.393, Accuracy: 52.632%\n","Iteration step: 19; Loss: 1.430, Accuracy: 50.000%\n","Iteration step: 20; Loss: 1.403, Accuracy: 52.381%\n","Iteration step: 21; Loss: 1.409, Accuracy: 50.000%\n","Iteration step: 22; Loss: 1.446, Accuracy: 47.826%\n","Iteration step: 23; Loss: 1.420, Accuracy: 50.000%\n","Iteration step: 24; Loss: 1.453, Accuracy: 48.000%\n","Iteration step: 25; Loss: 1.483, Accuracy: 46.154%\n","Iteration step: 26; Loss: 1.459, Accuracy: 48.148%\n","Iteration step: 27; Loss: 1.554, Accuracy: 46.429%\n","Iteration step: 28; Loss: 1.529, Accuracy: 48.276%\n","Iteration step: 29; Loss: 1.550, Accuracy: 46.667%\n","Iteration step: 30; Loss: 1.527, Accuracy: 48.387%\n","Iteration step: 31; Loss: 1.507, Accuracy: 50.000%\n","Iteration step: 32; Loss: 1.509, Accuracy: 48.485%\n","Iteration step: 33; Loss: 1.525, Accuracy: 47.059%\n","Iteration step: 34; Loss: 1.543, Accuracy: 45.714%\n","Iteration step: 35; Loss: 1.559, Accuracy: 44.444%\n","Iteration step: 36; Loss: 1.541, Accuracy: 45.946%\n","Iteration step: 37; Loss: 1.524, Accuracy: 47.368%\n","Iteration step: 38; Loss: 1.507, Accuracy: 48.718%\n","Iteration step: 39; Loss: 1.510, Accuracy: 47.500%\n","Iteration step: 40; Loss: 1.529, Accuracy: 46.341%\n","Iteration step: 41; Loss: 1.547, Accuracy: 45.238%\n","Iteration step: 42; Loss: 1.559, Accuracy: 44.186%\n","Iteration step: 43; Loss: 1.568, Accuracy: 43.182%\n","Iteration step: 44; Loss: 1.554, Accuracy: 44.444%\n","Iteration step: 45; Loss: 1.540, Accuracy: 45.652%\n","Iteration step: 46; Loss: 1.527, Accuracy: 46.809%\n","Iteration step: 47; Loss: 1.514, Accuracy: 47.917%\n","Iteration step: 48; Loss: 1.501, Accuracy: 48.980%\n","Iteration step: 49; Loss: 1.505, Accuracy: 48.000%\n","Iteration step: 50; Loss: 1.492, Accuracy: 49.020%\n","Iteration step: 51; Loss: 1.481, Accuracy: 50.000%\n","Iteration step: 52; Loss: 1.469, Accuracy: 50.943%\n","Iteration step: 53; Loss: 1.457, Accuracy: 51.852%\n","Iteration step: 54; Loss: 1.446, Accuracy: 52.727%\n","Iteration step: 55; Loss: 1.434, Accuracy: 53.571%\n","Iteration step: 56; Loss: 1.423, Accuracy: 54.386%\n","Iteration step: 57; Loss: 1.411, Accuracy: 55.172%\n","Iteration step: 58; Loss: 1.423, Accuracy: 54.237%\n","Iteration step: 59; Loss: 1.429, Accuracy: 53.333%\n","Iteration step: 60; Loss: 1.434, Accuracy: 52.459%\n","Iteration step: 61; Loss: 1.440, Accuracy: 51.613%\n","Iteration step: 62; Loss: 1.428, Accuracy: 52.381%\n","Iteration step: 63; Loss: 1.416, Accuracy: 53.125%\n","Iteration step: 64; Loss: 1.432, Accuracy: 52.308%\n","Iteration step: 65; Loss: 1.420, Accuracy: 53.030%\n","Iteration step: 66; Loss: 1.408, Accuracy: 53.731%\n","Iteration step: 67; Loss: 1.397, Accuracy: 54.412%\n","Iteration step: 68; Loss: 1.385, Accuracy: 55.072%\n","Iteration step: 69; Loss: 1.401, Accuracy: 54.286%\n","Iteration step: 70; Loss: 1.389, Accuracy: 54.930%\n","Iteration step: 71; Loss: 1.377, Accuracy: 55.556%\n","Iteration step: 72; Loss: 1.393, Accuracy: 54.795%\n","Iteration step: 73; Loss: 1.410, Accuracy: 54.054%\n","Iteration step: 74; Loss: 1.426, Accuracy: 53.333%\n","Iteration step: 75; Loss: 1.432, Accuracy: 52.632%\n","Iteration step: 76; Loss: 1.420, Accuracy: 53.247%\n","Iteration step: 77; Loss: 1.409, Accuracy: 53.846%\n","Iteration step: 78; Loss: 1.415, Accuracy: 53.165%\n","Iteration step: 79; Loss: 1.404, Accuracy: 53.750%\n","Iteration step: 80; Loss: 1.410, Accuracy: 53.086%\n","Iteration step: 81; Loss: 1.399, Accuracy: 53.659%\n","Iteration step: 82; Loss: 1.389, Accuracy: 54.217%\n","Iteration step: 83; Loss: 1.394, Accuracy: 53.571%\n","Iteration step: 84; Loss: 1.408, Accuracy: 52.941%\n","Iteration step: 85; Loss: 1.398, Accuracy: 53.488%\n","Iteration step: 86; Loss: 1.402, Accuracy: 52.874%\n","Iteration step: 87; Loss: 1.415, Accuracy: 52.273%\n","Iteration step: 88; Loss: 1.418, Accuracy: 51.685%\n","Iteration step: 89; Loss: 1.409, Accuracy: 52.222%\n","Iteration step: 90; Loss: 1.400, Accuracy: 52.747%\n","Iteration step: 91; Loss: 1.403, Accuracy: 52.174%\n","Iteration step: 92; Loss: 1.414, Accuracy: 51.613%\n","Iteration step: 93; Loss: 1.425, Accuracy: 51.064%\n","Iteration step: 94; Loss: 1.436, Accuracy: 50.526%\n","Iteration step: 95; Loss: 1.428, Accuracy: 51.042%\n","Iteration step: 96; Loss: 1.420, Accuracy: 51.546%\n","Iteration step: 97; Loss: 1.412, Accuracy: 52.041%\n","Iteration step: 98; Loss: 1.405, Accuracy: 52.525%\n","Iteration step: 99; Loss: 1.398, Accuracy: 53.000%\n","Iteration step: 100; Loss: 1.390, Accuracy: 53.465%\n","Iteration step: 101; Loss: 1.402, Accuracy: 52.941%\n","Iteration step: 102; Loss: 1.395, Accuracy: 53.398%\n","Iteration step: 103; Loss: 1.406, Accuracy: 52.885%\n","Iteration step: 104; Loss: 1.435, Accuracy: 52.381%\n","Iteration step: 105; Loss: 1.428, Accuracy: 52.830%\n","Iteration step: 106; Loss: 1.421, Accuracy: 53.271%\n","Iteration step: 107; Loss: 1.414, Accuracy: 53.704%\n","Iteration step: 108; Loss: 1.407, Accuracy: 54.128%\n","Iteration step: 109; Loss: 1.417, Accuracy: 53.636%\n","Iteration step: 110; Loss: 1.419, Accuracy: 53.153%\n","Iteration step: 111; Loss: 1.412, Accuracy: 53.571%\n","Iteration step: 112; Loss: 1.406, Accuracy: 53.982%\n","Iteration step: 113; Loss: 1.408, Accuracy: 53.509%\n","Iteration step: 114; Loss: 1.411, Accuracy: 53.043%\n","Iteration step: 115; Loss: 1.405, Accuracy: 53.448%\n","Iteration step: 116; Loss: 1.398, Accuracy: 53.846%\n","Iteration step: 117; Loss: 1.392, Accuracy: 54.237%\n","Iteration step: 118; Loss: 1.385, Accuracy: 54.622%\n","Iteration step: 119; Loss: 1.379, Accuracy: 55.000%\n","Iteration step: 120; Loss: 1.373, Accuracy: 55.372%\n","Iteration step: 121; Loss: 1.383, Accuracy: 54.918%\n","Iteration step: 122; Loss: 1.376, Accuracy: 55.285%\n","Iteration step: 123; Loss: 1.386, Accuracy: 54.839%\n","Iteration step: 124; Loss: 1.395, Accuracy: 54.400%\n","Iteration step: 125; Loss: 1.404, Accuracy: 53.968%\n","Iteration step: 126; Loss: 1.412, Accuracy: 53.543%\n","Iteration step: 127; Loss: 1.405, Accuracy: 53.906%\n","Iteration step: 128; Loss: 1.399, Accuracy: 54.264%\n","Iteration step: 129; Loss: 1.407, Accuracy: 53.846%\n","Iteration step: 130; Loss: 1.415, Accuracy: 53.435%\n","Iteration step: 131; Loss: 1.409, Accuracy: 53.788%\n","Iteration step: 132; Loss: 1.403, Accuracy: 54.135%\n","Iteration step: 133; Loss: 1.410, Accuracy: 53.731%\n","Iteration step: 134; Loss: 1.404, Accuracy: 54.074%\n","Iteration step: 135; Loss: 1.398, Accuracy: 54.412%\n","Iteration step: 136; Loss: 1.393, Accuracy: 54.745%\n","Iteration step: 137; Loss: 1.396, Accuracy: 54.348%\n","Iteration step: 138; Loss: 1.391, Accuracy: 54.676%\n","Iteration step: 139; Loss: 1.398, Accuracy: 54.286%\n","Iteration step: 140; Loss: 1.393, Accuracy: 54.610%\n","Iteration step: 141; Loss: 1.401, Accuracy: 54.225%\n","Iteration step: 142; Loss: 1.408, Accuracy: 53.846%\n","Iteration step: 143; Loss: 1.403, Accuracy: 54.167%\n","Iteration step: 144; Loss: 1.398, Accuracy: 54.483%\n","Iteration step: 145; Loss: 1.392, Accuracy: 54.795%\n","Iteration step: 146; Loss: 1.398, Accuracy: 54.422%\n","Iteration step: 147; Loss: 1.393, Accuracy: 54.730%\n","Iteration step: 148; Loss: 1.398, Accuracy: 54.362%\n","Iteration step: 149; Loss: 1.393, Accuracy: 54.667%\n","Iteration step: 150; Loss: 1.388, Accuracy: 54.967%\n","Iteration step: 151; Loss: 1.392, Accuracy: 54.605%\n","Iteration step: 152; Loss: 1.396, Accuracy: 54.248%\n","Iteration step: 153; Loss: 1.391, Accuracy: 54.545%\n","Iteration step: 154; Loss: 1.386, Accuracy: 54.839%\n","Iteration step: 155; Loss: 1.390, Accuracy: 54.487%\n","Iteration step: 156; Loss: 1.386, Accuracy: 54.777%\n","Iteration step: 157; Loss: 1.380, Accuracy: 55.063%\n","Iteration step: 158; Loss: 1.386, Accuracy: 54.717%\n","Iteration step: 159; Loss: 1.380, Accuracy: 55.000%\n","Iteration step: 160; Loss: 1.388, Accuracy: 54.658%\n","Iteration step: 161; Loss: 1.392, Accuracy: 54.321%\n","Iteration step: 162; Loss: 1.387, Accuracy: 54.601%\n","Iteration step: 163; Loss: 1.382, Accuracy: 54.878%\n","Iteration step: 164; Loss: 1.377, Accuracy: 55.152%\n","Iteration step: 165; Loss: 1.373, Accuracy: 55.422%\n","Iteration step: 166; Loss: 1.391, Accuracy: 55.090%\n","Iteration step: 167; Loss: 1.394, Accuracy: 54.762%\n","Iteration step: 168; Loss: 1.398, Accuracy: 54.438%\n","Iteration step: 169; Loss: 1.392, Accuracy: 54.706%\n","Iteration step: 170; Loss: 1.398, Accuracy: 54.386%\n","Iteration step: 171; Loss: 1.393, Accuracy: 54.651%\n","Iteration step: 172; Loss: 1.389, Accuracy: 54.913%\n","Iteration step: 173; Loss: 1.384, Accuracy: 55.172%\n","Iteration step: 174; Loss: 1.391, Accuracy: 54.857%\n","Iteration step: 175; Loss: 1.386, Accuracy: 55.114%\n","Iteration step: 176; Loss: 1.382, Accuracy: 55.367%\n","Iteration step: 177; Loss: 1.378, Accuracy: 55.618%\n","Iteration step: 178; Loss: 1.373, Accuracy: 55.866%\n","Iteration step: 179; Loss: 1.379, Accuracy: 55.556%\n","Iteration step: 180; Loss: 1.372, Accuracy: 55.801%\n","Iteration step: 181; Loss: 1.367, Accuracy: 56.044%\n","Iteration step: 182; Loss: 1.370, Accuracy: 55.738%\n","Iteration step: 183; Loss: 1.377, Accuracy: 55.435%\n","Iteration step: 184; Loss: 1.384, Accuracy: 55.135%\n","Iteration step: 185; Loss: 1.376, Accuracy: 55.376%\n","Iteration step: 186; Loss: 1.371, Accuracy: 55.615%\n","Iteration step: 187; Loss: 1.376, Accuracy: 55.319%\n","Iteration step: 188; Loss: 1.380, Accuracy: 55.026%\n","Iteration step: 189; Loss: 1.383, Accuracy: 54.737%\n","Iteration step: 190; Loss: 1.388, Accuracy: 54.450%\n","Iteration step: 191; Loss: 1.393, Accuracy: 54.167%\n","Iteration step: 192; Loss: 1.390, Accuracy: 54.404%\n","Iteration step: 193; Loss: 1.386, Accuracy: 54.639%\n","Iteration step: 194; Loss: 1.382, Accuracy: 54.872%\n","Iteration step: 195; Loss: 1.387, Accuracy: 54.592%\n","Iteration step: 196; Loss: 1.391, Accuracy: 54.315%\n","Iteration step: 197; Loss: 1.387, Accuracy: 54.545%\n","Iteration step: 198; Loss: 1.390, Accuracy: 54.271%\n","Iteration step: 199; Loss: 1.383, Accuracy: 54.500%\n","Iteration step: 200; Loss: 1.387, Accuracy: 54.229%\n","Iteration step: 201; Loss: 1.389, Accuracy: 53.960%\n","Iteration step: 202; Loss: 1.393, Accuracy: 53.695%\n","Iteration step: 203; Loss: 1.397, Accuracy: 53.431%\n","Iteration step: 204; Loss: 1.400, Accuracy: 53.171%\n","Iteration step: 205; Loss: 1.402, Accuracy: 52.913%\n","Iteration step: 206; Loss: 1.411, Accuracy: 52.657%\n","Iteration step: 207; Loss: 1.413, Accuracy: 52.404%\n","Iteration step: 208; Loss: 1.409, Accuracy: 52.632%\n","Iteration step: 209; Loss: 1.413, Accuracy: 52.381%\n","Iteration step: 210; Loss: 1.416, Accuracy: 52.133%\n","Iteration step: 211; Loss: 1.419, Accuracy: 51.887%\n","Iteration step: 212; Loss: 1.422, Accuracy: 51.643%\n","Iteration step: 213; Loss: 1.420, Accuracy: 51.869%\n","Iteration step: 214; Loss: 1.421, Accuracy: 51.628%\n","Iteration step: 215; Loss: 1.419, Accuracy: 51.852%\n","Iteration step: 216; Loss: 1.422, Accuracy: 51.613%\n","Iteration step: 217; Loss: 1.425, Accuracy: 51.376%\n","Iteration step: 218; Loss: 1.423, Accuracy: 51.598%\n","Iteration step: 219; Loss: 1.426, Accuracy: 51.364%\n","Iteration step: 220; Loss: 1.427, Accuracy: 51.131%\n","Iteration step: 221; Loss: 1.425, Accuracy: 51.351%\n","Iteration step: 222; Loss: 1.423, Accuracy: 51.570%\n","Iteration step: 223; Loss: 1.424, Accuracy: 51.339%\n","Iteration step: 224; Loss: 1.426, Accuracy: 51.111%\n","Iteration step: 225; Loss: 1.424, Accuracy: 51.327%\n","Iteration step: 226; Loss: 1.426, Accuracy: 51.101%\n","Iteration step: 227; Loss: 1.424, Accuracy: 51.316%\n","Iteration step: 228; Loss: 1.422, Accuracy: 51.528%\n","Iteration step: 229; Loss: 1.425, Accuracy: 51.304%\n","Epoch: 7; Validation loss 1.4439811706542969; acc: 0.5130434632301331\n","Current epoch: 8\n","Iteration step: 0; Loss: 1.660, Accuracy: 0.000%\n","Iteration step: 1; Loss: 1.657, Accuracy: 0.000%\n","Iteration step: 2; Loss: 1.436, Accuracy: 33.333%\n","Iteration step: 3; Loss: 1.324, Accuracy: 50.000%\n","Iteration step: 4; Loss: 1.257, Accuracy: 60.000%\n","Iteration step: 5; Loss: 1.395, Accuracy: 50.000%\n","Iteration step: 6; Loss: 1.488, Accuracy: 42.857%\n","Iteration step: 7; Loss: 1.424, Accuracy: 50.000%\n","Iteration step: 8; Loss: 1.504, Accuracy: 44.444%\n","Iteration step: 9; Loss: 1.450, Accuracy: 50.000%\n","Iteration step: 10; Loss: 1.496, Accuracy: 45.455%\n","Iteration step: 11; Loss: 1.451, Accuracy: 50.000%\n","Iteration step: 12; Loss: 1.499, Accuracy: 46.154%\n","Iteration step: 13; Loss: 1.459, Accuracy: 50.000%\n","Iteration step: 14; Loss: 1.499, Accuracy: 46.667%\n","Iteration step: 15; Loss: 1.464, Accuracy: 50.000%\n","Iteration step: 16; Loss: 1.433, Accuracy: 52.941%\n","Iteration step: 17; Loss: 1.445, Accuracy: 50.000%\n","Iteration step: 18; Loss: 1.456, Accuracy: 47.368%\n","Iteration step: 19; Loss: 1.429, Accuracy: 50.000%\n","Iteration step: 20; Loss: 1.405, Accuracy: 52.381%\n","Iteration step: 21; Loss: 1.382, Accuracy: 54.545%\n","Iteration step: 22; Loss: 1.361, Accuracy: 56.522%\n","Iteration step: 23; Loss: 1.389, Accuracy: 54.167%\n","Iteration step: 24; Loss: 1.399, Accuracy: 52.000%\n","Iteration step: 25; Loss: 1.500, Accuracy: 50.000%\n","Iteration step: 26; Loss: 1.521, Accuracy: 48.148%\n","Iteration step: 27; Loss: 1.541, Accuracy: 46.429%\n","Iteration step: 28; Loss: 1.559, Accuracy: 44.828%\n","Iteration step: 29; Loss: 1.576, Accuracy: 43.333%\n","Iteration step: 30; Loss: 1.596, Accuracy: 41.935%\n","Iteration step: 31; Loss: 1.574, Accuracy: 43.750%\n","Iteration step: 32; Loss: 1.587, Accuracy: 42.424%\n","Iteration step: 33; Loss: 1.567, Accuracy: 44.118%\n","Iteration step: 34; Loss: 1.548, Accuracy: 45.714%\n","Iteration step: 35; Loss: 1.552, Accuracy: 44.444%\n","Iteration step: 36; Loss: 1.564, Accuracy: 43.243%\n","Iteration step: 37; Loss: 1.547, Accuracy: 44.737%\n","Iteration step: 38; Loss: 1.531, Accuracy: 46.154%\n","Iteration step: 39; Loss: 1.516, Accuracy: 47.500%\n","Iteration step: 40; Loss: 1.520, Accuracy: 46.341%\n","Iteration step: 41; Loss: 1.530, Accuracy: 45.238%\n","Iteration step: 42; Loss: 1.515, Accuracy: 46.512%\n","Iteration step: 43; Loss: 1.532, Accuracy: 45.455%\n","Iteration step: 44; Loss: 1.542, Accuracy: 44.444%\n","Iteration step: 45; Loss: 1.529, Accuracy: 45.652%\n","Iteration step: 46; Loss: 1.515, Accuracy: 46.809%\n","Iteration step: 47; Loss: 1.502, Accuracy: 47.917%\n","Iteration step: 48; Loss: 1.490, Accuracy: 48.980%\n","Iteration step: 49; Loss: 1.477, Accuracy: 50.000%\n","Iteration step: 50; Loss: 1.465, Accuracy: 50.980%\n","Iteration step: 51; Loss: 1.454, Accuracy: 51.923%\n","Iteration step: 52; Loss: 1.459, Accuracy: 50.943%\n","Iteration step: 53; Loss: 1.448, Accuracy: 51.852%\n","Iteration step: 54; Loss: 1.436, Accuracy: 52.727%\n","Iteration step: 55; Loss: 1.425, Accuracy: 53.571%\n","Iteration step: 56; Loss: 1.435, Accuracy: 52.632%\n","Iteration step: 57; Loss: 1.424, Accuracy: 53.448%\n","Iteration step: 58; Loss: 1.412, Accuracy: 54.237%\n","Iteration step: 59; Loss: 1.419, Accuracy: 53.333%\n","Iteration step: 60; Loss: 1.425, Accuracy: 52.459%\n","Iteration step: 61; Loss: 1.414, Accuracy: 53.226%\n","Iteration step: 62; Loss: 1.403, Accuracy: 53.968%\n","Iteration step: 63; Loss: 1.392, Accuracy: 54.688%\n","Iteration step: 64; Loss: 1.381, Accuracy: 55.385%\n","Iteration step: 65; Loss: 1.396, Accuracy: 54.545%\n","Iteration step: 66; Loss: 1.385, Accuracy: 55.224%\n","Iteration step: 67; Loss: 1.401, Accuracy: 54.412%\n","Iteration step: 68; Loss: 1.390, Accuracy: 55.072%\n","Iteration step: 69; Loss: 1.404, Accuracy: 54.286%\n","Iteration step: 70; Loss: 1.393, Accuracy: 54.930%\n","Iteration step: 71; Loss: 1.382, Accuracy: 55.556%\n","Iteration step: 72; Loss: 1.371, Accuracy: 56.164%\n","Iteration step: 73; Loss: 1.385, Accuracy: 55.405%\n","Iteration step: 74; Loss: 1.401, Accuracy: 54.667%\n","Iteration step: 75; Loss: 1.406, Accuracy: 53.947%\n","Iteration step: 76; Loss: 1.396, Accuracy: 54.545%\n","Iteration step: 77; Loss: 1.386, Accuracy: 55.128%\n","Iteration step: 78; Loss: 1.400, Accuracy: 54.430%\n","Iteration step: 79; Loss: 1.390, Accuracy: 55.000%\n","Iteration step: 80; Loss: 1.396, Accuracy: 54.321%\n","Iteration step: 81; Loss: 1.386, Accuracy: 54.878%\n","Iteration step: 82; Loss: 1.376, Accuracy: 55.422%\n","Iteration step: 83; Loss: 1.366, Accuracy: 55.952%\n","Iteration step: 84; Loss: 1.372, Accuracy: 55.294%\n","Iteration step: 85; Loss: 1.378, Accuracy: 54.651%\n","Iteration step: 86; Loss: 1.383, Accuracy: 54.023%\n","Iteration step: 87; Loss: 1.374, Accuracy: 54.545%\n","Iteration step: 88; Loss: 1.387, Accuracy: 53.933%\n","Iteration step: 89; Loss: 1.398, Accuracy: 53.333%\n","Iteration step: 90; Loss: 1.403, Accuracy: 52.747%\n","Iteration step: 91; Loss: 1.394, Accuracy: 53.261%\n","Iteration step: 92; Loss: 1.406, Accuracy: 52.688%\n","Iteration step: 93; Loss: 1.416, Accuracy: 52.128%\n","Iteration step: 94; Loss: 1.427, Accuracy: 51.579%\n","Iteration step: 95; Loss: 1.418, Accuracy: 52.083%\n","Iteration step: 96; Loss: 1.410, Accuracy: 52.577%\n","Iteration step: 97; Loss: 1.421, Accuracy: 52.041%\n","Iteration step: 98; Loss: 1.424, Accuracy: 51.515%\n","Iteration step: 99; Loss: 1.416, Accuracy: 52.000%\n","Iteration step: 100; Loss: 1.426, Accuracy: 51.485%\n","Iteration step: 101; Loss: 1.419, Accuracy: 51.961%\n","Iteration step: 102; Loss: 1.412, Accuracy: 52.427%\n","Iteration step: 103; Loss: 1.405, Accuracy: 52.885%\n","Iteration step: 104; Loss: 1.407, Accuracy: 52.381%\n","Iteration step: 105; Loss: 1.400, Accuracy: 52.830%\n","Iteration step: 106; Loss: 1.429, Accuracy: 52.336%\n","Iteration step: 107; Loss: 1.422, Accuracy: 52.778%\n","Iteration step: 108; Loss: 1.415, Accuracy: 53.211%\n","Iteration step: 109; Loss: 1.424, Accuracy: 52.727%\n","Iteration step: 110; Loss: 1.417, Accuracy: 53.153%\n","Iteration step: 111; Loss: 1.420, Accuracy: 52.679%\n","Iteration step: 112; Loss: 1.413, Accuracy: 53.097%\n","Iteration step: 113; Loss: 1.407, Accuracy: 53.509%\n","Iteration step: 114; Loss: 1.415, Accuracy: 53.043%\n","Iteration step: 115; Loss: 1.409, Accuracy: 53.448%\n","Iteration step: 116; Loss: 1.402, Accuracy: 53.846%\n","Iteration step: 117; Loss: 1.396, Accuracy: 54.237%\n","Iteration step: 118; Loss: 1.390, Accuracy: 54.622%\n","Iteration step: 119; Loss: 1.398, Accuracy: 54.167%\n","Iteration step: 120; Loss: 1.392, Accuracy: 54.545%\n","Iteration step: 121; Loss: 1.400, Accuracy: 54.098%\n","Iteration step: 122; Loss: 1.394, Accuracy: 54.472%\n","Iteration step: 123; Loss: 1.396, Accuracy: 54.032%\n","Iteration step: 124; Loss: 1.404, Accuracy: 53.600%\n","Iteration step: 125; Loss: 1.398, Accuracy: 53.968%\n","Iteration step: 126; Loss: 1.392, Accuracy: 54.331%\n","Iteration step: 127; Loss: 1.386, Accuracy: 54.688%\n","Iteration step: 128; Loss: 1.394, Accuracy: 54.264%\n","Iteration step: 129; Loss: 1.401, Accuracy: 53.846%\n","Iteration step: 130; Loss: 1.408, Accuracy: 53.435%\n","Iteration step: 131; Loss: 1.402, Accuracy: 53.788%\n","Iteration step: 132; Loss: 1.396, Accuracy: 54.135%\n","Iteration step: 133; Loss: 1.391, Accuracy: 54.478%\n","Iteration step: 134; Loss: 1.398, Accuracy: 54.074%\n","Iteration step: 135; Loss: 1.401, Accuracy: 53.676%\n","Iteration step: 136; Loss: 1.395, Accuracy: 54.015%\n","Iteration step: 137; Loss: 1.390, Accuracy: 54.348%\n","Iteration step: 138; Loss: 1.397, Accuracy: 53.957%\n","Iteration step: 139; Loss: 1.405, Accuracy: 53.571%\n","Iteration step: 140; Loss: 1.400, Accuracy: 53.901%\n","Iteration step: 141; Loss: 1.394, Accuracy: 54.225%\n","Iteration step: 142; Loss: 1.389, Accuracy: 54.545%\n","Iteration step: 143; Loss: 1.383, Accuracy: 54.861%\n","Iteration step: 144; Loss: 1.391, Accuracy: 54.483%\n","Iteration step: 145; Loss: 1.385, Accuracy: 54.795%\n","Iteration step: 146; Loss: 1.380, Accuracy: 55.102%\n","Iteration step: 147; Loss: 1.375, Accuracy: 55.405%\n","Iteration step: 148; Loss: 1.381, Accuracy: 55.034%\n","Iteration step: 149; Loss: 1.385, Accuracy: 54.667%\n","Iteration step: 150; Loss: 1.379, Accuracy: 54.967%\n","Iteration step: 151; Loss: 1.374, Accuracy: 55.263%\n","Iteration step: 152; Loss: 1.380, Accuracy: 54.902%\n","Iteration step: 153; Loss: 1.386, Accuracy: 54.545%\n","Iteration step: 154; Loss: 1.380, Accuracy: 54.839%\n","Iteration step: 155; Loss: 1.375, Accuracy: 55.128%\n","Iteration step: 156; Loss: 1.370, Accuracy: 55.414%\n","Iteration step: 157; Loss: 1.365, Accuracy: 55.696%\n","Iteration step: 158; Loss: 1.372, Accuracy: 55.346%\n","Iteration step: 159; Loss: 1.367, Accuracy: 55.625%\n","Iteration step: 160; Loss: 1.371, Accuracy: 55.280%\n","Iteration step: 161; Loss: 1.378, Accuracy: 54.938%\n","Iteration step: 162; Loss: 1.372, Accuracy: 55.215%\n","Iteration step: 163; Loss: 1.379, Accuracy: 54.878%\n","Iteration step: 164; Loss: 1.383, Accuracy: 54.545%\n","Iteration step: 165; Loss: 1.378, Accuracy: 54.819%\n","Iteration step: 166; Loss: 1.382, Accuracy: 54.491%\n","Iteration step: 167; Loss: 1.401, Accuracy: 54.167%\n","Iteration step: 168; Loss: 1.396, Accuracy: 54.438%\n","Iteration step: 169; Loss: 1.391, Accuracy: 54.706%\n","Iteration step: 170; Loss: 1.397, Accuracy: 54.386%\n","Iteration step: 171; Loss: 1.392, Accuracy: 54.651%\n","Iteration step: 172; Loss: 1.388, Accuracy: 54.913%\n","Iteration step: 173; Loss: 1.383, Accuracy: 55.172%\n","Iteration step: 174; Loss: 1.386, Accuracy: 54.857%\n","Iteration step: 175; Loss: 1.392, Accuracy: 54.545%\n","Iteration step: 176; Loss: 1.387, Accuracy: 54.802%\n","Iteration step: 177; Loss: 1.383, Accuracy: 55.056%\n","Iteration step: 178; Loss: 1.378, Accuracy: 55.307%\n","Iteration step: 179; Loss: 1.381, Accuracy: 55.000%\n","Iteration step: 180; Loss: 1.377, Accuracy: 55.249%\n"]}],"source":["import os\n","\n","model = DeepConv(parameters)\n","\n","if os.path.isdir(\"/content/drive/MyDrive/HK2_2023-2024/NT230/demo_malware/result_training/models/{}/\".format(\"DeepConv\")):\n","    print(\"LOADING WEIGHTS!!!!\")\n","    latest = tf.train.latest_checkpoint(\"/content/drive/MyDrive/HK2_2023-2024/NT230/demo_malware/result_training/models/{}/\".format(\"DeepConv\"))\n","    print(latest)\n","    model.load_weights(latest)\n","\n","loss_func = tf.keras.losses.SparseCategoricalCrossentropy()\n","accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n","optimizer = tf.keras.optimizers.Adam(learning_rate=parameters['learning_rate'])\n","\n","\n","def train_loop(features, labels, training=False):\n","  # Define the GradientTape context\n","  with tf.GradientTape() as tape:\n","      # Get the probabilities\n","      predictions = model(features)\n","      #labels = tf.dtypes.cast(labels, tf.float32)\n","      # Calculate the loss\n","      loss = loss_func(labels, predictions)\n","  # Get the gradients\n","  gradients = tape.gradient(loss, model.trainable_variables)\n","  # Update the weights\n","  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","  return loss, predictions\n","\n","\n","# Training loop\n","# 1/ Iterate each epoch. An epoch is one pass through the dataset\n","# 2/ Whithin an epoch, iterate over each example in the training Dataset.\n","# 3/ Calculate model's loss and gradients\n","# 4/ Use an optimizer to update the model's variables\n","# 5/ Keep track of stats and repeat\n","\n","train_loss_results = []\n","train_accuracy_results = []\n","\n","validation_loss_results = []\n","validation_accuracy_results = []\n","\n","#checkpoint_path = \"models/ShallowCNN/model_ep_{}.ckpt\"\n","#checkpoint_dir = os.path.dirname(checkpoint_path)\n","\n","num_epochs = parameters['epochs']\n","\n","initial_loss = 10.0\n","for epoch in range(num_epochs):\n","  print(\"Current epoch: {}\".format(epoch))\n","\n","  checkpoint_path = \"/content/drive/MyDrive/HK2_2023-2024/NT230/demo_malware/result_training/models/{}/model_ep_{}.ckpt\".format(\"DeepConv\", epoch)\n","\n","\n","  d_train = make_dataset(file_reader,\n","                          lookup_table,\n","                          parameters['buffer_size'],\n","                          parameters['batch_size'],\n","                          1)\n","  d_val = make_dataset(file_reader,\n","                        lookup_table,\n","                        1024,\n","                        1,\n","                        1)\n","\n","\n","  # Training metrics\n","  epoch_loss_avg = tf.keras.metrics.Mean()\n","  epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n","  # Validation metrics\n","  val_epoch_loss_avg = tf.keras.metrics.Mean()\n","  val_epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n","  tr_step = 0\n","\n","  # Training loop\n","  for step, (x, y) in enumerate(d_train):\n","      #print(\"Input: {}\".format(x))\n","\n","      loss, y_ = train_loop(x, y, True)\n","\n","      # Track progress\n","      epoch_loss_avg(loss)\n","      epoch_accuracy(y, y_)\n","      print(\"Iteration step: {}; Loss: {:.3f}, Accuracy: {:.3%}\".format(tr_step,\n","                                                                        epoch_loss_avg.result(),\n","                                                                        epoch_accuracy.result()))\n","\n","\n","      tr_step += 1\n","\n","  # End epoch\n","  train_loss_results.append(epoch_loss_avg.result())\n","  train_accuracy_results.append(epoch_accuracy.result())\n","\n","\n","\n","  # Run a validation loop at the end of each epoch.\n","  for x_batch_val, y_batch_val in d_val:\n","      val_logits = model(x_batch_val)\n","      val_loss = loss_func(y_batch_val, val_logits)\n","\n","      # Update metrics\n","      val_epoch_loss_avg(val_loss)\n","      val_epoch_accuracy(y_batch_val, val_logits)\n","\n","  val_acc = val_epoch_accuracy.result()\n","  val_loss = val_epoch_loss_avg.result()\n","  print('Epoch: {}; Validation loss {}; acc: {}'.format(epoch, val_loss, val_acc))\n","\n","  validation_loss_results.append(val_loss)\n","  validation_accuracy_results.append(val_acc)\n","\n","  if float(val_loss) < initial_loss:\n","      initial_loss = float(val_loss)\n","      model.save_weights(checkpoint_path) # Save only the weights"]},{"cell_type":"markdown","metadata":{"id":"sEyrI-rECKJw"},"source":[]},{"cell_type":"markdown","metadata":{"id":"jyTc_Dyhwjhl"},"source":["# test"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2543796,"status":"ok","timestamp":1713402295834,"user":{"displayName":"Nguyễn Thành Công","userId":"07855209270426231427"},"user_tz":-420},"id":"AH_nAJX5wkgy","outputId":"84aed5c8-3175-4c49-bef9-c36ea07f7fd4"},"outputs":[{"name":"stdout","output_type":"stream","text":["LOADING WEIGHTS!!!!\n","/content/drive/MyDrive/HK2_2023-2024/NT230/result_trainning/models/DeepConv/model_ep_1.ckpt\n","Test loss 0.3129643201828003; acc: 0.9123204946517944\n","Confusion Matrix:\n"," [[172   3   0   1   0   1   0   1   0]\n"," [  1 287   0   2   0   8   0   3   0]\n"," [  0   0 379   0   0   0   0   0   1]\n"," [  0   0   0  32   0  17   0   1   1]\n"," [  5   1   1   0   0   1   0   1   0]\n"," [ 13   0   0   1   0  56   0   5   0]\n"," [  1   2   2   0   0   2  37   1   1]\n"," [ 17   2   1   2   0   2   0 126   0]\n"," [  1   6   0   0   0   5   0   3 118]]\n"]}],"source":["model_test = DeepConv(parameters)\n","\n","loss_func = tf.keras.losses.SparseCategoricalCrossentropy()\n","accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n","optimizer = tf.keras.optimizers.Adam(learning_rate=parameters['learning_rate'])\n","\n","if os.path.isdir(\"/content/drive/MyDrive/HK2_2023-2024/NT230/demo_malware/result_training/models{}/\".format(\"DeepConv\")):\n","    print(\"LOADING WEIGHTS!!!!\")\n","    latest = tf.train.latest_checkpoint(\"/content/drive/MyDrive/HK2_2023-2024/NT230/demo_malware/result_training/models/{}/\".format(\"DeepConv\"))\n","    print(latest)\n","    model_test.load_weights(latest)\n","test_epoch_loss_avg = tf.keras.metrics.Mean()\n","test_epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n","\n","y_actual_test = []\n","y_pred_test = []\n","# Evaluate model on the test set\n","\n","d_test = make_dataset(\"/content/drive/MyDrive/HK2_2023-2024/NT230/demo_malware/process_data/bytes/bytes_tf_record_test\",\n","                      lookup_table,\n","                      1,\n","                      1,\n","                      1)\n","\n","for x_batch_test, y_batch_test in d_test:\n","    test_logits = model_test(x_batch_test)\n","    test_loss = loss_func(y_batch_test, test_logits)\n","\n","    # For the confusion matrix\n","    y_pred = tf.argmax(test_logits, axis=-1)\n","    y_pred_test.extend(y_pred)\n","    y_actual_test.extend(y_batch_test)\n","\n","    # Update metrics\n","    test_epoch_loss_avg(test_loss)\n","    test_epoch_accuracy(y_batch_test, test_logits)\n","\n","test_acc = test_epoch_accuracy.result()\n","test_loss = test_epoch_loss_avg.result()\n","print('Test loss {}; acc: {}'.format(test_loss, test_acc))\n","\n","cm = confusion_matrix(y_actual_test, y_pred_test)\n","print(\"Confusion Matrix:\\n {}\".format(cm))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":358,"status":"ok","timestamp":1713407940393,"user":{"displayName":"Nguyễn Thành Công","userId":"07855209270426231427"},"user_tz":-420},"id":"uTRBmEIBEhqf","outputId":"dfa1a0cd-ee0c-4bf9-dfe6-8faa22f86605"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"deep_conv_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_1 (Embedding)     multiple                  2072      \n","                                                                 \n"," conv2d_4 (Conv2D)           multiple                  12336     \n","                                                                 \n"," conv2d_5 (Conv2D)           multiple                  147552    \n","                                                                 \n"," max_pooling2d_1 (MaxPoolin  multiple                  0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_6 (Conv2D)           multiple                  196736    \n","                                                                 \n"," conv2d_7 (Conv2D)           multiple                  393408    \n","                                                                 \n"," global_average_pooling2d_1  multiple                  0         \n","  (GlobalAveragePooling2D)                                       \n","                                                                 \n"," dropout_4 (Dropout)         multiple                  0         \n","                                                                 \n"," dense_4 (Dense)             multiple                  37056     \n","                                                                 \n"," dropout_5 (Dropout)         multiple                  0         \n","                                                                 \n"," dense_5 (Dense)             multiple                  30880     \n","                                                                 \n"," dropout_6 (Dropout)         multiple                  0         \n","                                                                 \n"," dense_6 (Dense)             multiple                  20608     \n","                                                                 \n"," dropout_7 (Dropout)         multiple                  0         \n","                                                                 \n"," dense_7 (Dense)             multiple                  1161      \n","                                                                 \n","=================================================================\n","Total params: 841809 (3.21 MB)\n","Trainable params: 841809 (3.21 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["model_test.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nj31himz3RLU"},"outputs":[],"source":["model.save_weights(\"/content/drive/MyDrive/HK2_2023-2024/NT230/result_trainning/models/{}/model_ep_{}.ckpt\".format(\"DeepConv\", 0))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PDXl_QPiOGO6"},"outputs":[],"source":["  d_train = make_dataset(file_reader,\n","                          lookup_table,\n","                          parameters['buffer_size'],\n","                          parameters['batch_size'],\n","                          1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":144},"executionInfo":{"elapsed":2097,"status":"error","timestamp":1713110052995,"user":{"displayName":"Nguyễn Thành Công","userId":"07855209270426231427"},"user_tz":-420},"id":"VUblxXjrUODg","outputId":"977d5db5-350a-41b8-de21-f30ca3a88755"},"outputs":[{"ename":"SystemExit","evalue":"0","output_type":"error","traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n","  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"]}],"source":["import sys\n","for step, (x, y) in enumerate(d_train):\n","  train = x\n","  label = y\n","  sys.exit(0)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":323,"status":"ok","timestamp":1713110064807,"user":{"displayName":"Nguyễn Thành Công","userId":"07855209270426231427"},"user_tz":-420},"id":"gMvebgkdURdD","outputId":"47f26c0c-15b7-4f39-ff70-4088b9c4824a"},"outputs":[{"name":"stdout","output_type":"stream","text":["[199 258  92  17  65 233 103  69 204  86 139 241 232  84 246  68  36 116\n"," 220 131 196 198  94 194 195  81 217 129 136 138 145 170 192 168  28 128\n"," 226  62 172 169  32 238 216 153 140 223 224 123 177 132 144 221 120 160\n"," 201 141  37 112 171 235 173 213 152  45 104 137  20 184  89 134 182 200\n"," 150 244 142 158  44  63  18  61  70  95 133  19  76  82 255 208 176  53\n"," 193  35 234  85  93  42 106  64 146  16  55 210  29 202 127 111  21 248\n","  59 218 240  46 164 159 180 166 174  96  97  88  80  72  56  48  40  24\n","  51 155 185  26 157  78 209 154 156  90 108  34 118 239 249  99  43 167\n","  39 247 230 206  31 197  73 252 219 165  66 242 237  87 126 253  22  57\n"," 143 130 231 124  79 207 102 243 187  41 191 119 251 225  27  67  30 250\n"," 215 148  74  49 228 236 115  58  77 114 254  75 203 214 189  47  83 110\n"," 211 109  91 100 161 163 101 125  52  25 117  71 229 121  60  50 245 222\n"," 105  54 227 107 162 183 122 149 190  33  38 135 186 181 205 178 179  23\n"," 212 188  98 113 147 175 151 256 257]\n"]}],"source":["import tensorflow as tf\n","\n","# Assuming your tensor is named 'tensor'\n","unique_values, _ = tf.unique(tf.reshape(train, [-1]))\n","\n","# Convert unique values tensor to numpy array\n","unique_values_array = unique_values.numpy()\n","\n","# Print unique values\n","print(unique_values_array)\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}